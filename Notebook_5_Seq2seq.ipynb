{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook 5 - Seq2seq",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronenbendavid/IDC_NLP/blob/master/Notebook_5_Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZncvdMieiQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy\n",
        "from torch import autograd, nn, optim\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOkiUCuuvBIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5b7339d7-e2ad-42b1-f076-f0dcac172ccb"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yKNnPMUdorH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "UNKNOWN = 2\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.index2word = {UNKNOWN:'__unk__'}\n",
        "        self.n_words = 3\n",
        "    \n",
        "    def index_sentence(self, sentence, write=True):\n",
        "        indexes = []\n",
        "        for w in sentence.split(' '):\n",
        "            indexes.append(self.index_word(w, write))\n",
        "        return indexes\n",
        "            \n",
        "    def index_word(self, word, write=True):\n",
        "        if word not in self.word2index:\n",
        "          if write:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words = self.n_words + 1\n",
        "          else:\n",
        "            return UNKNOWN\n",
        "        return self.word2index[word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nh0eJnNdorJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_string(s):\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Zא-ת.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu_LEC2qdorN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_langs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/drive/My Drive/Courses/Practical ML - BIU 2020/notebooks/data/%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
        "    \n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
        "    \n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_vocab = Vocab(lang2)\n",
        "        output_vocab = Vocab(lang1)\n",
        "    else:\n",
        "        input_vocab = Vocab(lang1)\n",
        "        output_vocab = Vocab(lang2)\n",
        "        \n",
        "    return input_vocab, output_vocab, pairs\n",
        "\n",
        "\n",
        "def print_pair(p):\n",
        "    print(p[0])\n",
        "    print(p[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NyPDUBPdorP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_pair(p):\n",
        "  return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filter_pairs(pairs):\n",
        "    return [pair for pair in pairs if filter_pair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "A-C2K28HdorR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f179929c-0e44-43d8-ecc8-15a2a3d1cee9"
      },
      "source": [
        "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
        "    input_vocab, output_vocab, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    \n",
        "    pairs = filter_pairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    print(\"Indexing words...\")\n",
        "    for pair in pairs:\n",
        "        input_vocab.index_sentence(pair[0])\n",
        "        output_vocab.index_sentence(pair[1])\n",
        "\n",
        "    return input_vocab, output_vocab, pairs\n",
        "\n",
        "input_vocab, output_vocab, pairs = prepare_data('eng', 'heb', True)\n",
        "\n",
        "# Print an example pair\n",
        "print_pair(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 125860 sentence pairs\n",
            "Trimmed to 104771 sentence pairs\n",
            "Indexing words...\n",
            "היא בכנסיה ברגע זה .\n",
            "she is at church right now .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "836gR78kdorX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers)\n",
        "        \n",
        "    def forward(self, word_inputs, hidden):\n",
        "        seq_len = len(word_inputs)\n",
        "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        hidden = (torch.zeros(self.n_layers, 1, self.hidden_size).cuda(),\n",
        "                  torch.zeros(self.n_layers, 1, self.hidden_size).cuda())\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5H-noTcucpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a simple decode - not using Attention at all\n",
        "\n",
        "class SimpleDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding_size, output_size, n_layers=1):\n",
        "        super(SimpleDecoderRNN, self).__init__()\n",
        "        \n",
        "        # Keep parameters for reference\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, word_input, last_hidden):\n",
        "        # Note: we run this one step at a time (word by word...)\n",
        "        \n",
        "        # Get the embedding of the current input word (last output word)\n",
        "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
        "        \n",
        "        # run through LSTM\n",
        "        rnn_output, hidden = self.lstm(word_embedded, last_hidden)\n",
        "        \n",
        "        # Final output layer (next word prediction) \n",
        "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
        "        output = F.log_softmax(self.out(rnn_output), 1)\n",
        "        \n",
        "        # Return final output and hidden state\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDQaPIhHvddL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "24f66cba-4a59-4331-919b-ae54f7656f0c"
      },
      "source": [
        "# Testing with Simple Decoder\n",
        "#===============================\n",
        "\n",
        "encoder_test = EncoderRNN(10, 30, 10, 2).cuda()\n",
        "decoder_test = SimpleDecoderRNN(10, 30, 10, 2).cuda()\n",
        "\n",
        "encoder_hidden = encoder_test.init_hidden()\n",
        "\n",
        "word_input = torch.LongTensor([1, 2, 3]).cuda()\n",
        "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
        "\n",
        "word_inputs = torch.LongTensor([1, 2, 3]).cuda()\n",
        "decoder_hidden = encoder_hidden\n",
        "\n",
        "for i in range(3):\n",
        "    decoder_output, decoder_hidden = decoder_test(word_inputs[i], decoder_hidden)\n",
        "    print('Word', i, decoder_output)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 0 tensor([[-2.1110, -2.1440, -2.5603, -2.2097, -2.7003, -2.2141, -2.3971, -2.2770,\n",
            "         -2.4199, -2.1565]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
            "Word 1 tensor([[-2.0954, -2.1332, -2.5680, -2.1901, -2.7035, -2.2075, -2.4106, -2.2944,\n",
            "         -2.4133, -2.1815]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
            "Word 2 tensor([[-2.0944, -2.1305, -2.5776, -2.1799, -2.7041, -2.1961, -2.4235, -2.3031,\n",
            "         -2.4072, -2.1868]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnsZEQZPdora",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, let's add Aditive Attention\n",
        "\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.v = nn.Parameter(torch.FloatTensor(self.hidden_size).cuda())\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "        \n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        seq_len = len(encoder_outputs)\n",
        "\n",
        "        attn_energies = torch.zeros(seq_len).cuda() \n",
        "\n",
        "        for i in range(seq_len):\n",
        "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
        "\n",
        "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
        "        return F.softmax(attn_energies, 0).unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "    def score(self, hidden, encoder_output):        \n",
        "        '''Aditive Attention'''\n",
        "        attn_input = torch.cat((hidden, encoder_output), 1)\n",
        "        energy = self.attn(attn_input)\n",
        "        energy = self.v.dot(energy.view(-1))\n",
        "        return energy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0TR9N-Zdord",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding_size, output_size, n_layers=1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        \n",
        "        # Keep parameters for reference\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size + hidden_size, hidden_size, n_layers)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        self.attn = Attn(hidden_size)\n",
        "    \n",
        "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step at a time\n",
        "        \n",
        "        # Get the embedding of the current input word (last output word)\n",
        "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
        "        \n",
        "        # Combine embedded input word and last context, run through RNN\n",
        "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
        "        rnn_output, hidden = self.lstm(rnn_input, last_hidden)\n",
        "\n",
        "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
        "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
        "        \n",
        "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
        "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
        "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
        "        output = F.log_softmax(self.out(rnn_output), 1)\n",
        "        \n",
        "        # Return final output, hidden state\n",
        "        return output, context, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_K5PGUndorh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8e59ce89-023c-4fb5-968e-7a7354d10fa8"
      },
      "source": [
        "# Testing with Attention\n",
        "\n",
        "encoder_test = EncoderRNN(10, 30, 10, 2).cuda()\n",
        "decoder_test = AttnDecoderRNN(10, 30, 10, 2).cuda()\n",
        "\n",
        "encoder_hidden = encoder_test.init_hidden()\n",
        "\n",
        "word_input = torch.LongTensor([1, 2, 3]).cuda()\n",
        "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
        "\n",
        "word_inputs = torch.LongTensor([1, 2, 3]).cuda()\n",
        "decoder_hidden = encoder_hidden\n",
        "decoder_context = torch.zeros(1, decoder_test.hidden_size).cuda()\n",
        "\n",
        "for i in range(3):\n",
        "    decoder_output, decoder_context, decoder_hidden = decoder_test(word_inputs[i], decoder_context, decoder_hidden, encoder_outputs)\n",
        "    print('Word', i, decoder_output)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 0 tensor([[-2.2632, -2.0632, -2.0785, -2.6232, -2.1132, -2.2636, -2.3090, -2.3885,\n",
            "         -2.5113, -2.6021]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
            "Word 1 tensor([[-2.2646, -2.0828, -2.0934, -2.5988, -2.1272, -2.2789, -2.3145, -2.3581,\n",
            "         -2.4633, -2.6067]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
            "Word 2 tensor([[-2.2497, -2.0978, -2.1015, -2.5749, -2.1509, -2.2803, -2.3242, -2.3564,\n",
            "         -2.4179, -2.6165]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQFL0an_dorj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, \n",
        "          encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients of both optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss = 0 # Added onto for each word\n",
        "\n",
        "    # Get size of input and target sentences\n",
        "    input_length = input_tensor.size()[0]\n",
        "    target_length = target_tensor.size()[0]\n",
        "\n",
        "    # Run words through encoder\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "    \n",
        "    # Prepare input and output variables\n",
        "    decoder_input = torch.LongTensor([[SOS_TOKEN]]).cuda()\n",
        "    decoder_context = torch.zeros(1, decoder.hidden_size).cuda()\n",
        "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
        "\n",
        "    # Choose whether to use teacher forcing\n",
        "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "    if use_teacher_forcing:\n",
        "        \n",
        "        # Teacher forcing: Use the ground-truth target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_context, decoder_hidden = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output[0].view(-1).unsqueeze(0), target_tensor[di].unsqueeze(0))\n",
        "            decoder_input = target_tensor[di] # Next target is next input\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use network's own prediction as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_context, decoder_hidden = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)           \n",
        "            loss += criterion(decoder_output[0].view(-1).unsqueeze(0), target_tensor[di].unsqueeze(0))\n",
        "            \n",
        "            # Get most likely word index (highest value) from output\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            ni = topi[0][0]\n",
        "            \n",
        "            decoder_input = torch.LongTensor([[ni]]).cuda() # Chosen word is next input\n",
        "\n",
        "            # Stop at end of sentence (not necessary when using known targets)\n",
        "            if ni == EOS_TOKEN: break\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sVCDg_3doro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 500\n",
        "embedding_size = 300\n",
        "n_layers = 2\n",
        "dropout_p = 0.05\n",
        "\n",
        "# Initialize models\n",
        "encoder = EncoderRNN(input_vocab.n_words, embedding_size, hidden_size, n_layers).cuda()\n",
        "decoder = AttnDecoderRNN(hidden_size, embedding_size, output_vocab.n_words, n_layers, dropout_p=dropout_p).cuda()\n",
        "\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0001)\n",
        "`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSbBSdXldorq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 50000\n",
        "print_every = 200\n",
        "\n",
        "print_loss_total = 0 # Reset every print_every\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LcrHj6HSdort",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4284
        },
        "outputId": "7c0209ec-c8c4-4dff-a3dc-8203219b688a"
      },
      "source": [
        "# Begin!\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    \n",
        "    # Get training data for this cycle\n",
        "    #training_pair = variables_from_pair(random.choice(pairs))\n",
        "    pair = random.choice(pairs)\n",
        "    input_tensor = torch.tensor(input_vocab.index_sentence(pair[0], write=False)).cuda()\n",
        "    target_tensor = torch.tensor(output_vocab.index_sentence(pair[1], write=False)).cuda()\n",
        "\n",
        "    # Run the train function\n",
        "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "\n",
        "    # Keep track of loss\n",
        "    print_loss_total += loss\n",
        "\n",
        "    if epoch == 0: continue\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print_loss_avg = print_loss_total / print_every\n",
        "        print_loss_total = 0\n",
        "        print('(Epoch %d) %.4f' % (epoch, print_loss_avg))\n",
        "        \n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Epoch 200) 7.3780\n",
            "(Epoch 400) 5.9807\n",
            "(Epoch 600) 5.8746\n",
            "(Epoch 800) 5.7627\n",
            "(Epoch 1000) 5.4878\n",
            "(Epoch 1200) 5.4703\n",
            "(Epoch 1400) 5.4229\n",
            "(Epoch 1600) 5.3556\n",
            "(Epoch 1800) 5.3679\n",
            "(Epoch 2000) 5.2351\n",
            "(Epoch 2200) 5.1931\n",
            "(Epoch 2400) 5.3306\n",
            "(Epoch 2600) 5.1281\n",
            "(Epoch 2800) 5.1100\n",
            "(Epoch 3000) 5.0842\n",
            "(Epoch 3200) 4.9742\n",
            "(Epoch 3400) 5.0244\n",
            "(Epoch 3600) 4.9717\n",
            "(Epoch 3800) 4.9417\n",
            "(Epoch 4000) 5.1147\n",
            "(Epoch 4200) 5.0232\n",
            "(Epoch 4400) 5.0495\n",
            "(Epoch 4600) 4.9528\n",
            "(Epoch 4800) 4.7680\n",
            "(Epoch 5000) 4.9683\n",
            "(Epoch 5200) 4.9016\n",
            "(Epoch 5400) 4.9039\n",
            "(Epoch 5600) 4.8233\n",
            "(Epoch 5800) 4.7177\n",
            "(Epoch 6000) 4.7143\n",
            "(Epoch 6200) 4.8374\n",
            "(Epoch 6400) 4.7655\n",
            "(Epoch 6600) 4.7540\n",
            "(Epoch 6800) 4.7064\n",
            "(Epoch 7000) 4.7293\n",
            "(Epoch 7200) 4.7990\n",
            "(Epoch 7400) 4.7441\n",
            "(Epoch 7600) 4.6468\n",
            "(Epoch 7800) 4.6972\n",
            "(Epoch 8000) 4.9011\n",
            "(Epoch 8200) 4.8244\n",
            "(Epoch 8400) 4.7138\n",
            "(Epoch 8600) 4.7208\n",
            "(Epoch 8800) 4.7072\n",
            "(Epoch 9000) 4.7683\n",
            "(Epoch 9200) 4.5813\n",
            "(Epoch 9400) 4.6413\n",
            "(Epoch 9600) 4.6094\n",
            "(Epoch 9800) 4.5139\n",
            "(Epoch 10000) 4.6866\n",
            "(Epoch 10200) 4.5908\n",
            "(Epoch 10400) 4.5917\n",
            "(Epoch 10600) 4.5453\n",
            "(Epoch 10800) 4.5562\n",
            "(Epoch 11000) 4.6343\n",
            "(Epoch 11200) 4.5799\n",
            "(Epoch 11400) 4.6460\n",
            "(Epoch 11600) 4.6112\n",
            "(Epoch 11800) 4.5911\n",
            "(Epoch 12000) 4.3819\n",
            "(Epoch 12200) 4.5438\n",
            "(Epoch 12400) 4.3209\n",
            "(Epoch 12600) 4.6112\n",
            "(Epoch 12800) 4.5059\n",
            "(Epoch 13000) 4.5119\n",
            "(Epoch 13200) 4.4253\n",
            "(Epoch 13400) 4.5769\n",
            "(Epoch 13600) 4.3035\n",
            "(Epoch 13800) 4.4350\n",
            "(Epoch 14000) 4.4031\n",
            "(Epoch 14200) 4.4327\n",
            "(Epoch 14400) 4.5189\n",
            "(Epoch 14600) 4.3982\n",
            "(Epoch 14800) 4.5806\n",
            "(Epoch 15000) 4.5442\n",
            "(Epoch 15200) 4.3380\n",
            "(Epoch 15400) 4.2743\n",
            "(Epoch 15600) 4.4204\n",
            "(Epoch 15800) 4.3823\n",
            "(Epoch 16000) 4.3392\n",
            "(Epoch 16200) 4.3718\n",
            "(Epoch 16400) 4.2906\n",
            "(Epoch 16600) 4.2461\n",
            "(Epoch 16800) 4.4796\n",
            "(Epoch 17000) 4.4522\n",
            "(Epoch 17200) 4.2340\n",
            "(Epoch 17400) 4.3213\n",
            "(Epoch 17600) 4.3392\n",
            "(Epoch 17800) 4.2907\n",
            "(Epoch 18000) 4.3442\n",
            "(Epoch 18200) 4.1310\n",
            "(Epoch 18400) 4.2827\n",
            "(Epoch 18600) 4.3761\n",
            "(Epoch 18800) 4.2895\n",
            "(Epoch 19000) 4.2680\n",
            "(Epoch 19200) 4.3223\n",
            "(Epoch 19400) 4.3185\n",
            "(Epoch 19600) 4.3328\n",
            "(Epoch 19800) 4.1493\n",
            "(Epoch 20000) 4.1310\n",
            "(Epoch 20200) 4.1752\n",
            "(Epoch 20400) 3.9753\n",
            "(Epoch 20600) 4.1478\n",
            "(Epoch 20800) 4.2271\n",
            "(Epoch 21000) 4.2258\n",
            "(Epoch 21200) 4.1837\n",
            "(Epoch 21400) 4.2140\n",
            "(Epoch 21600) 4.2392\n",
            "(Epoch 21800) 4.1883\n",
            "(Epoch 22000) 4.3276\n",
            "(Epoch 22200) 4.2411\n",
            "(Epoch 22400) 4.2471\n",
            "(Epoch 22600) 4.1816\n",
            "(Epoch 22800) 4.1924\n",
            "(Epoch 23000) 4.3645\n",
            "(Epoch 23200) 4.1397\n",
            "(Epoch 23400) 4.2418\n",
            "(Epoch 23600) 4.1977\n",
            "(Epoch 23800) 4.1388\n",
            "(Epoch 24000) 4.1123\n",
            "(Epoch 24200) 4.2894\n",
            "(Epoch 24400) 4.1706\n",
            "(Epoch 24600) 4.0913\n",
            "(Epoch 24800) 4.1949\n",
            "(Epoch 25000) 4.1866\n",
            "(Epoch 25200) 4.1678\n",
            "(Epoch 25400) 4.2551\n",
            "(Epoch 25600) 4.1335\n",
            "(Epoch 25800) 4.1128\n",
            "(Epoch 26000) 3.9874\n",
            "(Epoch 26200) 4.2793\n",
            "(Epoch 26400) 4.1220\n",
            "(Epoch 26600) 4.1703\n",
            "(Epoch 26800) 4.0684\n",
            "(Epoch 27000) 4.1531\n",
            "(Epoch 27200) 4.2072\n",
            "(Epoch 27400) 4.2747\n",
            "(Epoch 27600) 3.8817\n",
            "(Epoch 27800) 4.0568\n",
            "(Epoch 28000) 4.0067\n",
            "(Epoch 28200) 4.0538\n",
            "(Epoch 28400) 4.1147\n",
            "(Epoch 28600) 3.9676\n",
            "(Epoch 28800) 4.1270\n",
            "(Epoch 29000) 4.0678\n",
            "(Epoch 29200) 3.9515\n",
            "(Epoch 29400) 4.0666\n",
            "(Epoch 29600) 4.0306\n",
            "(Epoch 29800) 3.9033\n",
            "(Epoch 30000) 4.0543\n",
            "(Epoch 30200) 3.9148\n",
            "(Epoch 30400) 4.0652\n",
            "(Epoch 30600) 3.9296\n",
            "(Epoch 30800) 4.1704\n",
            "(Epoch 31000) 4.0736\n",
            "(Epoch 31200) 3.8859\n",
            "(Epoch 31400) 3.9718\n",
            "(Epoch 31600) 3.9526\n",
            "(Epoch 31800) 4.0617\n",
            "(Epoch 32000) 3.9093\n",
            "(Epoch 32200) 4.0217\n",
            "(Epoch 32400) 3.9315\n",
            "(Epoch 32600) 4.0986\n",
            "(Epoch 32800) 4.0170\n",
            "(Epoch 33000) 3.9155\n",
            "(Epoch 33200) 4.1246\n",
            "(Epoch 33400) 3.9358\n",
            "(Epoch 33600) 4.0020\n",
            "(Epoch 33800) 4.0446\n",
            "(Epoch 34000) 3.9191\n",
            "(Epoch 34200) 3.9831\n",
            "(Epoch 34400) 3.8377\n",
            "(Epoch 34600) 3.9646\n",
            "(Epoch 34800) 3.8057\n",
            "(Epoch 35000) 3.9744\n",
            "(Epoch 35200) 3.7998\n",
            "(Epoch 35400) 3.7945\n",
            "(Epoch 35600) 3.9746\n",
            "(Epoch 35800) 3.7140\n",
            "(Epoch 36000) 3.8958\n",
            "(Epoch 36200) 3.8896\n",
            "(Epoch 36400) 3.6735\n",
            "(Epoch 36600) 3.9285\n",
            "(Epoch 36800) 3.8620\n",
            "(Epoch 37000) 3.9210\n",
            "(Epoch 37200) 4.0338\n",
            "(Epoch 37400) 3.8770\n",
            "(Epoch 37600) 3.7684\n",
            "(Epoch 37800) 3.9854\n",
            "(Epoch 38000) 3.8742\n",
            "(Epoch 38200) 4.0034\n",
            "(Epoch 38400) 4.0325\n",
            "(Epoch 38600) 3.9570\n",
            "(Epoch 38800) 3.8946\n",
            "(Epoch 39000) 3.7552\n",
            "(Epoch 39200) 4.0719\n",
            "(Epoch 39400) 3.9383\n",
            "(Epoch 39600) 3.8940\n",
            "(Epoch 39800) 3.8352\n",
            "(Epoch 40000) 3.8226\n",
            "(Epoch 40200) 3.8375\n",
            "(Epoch 40400) 3.8227\n",
            "(Epoch 40600) 3.6833\n",
            "(Epoch 40800) 3.9354\n",
            "(Epoch 41000) 3.8900\n",
            "(Epoch 41200) 3.9280\n",
            "(Epoch 41400) 3.7580\n",
            "(Epoch 41600) 3.7600\n",
            "(Epoch 41800) 3.9024\n",
            "(Epoch 42000) 3.7728\n",
            "(Epoch 42200) 3.7244\n",
            "(Epoch 42400) 3.9159\n",
            "(Epoch 42600) 3.7872\n",
            "(Epoch 42800) 3.8629\n",
            "(Epoch 43000) 3.7646\n",
            "(Epoch 43200) 3.7024\n",
            "(Epoch 43400) 3.8943\n",
            "(Epoch 43600) 3.6621\n",
            "(Epoch 43800) 3.5757\n",
            "(Epoch 44000) 3.9205\n",
            "(Epoch 44200) 3.7986\n",
            "(Epoch 44400) 3.7719\n",
            "(Epoch 44600) 3.6719\n",
            "(Epoch 44800) 3.9402\n",
            "(Epoch 45000) 3.6706\n",
            "(Epoch 45200) 3.7253\n",
            "(Epoch 45400) 3.6863\n",
            "(Epoch 45600) 3.5951\n",
            "(Epoch 45800) 3.6940\n",
            "(Epoch 46000) 3.6053\n",
            "(Epoch 46200) 3.8091\n",
            "(Epoch 46400) 3.7645\n",
            "(Epoch 46600) 3.7047\n",
            "(Epoch 46800) 3.5958\n",
            "(Epoch 47000) 3.7395\n",
            "(Epoch 47200) 3.7426\n",
            "(Epoch 47400) 3.7270\n",
            "(Epoch 47600) 3.6516\n",
            "(Epoch 47800) 3.7072\n",
            "(Epoch 48000) 3.7183\n",
            "(Epoch 48200) 3.6840\n",
            "(Epoch 48400) 3.8458\n",
            "(Epoch 48600) 3.7854\n",
            "(Epoch 48800) 3.6663\n",
            "(Epoch 49000) 3.8464\n",
            "(Epoch 49200) 3.7193\n",
            "(Epoch 49400) 3.7896\n",
            "(Epoch 49600) 3.6418\n",
            "(Epoch 49800) 3.7130\n",
            "(Epoch 50000) 3.7048\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivPXrB-idor1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence, max_length=MAX_LENGTH):\n",
        "    input_tensor = torch.tensor(input_vocab.index_sentence(sentence)).cuda()\n",
        "    input_length = input_tensor.size()[0]\n",
        "    \n",
        "    # Run through encoder\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "    # Create starting vectors for decoder\n",
        "    decoder_input = torch.LongTensor([[SOS_TOKEN]]).cuda() # SOS\n",
        "    decoder_context = torch.zeros(1, decoder.hidden_size).cuda()\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    decoded_words = []\n",
        "    decoder_attentions = torch.zeros(max_length, max_length).cuda()\n",
        "    \n",
        "    # Run through decoder\n",
        "    for di in range(max_length):\n",
        "        decoder_output, decoder_context, decoder_hidden = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
        "        # Choose top word from output\n",
        "        topv, topi = decoder_output.cpu().topk(1)\n",
        "        ni = topi[0][0].item()\n",
        "        if ni == EOS_TOKEN:\n",
        "            decoded_words.append('<EOS>')\n",
        "            break\n",
        "        else:\n",
        "            decoded_words.append(output_vocab.index2word[ni])\n",
        "            \n",
        "        # Next input is chosen word\n",
        "        decoder_input = torch.LongTensor([[ni]]).cuda()\n",
        "    \n",
        "    return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExP97mC2dor5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_randomly():\n",
        "    pair = random.choice(pairs)\n",
        "    \n",
        "    output_words = evaluate(pair[0])\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    \n",
        "    print(pair[0])\n",
        "    print(pair[1])\n",
        "    print(output_sentence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7xeTa0Ydor7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "45d55d4c-d78c-4ba5-847f-c1435c6b87e8"
      },
      "source": [
        "for i in range(5):\n",
        "    evaluate_randomly()\n",
        "    print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "תחכו עד מחר בבוקר .\n",
            "wait until tomorrow morning .\n",
            "i tomorrow tomorrow tomorrow . tomorrow . . . .\n",
            "\n",
            "\n",
            "אתם יכולים ללכת במקומי בבקשה ?\n",
            "can you please go for me ?\n",
            "can you please please please please ? ? ? ?\n",
            "\n",
            "\n",
            "כבר ראיתי אותו פעם .\n",
            "i have seen him once .\n",
            "i have already to him . . . . .\n",
            "\n",
            "\n",
            "לך ודבר עם טום .\n",
            "go and talk to tom .\n",
            "you you have to tom . . . . .\n",
            "\n",
            "\n",
            "אעדיף בהרבה להיות בבית .\n",
            "i d much rather be at home .\n",
            "you must be home to be home . . .\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}