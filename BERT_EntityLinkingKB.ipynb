{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_EntityLinkingKB",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronenbendavid/IDC_NLP/blob/master/BERT_EntityLinkingKB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-WJBimYDLJS",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 3\n",
        "Training a neural named entity recognition (NER) tagger "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3enPCGBF8FlX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1b5d17c-2adf-4e8b-ed1c-4916c4ce1b8f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('version: {}, device: {}'.format(torch.__version__, device))"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "version: 1.6.0+cu101, device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5QSIEoyDdWh",
        "colab_type": "text"
      },
      "source": [
        "In this assignment you are required to build a full training and testing pipeline for a neural sequentail tagger for named entities, using LSTM.\n",
        "\n",
        "The dataset that you will be working on is called ReCoNLL 2003, which is a corrected version of the CoNLL 2003 dataset: https://www.clips.uantwerpen.be/conll2003/ner/\n",
        "\n",
        "[Train data](https://drive.google.com/file/d/1hG66e_OoezzeVKho1w7ysyAx4yp0ShDz/view?usp=sharing)\n",
        "\n",
        "[Dev data](https://drive.google.com/file/d/1EAF-VygYowU1XknZhvzMi2CID65I127L/view?usp=sharing)\n",
        "\n",
        "[Test data](https://drive.google.com/file/d/16gug5wWnf06JdcBXQbcICOZGZypgr4Iu/view?usp=sharing)\n",
        "\n",
        "As you can see, the annotated texts are labeled according to the IOB annotation scheme, for 3 entity types: Person, Organization, Location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ul2Y3vuPoV8",
        "colab_type": "text"
      },
      "source": [
        "**Task 1:** Write a funtion for reading the data from a single file (of the ones that are provided above). The function recieves a filepath and then it encodes every sentence individually using a pair of lists, one list contains the words and one list contains the tags. Each list pair will be added to a general list (data), which will be returned back from the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prgzgtt8Jw4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "23e2acb6-5e02-4889-c81a-daf40a50bf6d"
      },
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "def read_data(filepath):\n",
        "    data = []\n",
        "\n",
        "    result = re.compile(\".*drive.google.com/file/d/([^/]*)/.*\").match(filepath)\n",
        "    if result:\n",
        "      filepath = 'https://docs.google.com/uc?export=download&id={}'.format(result.group(1))\n",
        "    print(filepath)\n",
        "\n",
        "    response = requests.get(filepath)\n",
        "    words = []\n",
        "    tags = []\n",
        "\n",
        "    for line in response.text.split('\\n'):\n",
        "        if not line:\n",
        "            if len(words) > 0:\n",
        "                data.append((words, tags))\n",
        "            words = []\n",
        "            tags = []\n",
        "        else:\n",
        "            line = line.strip().split()\n",
        "            words.append(line[0].lower())\n",
        "            tags.append(line[1])\n",
        "\n",
        "    return data\n",
        "\n",
        "train = read_data('https://drive.google.com/file/d/1hG66e_OoezzeVKho1w7ysyAx4yp0ShDz/view?usp=sharing')\n",
        "dev = read_data('https://drive.google.com/file/d/1EAF-VygYowU1XknZhvzMi2CID65I127L/view?usp=sharing')\n",
        "test = read_data('https://drive.google.com/file/d/16gug5wWnf06JdcBXQbcICOZGZypgr4Iu/view?usp=sharing')\n"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://docs.google.com/uc?export=download&id=1hG66e_OoezzeVKho1w7ysyAx4yp0ShDz\n",
            "https://docs.google.com/uc?export=download&id=1EAF-VygYowU1XknZhvzMi2CID65I127L\n",
            "https://docs.google.com/uc?export=download&id=16gug5wWnf06JdcBXQbcICOZGZypgr4Iu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuGwk6OwRWGS",
        "colab_type": "text"
      },
      "source": [
        "The following Vocab class can be served as a dictionary that maps words and tags into Ids. The UNK_TOKEN should be used for words that are not part of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKIB5o_vQO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UNK_TOKEN = 0\n",
        "\n",
        "# class Vocab:\n",
        "#     def __init__(self):\n",
        "#         self.word2id = {\"__unk__\": UNK_TOKEN}\n",
        "#         self.id2word = {UNK_TOKEN: \"__unk__\"}\n",
        "#         self.n_words = 1\n",
        "        \n",
        "#         self.tag2id = {\"O\":0, \"B-PER\":1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4, \"B-ORG\": 5, \"I-ORG\": 6}\n",
        "#         self.id2tag = {0:\"O\", 1:\"B-PER\", 2:\"I-PER\", 3:\"B-LOC\", 4:\"I-LOC\", 5:\"B-ORG\", 6:\"I-ORG\"}\n",
        "        \n",
        "#     def index_words(self, words):\n",
        "#       word_indexes = [self.index_word(w) for w in words]\n",
        "#       return word_indexes\n",
        "\n",
        "#     def index_tags(self, tags):\n",
        "#       tag_indexes = [self.tag2id[t] for t in tags]\n",
        "#       return tag_indexes\n",
        "    \n",
        "#     def index_word(self, w):\n",
        "#         if w not in self.word2id:\n",
        "#             self.word2id[w] = self.n_words\n",
        "#             self.id2word[self.n_words] = w\n",
        "#             self.n_words += 1\n",
        "#         return self.word2id[w]\n",
        "            \n",
        "            "
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GlOg4zGpqRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08dbfc52-da87-46b7-aca1-c967051c2b84"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDKYryfKfNdh",
        "colab_type": "text"
      },
      "source": [
        "**Task 2:** Write a function prepare_data that takes one of the [train, dev, test] and the Vocab instance, for converting each pair of (words,tags) to a pair of indexes. Each pair should be added to data_sequences, which will be returned back from the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRY0ASF6lGk1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "df750758-c7d6-466c-fda1-985825a9304f"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import pickle\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "class Vocab:\n",
        "    def __init__(self, args=None):\n",
        "        self.tag2idx = None\n",
        "        self.idx2tag = None\n",
        "        self.OUTSIDE_ID = None\n",
        "        self.PAD_ID = None\n",
        "        self.SPECIAL_TOKENS = None\n",
        "        self.tokenizer = None\n",
        "        if args is not None:\n",
        "            self.load(args)\n",
        "\n",
        "    def load(self, args, popular_entity_to_id_dict=None):\n",
        "\n",
        "        if popular_entity_to_id_dict is None:\n",
        "            with open(f\"/content/drive/My Drive/data/versions/dummy/indexes/popular_entity_to_id_dict.pickle\", \"rb\") as f:\n",
        "              # with open(f\"/content/drive/My Drive/data/versions/dummy/indexes/mention_entity_counter.pickle\", \"rb\") as f:\n",
        "              \n",
        "                popular_entity_to_id_dict = pickle.load(f)\n",
        "\n",
        "        # if args.uncased:\n",
        "        if True:\n",
        "            tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "        else:\n",
        "            tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=False)\n",
        "\n",
        "        self.tag2idx = popular_entity_to_id_dict\n",
        "\n",
        "        self.OUTSIDE_ID = len(self.tag2idx)\n",
        "        self.tag2idx[\"|||O|||\"] = self.OUTSIDE_ID\n",
        "\n",
        "        self.PAD_ID = len(self.tag2idx)\n",
        "        self.tag2idx[\"|||PAD|||\"] = self.PAD_ID\n",
        "\n",
        "        self.SPECIAL_TOKENS = [self.OUTSIDE_ID, self.PAD_ID]\n",
        "\n",
        "        self.idx2tag = {v: k for k, v in self.tag2idx.items()}\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # args.vocab_size = self.size()\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.tag2idx)"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moRTnapWlTnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import subprocess\n",
        "from collections import Counter\n",
        "\n",
        "import torch.optim\n",
        "from torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def capitalize(text: str) -> str:\n",
        "    return text[0].upper() + text[1:]\n",
        "\n",
        "\n",
        "def snip(string, search, keep, keep_search):\n",
        "    pos = string.find(search)\n",
        "    if pos != -1:\n",
        "        if keep == \"left\":\n",
        "            if keep_search:\n",
        "                pos += len(search)\n",
        "            string = string[:pos]\n",
        "        if keep == \"right\":\n",
        "            if not keep_search:\n",
        "                pos += len(search)\n",
        "            string = string[pos:]\n",
        "    return string\n",
        "\n",
        "\n",
        "def snip_anchor(text: str) -> str:\n",
        "    return snip(text, \"#\", keep=\"left\", keep_search=False)\n",
        "\n",
        "\n",
        "def normalize_wiki_entity(i, replace_ws=False):\n",
        "    i = snip_anchor(i)\n",
        "    if len(i) == 0:\n",
        "        return None\n",
        "    i = capitalize(i)\n",
        "    if replace_ws:\n",
        "        return i.replace(\" \", \"_\")\n",
        "    return i\n",
        "\n",
        "\n",
        "# most frequent English words from English Wikipedia\n",
        "stopwords = {\n",
        "    \"a\",\n",
        "    \"also\",\n",
        "    \"an\",\n",
        "    \"are\",\n",
        "    \"as\",\n",
        "    \"at\",\n",
        "    \"be\",\n",
        "    \"by\",\n",
        "    \"city\",\n",
        "    \"company\",\n",
        "    \"film\",\n",
        "    \"first\",\n",
        "    \"for\",\n",
        "    \"from\",\n",
        "    \"had\",\n",
        "    \"has\",\n",
        "    \"her\",\n",
        "    \"his\",\n",
        "    \"in\",\n",
        "    \"is\",\n",
        "    \"its\",\n",
        "    \"john\",\n",
        "    \"national\",\n",
        "    \"new\",\n",
        "    \"of\",\n",
        "    \"on\",\n",
        "    \"one\",\n",
        "    \"people\",\n",
        "    \"school\",\n",
        "    \"state\",\n",
        "    \"the\",\n",
        "    \"their\",\n",
        "    \"these\",\n",
        "    \"this\",\n",
        "    \"time\",\n",
        "    \"to\",\n",
        "    \"two\",\n",
        "    \"university\",\n",
        "    \"was\",\n",
        "    \"were\",\n",
        "    \"with\",\n",
        "    \"world\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_stopwordless_token_set(s):\n",
        "    result = set(s.lower().split(\" \"))\n",
        "    result_minus_stopwords = result.difference(stopwords)\n",
        "    if len(result_minus_stopwords) == 0:\n",
        "        return result\n",
        "    else:\n",
        "        return result_minus_stopwords\n",
        "\n",
        "\n",
        "def argparse_bool_type(v):\n",
        "    \"Type for argparse that correctly treats Boolean values\"\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n",
        "        return True\n",
        "    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n",
        "\n",
        "\n",
        "def get_gpu_memory_map():\n",
        "    \"\"\"Get the current gpu usage.\n",
        "    Returns\n",
        "    -------\n",
        "    usage: dict\n",
        "        Keys are device ids as integers.\n",
        "        Values are memory usage as integers in MB.\n",
        "    \"\"\"\n",
        "    result = subprocess.check_output(\n",
        "        [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,nounits,noheader\"], encoding=\"utf-8\"\n",
        "    )\n",
        "    # Convert lines into a dictionary\n",
        "    gpu_memory = [int(x) for x in result.strip().split(\"\\n\")]\n",
        "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
        "    return gpu_memory_map\n",
        "\n",
        "\n",
        "def create_chunks(a_list, n):\n",
        "    for i in range(0, len(a_list), n):\n",
        "        yield a_list[i : i + n]\n",
        "\n",
        "\n",
        "def unescape(s):\n",
        "    if s.startswith('\"'):\n",
        "        s = s[1:-1]\n",
        "    return s.replace('\"\"\"\"', '\"').replace('\"\"', '\"')\n",
        "\n",
        "\n",
        "def create_overlapping_chunks(a_list, n, overlap):\n",
        "    for i in range(0, len(a_list), n - overlap):\n",
        "        yield a_list[i : i + n]\n",
        "\n",
        "\n",
        "def running_mean(new, old=None, momentum=0.9):\n",
        "    if old is None:\n",
        "        return new\n",
        "    else:\n",
        "        return momentum * old + (1 - momentum) * new\n",
        "\n",
        "\n",
        "def get_topk_ids_aggregated_from_seq_prediction(logits, topk_per_token, topk_from_batch):\n",
        "    topk_logit_per_token, topk_eids_per_token = logits.topk(topk_per_token, sorted=False, dim=-1)\n",
        "\n",
        "    i = torch.cat(\n",
        "        [\n",
        "            topk_eids_per_token.view(1, -1),\n",
        "            torch.zeros(topk_eids_per_token.view(-1).size(), dtype=torch.long, device=topk_eids_per_token.device).view(\n",
        "                1, -1\n",
        "            ),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "    v = topk_logit_per_token.view(-1)\n",
        "    st = torch.sparse.FloatTensor(i, v)\n",
        "    stc = st.coalesce()\n",
        "    topk_indices = stc._values().sort(descending=True)[1][:topk_from_batch]\n",
        "    result = stc._indices()[0, topk_indices]\n",
        "\n",
        "    return result.cpu().tolist()\n",
        "\n",
        "\n",
        "def get_entity_annotations(t, outside_id):\n",
        "    annos = list()\n",
        "    begin = -1\n",
        "    in_entity = -1\n",
        "    for i, j in enumerate(t):\n",
        "        if j < outside_id and begin == -1:\n",
        "            begin = i\n",
        "            in_entity = j.item()\n",
        "        elif j < outside_id and j != in_entity:\n",
        "            annos.append((tuple(range(begin, i)), in_entity))\n",
        "            begin = i\n",
        "            in_entity = j.item()\n",
        "        elif j == outside_id and begin != -1:\n",
        "            annos.append((tuple(range(begin, i)), in_entity))\n",
        "            begin = -1\n",
        "    return annos\n",
        "\n",
        "\n",
        "def get_entity_annotations_with_gold_spans(t, t_gold, outside_id):\n",
        "    annos = list()\n",
        "    begin = -1\n",
        "    in_gold_entity = -1\n",
        "    collected_entities_in_span = Counter()\n",
        "    for i, (j, j_gold) in enumerate(zip(t, t_gold)):\n",
        "        if j_gold < outside_id and begin == -1:\n",
        "            begin = i\n",
        "            in_gold_entity = j_gold.item()\n",
        "            collected_entities_in_span[j.item()] += 1\n",
        "        elif j_gold != in_gold_entity and begin != -1:\n",
        "            in_entity = collected_entities_in_span.most_common()[0][0]\n",
        "            annos.append((tuple(range(begin, i)), in_entity))\n",
        "            collected_entities_in_span = Counter()\n",
        "            begin = i\n",
        "            in_gold_entity = j_gold.item()\n",
        "            collected_entities_in_span[j.item()] += 1\n",
        "        elif j_gold == outside_id and begin != -1:\n",
        "            in_entity = collected_entities_in_span.most_common()[0][0]\n",
        "            annos.append((tuple(range(begin, i)), in_entity))\n",
        "            collected_entities_in_span = Counter()\n",
        "            begin = -1\n",
        "    return annos\n",
        "\n",
        "\n",
        "class DummyOptimizer(torch.optim.Optimizer):\n",
        "    def step(self, closure=None):\n",
        "        pass\n",
        "\n",
        "\n",
        "class LRMilestones(_LRScheduler):\n",
        "    \"\"\"Set the learning rate of each parameter group to the initial lr decayed\n",
        "    by gamma once the number of epoch reaches one of the milestones. When\n",
        "    last_epoch=-1, sets initial lr as lr.\n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        milestones (list): List of epoch indices. Must be increasing.\n",
        "        gamma (float): Multiplicative factor of learning rate decay.\n",
        "            Default: 0.1.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "    Example:\n",
        "        >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
        "        >>> # lr = 0.05     if epoch < 30\n",
        "        >>> # lr = 0.005    if 30 <= epoch < 80\n",
        "        >>> # lr = 0.0005   if epoch >= 80\n",
        "        >>> scheduler = LRMilestones(optimizer, milestones=[(30, 0.1), (80, 0.2), ])\n",
        "        >>> for epoch in range(100):\n",
        "        >>>     scheduler.step()\n",
        "        >>>     train(...)\n",
        "        >>>     validate(...)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, milestones, last_epoch=-1):\n",
        "        super().__init__(optimizer)\n",
        "        if not list(milestones) == sorted(milestones):\n",
        "            raise ValueError(\"Milestones should be a list of\" \" increasing integers. Got {}\", milestones)\n",
        "        self.milestones = milestones\n",
        "        super(LRMilestones, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        for ep, lr in self.milestones:\n",
        "            if self.last_epoch >= ep:\n",
        "                print(\"Set lr to {} in epoch {}\".format(lr, ep))\n",
        "                return lr\n",
        "\n",
        "\n",
        "def pad_to(arr, max_len, pad_id, cls_id, sep_id):\n",
        "    return [cls_id] + arr + [sep_id] + [pad_id] * (max_len - len(arr) - 2)\n",
        "\n",
        "\n",
        "def set_out_id(t, repl, dummy=-1):\n",
        "    t[(t == dummy)] = repl\n",
        "    return t\n",
        "\n",
        "\n",
        "class LRSchedulers:\n",
        "    ReduceLROnPlateau = ReduceLROnPlateau\n",
        "    LRMilestones = LRMilestones"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIHNAiDDbpvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "import pandas\n",
        "from itertools import cycle\n",
        "from operator import gt, lt\n",
        "\n",
        "\n",
        "class Metrics:\n",
        "\n",
        "    meta = OrderedDict(\n",
        "        [\n",
        "            (\"epoch\", {\"comp\": gt, \"type\": int, \"str\": lambda a: a}),\n",
        "            (\"step\", {\"comp\": gt, \"type\": int, \"str\": lambda a: a}),\n",
        "            (\"num_gold\", {\"comp\": gt, \"type\": int, \"str\": lambda a: a}),\n",
        "            (\"num_correct\", {\"comp\": gt, \"type\": int, \"str\": lambda a: a}),\n",
        "            (\"num_proposed\", {\"comp\": gt, \"type\": int, \"str\": lambda a: a}),\n",
        "            (\"f1\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"f05\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"precision\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"recall\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"span_f1\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"span_precision\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"span_recall\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"lenient_span_f1\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"lenient_span_precision\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"lenient_span_recall\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"precision_gold_mentions\", {\"comp\": gt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "            (\"avg_loss\", {\"comp\": lt, \"type\": float, \"str\": lambda a: f\"{a:.5f}\"}),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    def __init__(self, epoch=0, step=0, num_correct=0, num_gold=0, num_proposed=0, model_selection=\"f1\", num_best_checkpoints=4, **kwargs):\n",
        "\n",
        "        self.epoch = epoch\n",
        "        self.step = step\n",
        "        self.num_correct = num_correct\n",
        "        self.num_gold = num_gold\n",
        "        self.num_proposed = num_proposed\n",
        "\n",
        "        self.precision = Metrics.compute_precision(num_correct, num_proposed)\n",
        "        self.recall = Metrics.compute_recall(num_correct, num_gold)\n",
        "        self.f1 = Metrics.compute_fmeasure(self.precision, self.recall)\n",
        "        self.f05 = Metrics.compute_fmeasure(self.precision, self.recall, weight=1.5)\n",
        "\n",
        "        self.avg_loss = float(\"inf\")\n",
        "\n",
        "        for k,v in kwargs.items():\n",
        "            if k in self.meta:\n",
        "                self.__dict__[k] = v\n",
        "\n",
        "        self.model_selection = model_selection\n",
        "        self.checkpoint_cycle = cycle(range(num_best_checkpoints),)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_precision(correct, proposed):\n",
        "        try:\n",
        "            precision = correct/proposed\n",
        "        except ZeroDivisionError:\n",
        "            precision = 0.0\n",
        "        return precision\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_recall(correct, gold):\n",
        "        try:\n",
        "            recall = correct/ gold\n",
        "        except ZeroDivisionError:\n",
        "            recall = 0.0\n",
        "        return recall\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_fmeasure(precision, recall, weight=2.0):\n",
        "        try:\n",
        "            f = weight * precision * recall / (precision + recall)\n",
        "        except ZeroDivisionError:\n",
        "            f = 0.0\n",
        "        return f\n",
        "\n",
        "    def was_improved(self, other: \"Metrics\"):\n",
        "        return Metrics.meta[self.model_selection][\"comp\"](\n",
        "            other.get_model_selection_metric(), self.get_model_selection_metric()\n",
        "        )\n",
        "\n",
        "    def update(self, other: \"Metrics\"):\n",
        "        if self.was_improved(other):\n",
        "            for key, val in other.__dict__.items():\n",
        "                self.__setattr__(key, other.__dict__.get(key))\n",
        "\n",
        "    def get_model_selection_metric(self):\n",
        "        return self.__dict__.get(self.model_selection)\n",
        "\n",
        "    def get_best_checkpoint_filename(self):\n",
        "        return f\"best_{self.model_selection}-{next(self.checkpoint_cycle)}\"\n",
        "\n",
        "    def to_csv(self, epoch, step, args):\n",
        "        header = (\n",
        "            [k for k in list(self.meta.keys()) if k in self.__dict__]\n",
        "            if not os.path.exists(\"{}/log.csv\".format(args.logdir))\n",
        "            else False\n",
        "        )\n",
        "        pandas.DataFrame(\n",
        "            [[self.__dict__[k] for k in list(self.meta.keys()) if k in self.__dict__]]\n",
        "        ).to_csv(f\"{args.logdir}/log.csv\", mode=\"a\", header=header)\n",
        "\n",
        "    def dict(self):\n",
        "        return self.__dict__\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.__dict__)\n",
        "\n",
        "    def report(self, filter=None):\n",
        "        if not filter:\n",
        "            filter = set(self.meta.keys())\n",
        "        return [f\"{k}: {Metrics.meta[k]['str'](self.__dict__[k])}\" for k in list(self.meta.keys()) if k in filter and k in self.__dict__]"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBTsGEJuCJ8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "9cb67b40-29e7-4936-b701-d43f5f0dbbdd"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "from itertools import chain\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "from torch import optim\n",
        "from tqdm import trange\n",
        "# from metrics import Metrics\n",
        "# !pip install metrics  metrics.bumpversion metrics.gitinfo metrics.pylint metrics.pytest-cov\n",
        "!pip install pytorch_pretrained_bert\n",
        "# from data_loader_wiki import EDLDataset_collate_func\n",
        "# from skmisc import running_mean, get_topk_ids_aggregated_from_seq_prediction, DummyOptimizer, LRSchedulers\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "\n",
        "# Metric = METRICS_FILENAME"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acvM3qDsWZM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AlbertConfig, AlbertModel\n",
        "# Initializing an ALBERT-xxlarge style configuration\n",
        "albert_xxlarge_configuration = AlbertConfig()\n",
        "\n",
        "# Initializing an ALBERT-base style configuration\n",
        "albert_base_configuration = AlbertConfig(hidden_size=768,num_attention_heads=12,intermediate_size=3072 )\n",
        "\n",
        ">>> # Initializing a model from the ALBERT-base style configuration\n",
        ">>> model = AlbertModel(albert_xxlarge_configuration)\n",
        "\n",
        ">>> # Accessing the model configuration\n",
        ">>> configuration = model.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UccfiRRtiEet",
        "colab_type": "text"
      },
      "source": [
        "**Task 3:** Write NERNet, a PyTorch Module for labeling words with NER tags. \n",
        "\n",
        "*input_size:* the size of the vocabulary\n",
        "\n",
        "*embedding_size:* the size of the embeddings\n",
        "\n",
        "*hidden_size:* the LSTM hidden size\n",
        "\n",
        "*output_size:* the number tags we are predicting for\n",
        "\n",
        "*n_layers:* the number of layers we want to use in LSTM\n",
        "\n",
        "*directions:* could 1 or 2, indicating unidirectional or bidirectional LSTM, respectively\n",
        "\n",
        "The input for your forward function should be a single sentence tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UeerijSCLMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke1LyUQNyQaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(\n",
        "        self, args, vocab_size=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # if args.uncased:\n",
        "        if True:\n",
        "            self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        else:\n",
        "            self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "        # self.top_rnns = args.top_rnns\n",
        "        self.top_rnns = False\n",
        "        if self.top_rnns:\n",
        "        # if args.top_rnns:\n",
        "          self.rnn = nn.LSTM(bidirectional=True, num_layers=2, input_size=768, hidden_size=768 // 2, batch_first=True)\n",
        "        self.fc = None\n",
        "        # if args.project:\n",
        "        #     self.fc = nn.Linear(768, args.entity_embedding_size)\n",
        "        entity_embedding_size = 768\n",
        "        asparse = True\n",
        "        self.fc = nn.Linear(768, entity_embedding_size)\n",
        "        self.out = nn.Embedding(num_embeddings=vocab_size, embedding_dim=entity_embedding_size, sparse=asparse)\n",
        "        # torch.nn.init.normal_(self.out, std=0.1)\n",
        "        finetuning = 3\n",
        "        # self.device = args.device\n",
        "        # self.out_device = args.out_device\n",
        "        # self.finetuning = args.finetuning == 0\n",
        "        self.device = 0\n",
        "        self.out_device = 0\n",
        "        self.finetuning = finetuning == 0\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def to(self, device, out_device):\n",
        "        self.bert.to(device)\n",
        "        if self.fc:\n",
        "            self.fc.to(device)\n",
        "        self.out.to(out_device)\n",
        "        self.device = device\n",
        "        self.out_device = out_device\n",
        "\n",
        "    def forward(self, x, y=None, probs=None, enc=None):\n",
        "        \"\"\"\n",
        "        x: (N, T). int64\n",
        "        y: (N, T). int64\n",
        "\n",
        "        Returns\n",
        "        enc: (N, T, VOCAB)\n",
        "        \"\"\"\n",
        "        if y is not None:\n",
        "            y = y.to(self.out_device)\n",
        "        if probs is not None:\n",
        "            probs = probs.to(self.out_device)\n",
        "\n",
        "            # fake_y = torch.Tensor(range(10)).long().to(self.device)\n",
        "\n",
        "        if enc is None:\n",
        "            x = x.to(self.device)\n",
        "            if self.training:\n",
        "                if self.finetuning:\n",
        "                    # print(\"->bert.train()\")\n",
        "                    self.bert.train()\n",
        "                    encoded_layers, _ = self.bert(x)\n",
        "                    enc = encoded_layers[-1]\n",
        "                else:\n",
        "                    self.bert.eval()\n",
        "                    with torch.no_grad():\n",
        "                        encoded_layers, _ = self.bert(x)\n",
        "                        enc = encoded_layers[-1]\n",
        "            else:\n",
        "                encoded_layers, _ = self.bert(x)\n",
        "                enc = encoded_layers[-1]\n",
        "\n",
        "            if self.top_rnns:\n",
        "                enc, _ = self.rnn(enc)\n",
        "\n",
        "            if self.fc:\n",
        "                enc = self.fc(enc)\n",
        "\n",
        "            enc = enc.to(self.out_device)\n",
        "\n",
        "        if y is not None:\n",
        "            out = self.out(y)\n",
        "            logits = enc.matmul(out.transpose(0, 1))\n",
        "            y_hat = logits.argmax(-1)\n",
        "            return logits, y, y_hat, probs, out, enc\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                out = self.out.weight\n",
        "                logits = enc.matmul(out.transpose(0, 1))\n",
        "                y_hat = logits.argmax(-1)\n",
        "                return logits, None, y_hat, None, None, enc\n",
        "\n",
        "    @staticmethod\n",
        "    def train_one_epoch(\n",
        "        args,\n",
        "        model,\n",
        "        train_iter,\n",
        "        optimizers,\n",
        "        criterion,\n",
        "        eval_iter,\n",
        "        vocab,\n",
        "        epoch,\n",
        "        # metrics=Metrics(),\n",
        "        metrics = None,\n",
        "        loss_aggr=None,\n",
        "    ):\n",
        "        labels_with_high_model_score = None\n",
        "\n",
        "        with trange(len(train_iter)) as t:\n",
        "            for iter, batch in enumerate(train_iter):\n",
        "\n",
        "                model.to(\n",
        "                    args.device, args.out_device,\n",
        "                )\n",
        "                model.train()\n",
        "\n",
        "                batch_token_ids, label_ids, label_probs, eval_mask, _, _, orig_batch, loaded_batch = batch\n",
        "\n",
        "                enc = None\n",
        "\n",
        "                if (\n",
        "                    args.collect_most_popular_labels_steps is not None\n",
        "                    and args.collect_most_popular_labels_steps > 0\n",
        "                    and iter > 0\n",
        "                    and iter % args.collect_most_popular_labels_steps == 0\n",
        "                ):\n",
        "                    model.to(args.device, args.eval_device)\n",
        "                    with torch.no_grad():\n",
        "                        logits_, _, _, _, _, enc = model(\n",
        "                            batch_token_ids, None, None,\n",
        "                        )  # logits: (N, T, VOCAB), y: (N, T)\n",
        "                        labels_with_high_model_score = get_topk_ids_aggregated_from_seq_prediction(\n",
        "                            logits_, topk_from_batch=args.label_size, topk_per_token=args.topk_neg_examples\n",
        "                        )\n",
        "                        batch_token_ids, label_ids, label_probs, eval_mask, _, _, _, _ = EDLDataset_collate_func(\n",
        "                            args=args,\n",
        "                            labels_with_high_model_score=labels_with_high_model_score,\n",
        "                            batch=orig_batch,\n",
        "                            return_labels=True,\n",
        "                            vocab=vocab,\n",
        "                            is_training=False,\n",
        "                            loaded_batch=loaded_batch,\n",
        "                        )\n",
        "\n",
        "                # if args.label_size is not None:\n",
        "                logits, y, y_hat, label_probs, sparse_params, _ = model(\n",
        "                    batch_token_ids, label_ids, label_probs, enc=enc\n",
        "                )  # logits: (N, T, VOCAB), y: (N, T)\n",
        "                logits = logits.view(-1)  # (N*T, VOCAB)\n",
        "                label_probs = label_probs.view(-1)  # (N*T,)\n",
        "\n",
        "                loss = criterion(logits, label_probs)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                if (iter + 1) % args.accumulate_batch_gradients == 0:\n",
        "                    for optimizer in optimizers:\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                if iter == 0:\n",
        "                    logging.debug(f\"Sanity check\")\n",
        "                    logging.debug(\"x:\", batch_token_ids.cpu().numpy()[0])\n",
        "                    logging.debug(\"tokens:\", vocab.tokenizer.convert_ids_to_tokens(batch_token_ids.cpu().numpy()[0]))\n",
        "                    logging.debug(\"y:\", label_probs.cpu().numpy()[0])\n",
        "\n",
        "                loss_aggr = running_mean(loss.detach().item(), loss_aggr)\n",
        "\n",
        "                if iter > 0 and iter % args.checkpoint_eval_steps == 0:\n",
        "                    metrics = Net.evaluate(\n",
        "                        args=args,\n",
        "                        model=model,\n",
        "                        iterator=eval_iter,\n",
        "                        optimizers=optimizers,\n",
        "                        step=iter,\n",
        "                        epoch=epoch,\n",
        "                        save_checkpoint=iter % args.checkpoint_save_steps == 0,\n",
        "                        sampled_evaluation=False,\n",
        "                        metrics=metrics,\n",
        "                        vocab=vocab,\n",
        "                    )\n",
        "\n",
        "                t.set_postfix(\n",
        "                    loss=loss_aggr,\n",
        "                    nr_labels=len(label_ids),\n",
        "                    aggr_labels=len(labels_with_high_model_score) if labels_with_high_model_score else 0,\n",
        "                    last_eval=metrics.report(filter={\"f1\", \"num_proposed\", \"epoch\", \"step\"}),\n",
        "                )\n",
        "                t.update()\n",
        "\n",
        "        for optimizer in optimizers:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    @staticmethod\n",
        "    # def evaluate(\n",
        "    #     args,\n",
        "    #     model,\n",
        "    #     iterator,\n",
        "    #     vocab,\n",
        "    #     optimizers,\n",
        "    #     step=0,\n",
        "    #     epoch=0,\n",
        "    #     save_checkpoint=True,\n",
        "    #     save_predictions=True,\n",
        "    #     save_csv=True,\n",
        "    #     sampled_evaluation=False,\n",
        "    #     metrics = None\n",
        "    #     # metrics=Metrics(),\n",
        "    # ):\n",
        "\n",
        "    #     print()\n",
        "    #     logging.info(f\"Start evaluation on split {'test' if args_eval_on_test_only else 'valid'}\")\n",
        "\n",
        "    #     model.eval()\n",
        "    #     model.to(args.device, args.eval_device)\n",
        "\n",
        "    #     all_words, all_tags, all_y, all_y_hat, all_predicted, all_token_ids = [], [], [], [], [], []\n",
        "    #     with torch.no_grad():\n",
        "    #         for iter, batch in enumerate(tqdm.tqdm(iterator)):\n",
        "    #             (\n",
        "    #                 batch_token_ids,\n",
        "    #                 label_ids,\n",
        "    #                 label_probs,\n",
        "    #                 eval_mask,\n",
        "    #                 label_id_to_entity_id_dict,\n",
        "    #                 batch_entity_ids,\n",
        "    #                 orig_batch,\n",
        "    #                 _,\n",
        "    #             ) = batch\n",
        "\n",
        "    #             logits, y, y_hat, probs, _, _ = model(batch_token_ids, None, None)  # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "    #             tags = list()\n",
        "    #             predtags = list()\n",
        "    #             y_resolved_list = list()\n",
        "    #             y_hat_resolved_list = list()\n",
        "    #             token_list = list()\n",
        "\n",
        "    #             chunk_len = args.create_integerized_training_instance_text_length\n",
        "    #             chunk_overlap = args.create_integerized_training_instance_text_overlap\n",
        "\n",
        "    #             for batch_id, seq in enumerate(label_probs.max(-1)[1]):\n",
        "    #                 for tok_id, label_id in enumerate(seq[chunk_overlap : -chunk_overlap]):\n",
        "    #                     y_resolved = (\n",
        "    #                         vocab.PAD_ID\n",
        "    #                         if eval_mask[batch_id][tok_id + chunk_overlap] == 0\n",
        "    #                         else label_ids[label_id].item()\n",
        "    #                     )\n",
        "    #                     y_resolved_list.append(y_resolved)\n",
        "    #                     tags.append(vocab.idx2tag[y_resolved])\n",
        "    #                     if sampled_evaluation:\n",
        "    #                         y_hat_resolved = (\n",
        "    #                             vocab.PAD_ID\n",
        "    #                             if eval_mask[batch_id][tok_id + chunk_overlap] == 0\n",
        "    #                             else label_ids[y_hat[batch_id][tok_id + chunk_overlap]].item()\n",
        "    #                         )\n",
        "    #                     else:\n",
        "    #                         y_hat_resolved = y_hat[batch_id][tok_id + chunk_overlap].item()\n",
        "    #                     y_hat_resolved_list.append(y_hat_resolved)\n",
        "    #                     predtags.append(vocab.idx2tag[y_hat_resolved])\n",
        "    #                     token_list.append(batch_token_ids[batch_id][tok_id + chunk_overlap].item())\n",
        "\n",
        "    #             all_y.append(y_resolved_list)\n",
        "    #             all_y_hat.append(y_hat_resolved_list)\n",
        "    #             all_tags.append(tags)\n",
        "    #             all_predicted.append(predtags)\n",
        "    #             all_words.append(vocab.tokenizer.convert_ids_to_tokens(token_list))\n",
        "    #             all_token_ids.append(token_list)\n",
        "\n",
        "    #     ## calc metric\n",
        "    #     y_true = numpy.array(list(chain(*all_y)))\n",
        "    #     y_pred = numpy.array(list(chain(*all_y_hat)))\n",
        "    #     all_token_ids = numpy.array(list(chain(*all_token_ids)))\n",
        "\n",
        "    #     num_proposed = len(y_pred[(vocab.OUTSIDE_ID > y_pred) & (all_token_ids > 0)])\n",
        "    #     num_correct = (((y_true == y_pred) & (vocab.OUTSIDE_ID > y_true) & (all_token_ids > 0))).astype(numpy.int).sum()\n",
        "    #     num_gold = len(y_true[(vocab.OUTSIDE_ID > y_true) & (all_token_ids > 0)])\n",
        "\n",
        "    #     new_metrics = Metrics(\n",
        "    #         epoch=epoch, step=step, num_correct=num_correct, num_proposed=num_proposed, num_gold=num_gold,\n",
        "    #     )\n",
        "\n",
        "    #     if save_predictions:\n",
        "    #         final = args.logdir + \"/%s.P%.2f_R%.2f_F%.2f\" % (\n",
        "    #             \"{}-{}\".format(str(epoch), str(step)),\n",
        "    #             new_metrics.precision,\n",
        "    #             new_metrics.recall,\n",
        "    #             new_metrics.f1,\n",
        "    #         )\n",
        "    #         with open(final, \"w\") as fout:\n",
        "\n",
        "    #             for words, tags, y_hat, preds in zip(all_words, all_tags, all_y_hat, all_predicted):\n",
        "    #                 assert len(preds) == len(words) == len(tags)\n",
        "    #                 for w, t, p in zip(words, tags, preds):\n",
        "    #                     fout.write(f\"{w}\\t{t}\\t{p}\\n\")\n",
        "    #                 fout.write(\"\\n\")\n",
        "\n",
        "    #             fout.write(f\"num_proposed:{num_proposed}\\n\")\n",
        "    #             fout.write(f\"num_correct:{num_correct}\\n\")\n",
        "    #             fout.write(f\"num_gold:{num_gold}\\n\")\n",
        "    #             fout.write(f\"precision={new_metrics.precision}\\n\")\n",
        "    #             fout.write(f\"recall={new_metrics.recall}\\n\")\n",
        "    #             fout.write(f\"f1={new_metrics.f1}\\n\")\n",
        "\n",
        "    #     if not args.dont_save_checkpoints:\n",
        "\n",
        "    #         if save_checkpoint and metrics.was_improved(new_metrics):\n",
        "    #             config = {\n",
        "    #                 \"args\": args,\n",
        "    #                 \"optimizer_dense\": optimizers[0].state_dict(),\n",
        "    #                 \"optimizer_sparse\": optimizers[1].state_dict(),\n",
        "    #                 \"model\": model.state_dict(),\n",
        "    #                 \"epoch\": epoch,\n",
        "    #                 \"step\": step,\n",
        "    #                 \"performance\": new_metrics.dict(),\n",
        "    #             }\n",
        "    #             fname = os.path.join(args.logdir, \"{}-{}\".format(str(epoch), str(step)))\n",
        "    #             torch.save(config, f\"{fname}.pt\")\n",
        "    #             fname = os.path.join(args.logdir, new_metrics.get_best_checkpoint_filename())\n",
        "    #             torch.save(config, f\"{fname}.pt\")\n",
        "    #             logging.info(f\"weights were saved to {fname}.pt\")\n",
        "\n",
        "    #     if save_csv:\n",
        "    #         new_metrics.to_csv(epoch=epoch, step=step, args=args)\n",
        "\n",
        "    #     if metrics.was_improved(new_metrics):\n",
        "    #         metrics.update(new_metrics)\n",
        "\n",
        "    #     logging.info(\"Finished evaluation\")\n",
        "\n",
        "    #     return metrics\n",
        "    def evaluate(\n",
        "        args,\n",
        "        model,\n",
        "        iterator,\n",
        "        vocab,\n",
        "        optimizers,\n",
        "        step=0,\n",
        "        epoch=0,\n",
        "        save_checkpoint=True,\n",
        "        save_predictions=True,\n",
        "        save_csv=True,\n",
        "        sampled_evaluation=False,\n",
        "        metrics = None\n",
        "        # metrics=Metrics(),\n",
        "    ):\n",
        "          args_device = 0\n",
        "          args_eval_on_test_only = False\n",
        "          args_eval_device = 0\n",
        "          args_logdir = '/content/drive/My Drive/data/checkpoints/dummy_wiki_00001'\n",
        "          \n",
        "          args_create_integerized_training_instance_text_length = 64\n",
        "          args_create_integerized_training_instance_text_overlap = 128\n",
        "          args_dont_save_checkpoints = False\n",
        "          print()\n",
        "          logging.info(f\"Start evaluation on split {'test' if args_eval_on_test_only else 'valid'}\")\n",
        "\n",
        "          model.eval()\n",
        "          model.to(args_device, args_eval_device)\n",
        "\n",
        "          all_words, all_tags, all_y, all_y_hat, all_predicted, all_token_ids = [], [], [], [], [], []\n",
        "          with torch.no_grad():\n",
        "              # for iter, batch in enumerate(tqdm.tqdm(iterator)):\n",
        "              for iter, batch in enumerate(tqdm(iterator)):\n",
        "                  (\n",
        "                      batch_token_ids,\n",
        "                      label_ids,\n",
        "                      label_probs,\n",
        "                      eval_mask,\n",
        "                      label_id_to_entity_id_dict,\n",
        "                      batch_entity_ids,\n",
        "                      orig_batch,\n",
        "                      _,\n",
        "                  ) = batch\n",
        "\n",
        "                  logits, y, y_hat, probs, _, _ = model(batch_token_ids, None, None)  # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "                  tags = list()\n",
        "                  predtags = list()\n",
        "                  y_resolved_list = list()\n",
        "                  y_hat_resolved_list = list()\n",
        "                  token_list = list()\n",
        "\n",
        "                  chunk_len = args_create_integerized_training_instance_text_length\n",
        "                  chunk_overlap = args_create_integerized_training_instance_text_overlap\n",
        "\n",
        "                  for batch_id, seq in enumerate(label_probs.max(-1)[1]):\n",
        "                      for tok_id, label_id in enumerate(seq[chunk_overlap : -chunk_overlap]):\n",
        "                          y_resolved = (\n",
        "                              vocab.PAD_ID\n",
        "                              if eval_mask[batch_id][tok_id + chunk_overlap] == 0\n",
        "                              else label_ids[label_id].item()\n",
        "                          )\n",
        "                          y_resolved_list.append(y_resolved)\n",
        "                          tags.append(vocab.idx2tag[y_resolved])\n",
        "                          if sampled_evaluation:\n",
        "                              y_hat_resolved = (\n",
        "                                  vocab.PAD_ID\n",
        "                                  if eval_mask[batch_id][tok_id + chunk_overlap] == 0\n",
        "                                  else label_ids[y_hat[batch_id][tok_id + chunk_overlap]].item()\n",
        "                              )\n",
        "                          else:\n",
        "                              y_hat_resolved = y_hat[batch_id][tok_id + chunk_overlap].item()\n",
        "                          y_hat_resolved_list.append(y_hat_resolved)\n",
        "                          predtags.append(vocab.idx2tag[y_hat_resolved])\n",
        "                          token_list.append(batch_token_ids[batch_id][tok_id + chunk_overlap].item())\n",
        "\n",
        "                  all_y.append(y_resolved_list)\n",
        "                  all_y_hat.append(y_hat_resolved_list)\n",
        "                  all_tags.append(tags)\n",
        "                  all_predicted.append(predtags)\n",
        "                  all_words.append(vocab.tokenizer.convert_ids_to_tokens(token_list))\n",
        "                  all_token_ids.append(token_list)\n",
        "\n",
        "          ## calc metric\n",
        "          y_true = numpy.array(list(chain(*all_y)))\n",
        "          y_pred = numpy.array(list(chain(*all_y_hat)))\n",
        "          all_token_ids = numpy.array(list(chain(*all_token_ids)))\n",
        "\n",
        "          num_proposed = len(y_pred[(vocab.OUTSIDE_ID > y_pred) & (all_token_ids > 0)])\n",
        "          num_correct = (((y_true == y_pred) & (vocab.OUTSIDE_ID > y_true) & (all_token_ids > 0))).astype(numpy.int).sum()\n",
        "          num_gold = len(y_true[(vocab.OUTSIDE_ID > y_true) & (all_token_ids > 0)])\n",
        "\n",
        "          new_metrics = Metrics(\n",
        "              epoch=epoch, step=step, num_correct=num_correct, num_proposed=num_proposed, num_gold=num_gold,\n",
        "          )\n",
        "\n",
        "          if save_predictions:\n",
        "              final = args_logdir + \"/%s.P%.2f_R%.2f_F%.2f\" % (\n",
        "                  \"{}-{}\".format(str(epoch), str(step)),\n",
        "                  new_metrics.precision,\n",
        "                  new_metrics.recall,\n",
        "                  new_metrics.f1,\n",
        "              )\n",
        "              with open(final, \"w\") as fout:\n",
        "\n",
        "                  for words, tags, y_hat, preds in zip(all_words, all_tags, all_y_hat, all_predicted):\n",
        "                      assert len(preds) == len(words) == len(tags)\n",
        "                      for w, t, p in zip(words, tags, preds):\n",
        "                          fout.write(f\"{w}\\t{t}\\t{p}\\n\")\n",
        "                      fout.write(\"\\n\")\n",
        "\n",
        "                  fout.write(f\"num_proposed:{num_proposed}\\n\")\n",
        "                  fout.write(f\"num_correct:{num_correct}\\n\")\n",
        "                  fout.write(f\"num_gold:{num_gold}\\n\")\n",
        "                  fout.write(f\"precision={new_metrics.precision}\\n\")\n",
        "                  fout.write(f\"recall={new_metrics.recall}\\n\")\n",
        "                  fout.write(f\"f1={new_metrics.f1}\\n\")\n",
        "\n",
        "          if not args_dont_save_checkpoints:\n",
        "\n",
        "              if save_checkpoint and metrics.was_improved(new_metrics):\n",
        "                  config = {\n",
        "                      \"args\": args,\n",
        "                      \"optimizer_dense\": optimizers[0].state_dict(),\n",
        "                      \"optimizer_sparse\": optimizers[1].state_dict(),\n",
        "                      \"model\": model.state_dict(),\n",
        "                      \"epoch\": epoch,\n",
        "                      \"step\": step,\n",
        "                      \"performance\": new_metrics.dict(),\n",
        "                  }\n",
        "                  fname = os.path.join(args_logdir, \"{}-{}\".format(str(epoch), str(step)))\n",
        "                  torch.save(config, f\"{fname}.pt\")\n",
        "                  fname = os.path.join(args_logdir, new_metrics.get_best_checkpoint_filename())\n",
        "                  torch.save(config, f\"{fname}.pt\")\n",
        "                  logging.info(f\"weights were saved to {fname}.pt\")\n",
        "\n",
        "          if save_csv:\n",
        "              new_metrics.to_csv(epoch=epoch, step=step, args=args)\n",
        "\n",
        "          if metrics.was_improved(new_metrics):\n",
        "              metrics.update(new_metrics)\n",
        "\n",
        "          logging.info(\"Finished evaluation\")\n",
        "\n",
        "          return metrics\n",
        "\n",
        "    # def get_optimizers(self, args, checkpoint):\n",
        "\n",
        "    #     optimizers = list()\n",
        "    #     args_encoder_lr = 5e-05\n",
        "    #     args_encoder_weight_decay = 0.0\n",
        "    #     args_sparse = 0\n",
        "    #     args_encoder_lr_scheduler_config = 0\n",
        "    #     if args_encoder_lr_scheduler_config:\n",
        "    #       args_encoder_lr_scheduler_config = ast.literal_eval(args_encoder_lr_scheduler_config)\n",
        "    #     if args_decoder_lr_scheduler_config:\n",
        "    #       args_decoder_lr_scheduler_config = ast.literal_eval(args.decoder_lr_scheduler_config)\n",
        "    #     if args.segm_decoder_lr_scheduler_config:\n",
        "    #     args.segm_decoder_lr_scheduler_config = ast.literal_eval(args.segm_decoder_lr_scheduler_config)\n",
        "\n",
        "    #     args.eval_batch_size = args.eval_batch_size if args.eval_batch_size else args.batch_size\n",
        "        \n",
        "    #     args.encoder_lr_scheduler_config = 0\n",
        "    #     args.encoder_lr_scheduler = 0\n",
        "    #     if args.encoder_lr > 0:\n",
        "    #         optimizer_encoder = optim.Adam(\n",
        "    #             list(self.bert.parameters()) + list(self.fc.parameters() if args.project else list()),\n",
        "    #             lr=args.encoder_lr,\n",
        "    #         )\n",
        "    #         if args.resume_from_checkpoint is not None:\n",
        "    #             optimizer_encoder.load_state_dict(checkpoint[\"optimizer_dense\"])\n",
        "    #             optimizer_encoder.param_groups[0][\"lr\"] = args.encoder_lr\n",
        "    #             optimizer_encoder.param_groups[0][\"weight_decay\"] = args.encoder_weight_decay\n",
        "    #         optimizers.append(optimizer_encoder)\n",
        "    #     else:\n",
        "    #         optimizers.append(DummyOptimizer(self.out.parameters(), defaults={}))\n",
        "\n",
        "    #     if args.decoder_lr > 0:\n",
        "    #         if args.sparse:\n",
        "    #             optimizer_decoder = optim.SparseAdam(self.out.parameters(), lr=args.decoder_lr)\n",
        "    #         else:\n",
        "    #             optimizer_decoder = optim.Adam(self.out.parameters(), lr=args.decoder_lr)\n",
        "    #         if args.resume_from_checkpoint is not None:\n",
        "    #             optimizer_decoder.load_state_dict(checkpoint[\"optimizer_sparse\"])\n",
        "    #             if \"weight_decay\" not in optimizer_decoder.param_groups[0]:\n",
        "    #                 optimizer_decoder.param_groups[0][\"weight_decay\"] = 0\n",
        "    #             optimizer_decoder.param_groups[0][\"lr\"] = args.decoder_lr\n",
        "    #             if not args.sparse:\n",
        "    #                 optimizer_decoder.param_groups[0][\"weight_decay\"] = args.decoder_weight_decay\n",
        "    #         optimizers.append(optimizer_decoder)\n",
        "    #     else:\n",
        "    #         optimizers.append(DummyOptimizer(self.out.parameters(), defaults={}))\n",
        "\n",
        "    #     lr_schedulers = [\n",
        "    #         getattr(LRSchedulers, lr_scheduler)(optimizer=optimizer, **lr_scheduler_config)\n",
        "    #         for optimizer, (lr_scheduler, lr_scheduler_config) in zip(\n",
        "    #             optimizers,\n",
        "    #             [\n",
        "    #                 (args.encoder_lr_scheduler, args.encoder_lr_scheduler_config),\n",
        "    #                 (args.decoder_lr_scheduler, args.decoder_lr_scheduler_config),\n",
        "    #             ],\n",
        "    #         )\n",
        "    #         if lr_scheduler is not None  # and not isinstance(optimizer, DummyOptimizer)\n",
        "    #     ]\n",
        "\n",
        "    #     return tuple(optimizers), tuple(lr_schedulers)\n",
        "    def get_optimizers(self, args, checkpoint):\n",
        "\n",
        "        optimizers = list()\n",
        "        args_encoder_lr = 5e-05\n",
        "        args_encoder_weight_decay = 0.0\n",
        "        args_sparse = 0\n",
        "        args_encoder_lr_scheduler_config = 0\n",
        "        args_decoder_lr_scheduler_config = 0\n",
        "        args_segm_decoder_lr_scheduler_config = 0\n",
        "        if args_encoder_lr_scheduler_config:\n",
        "          args_encoder_lr_scheduler_config = ast.literal_eval(args_encoder_lr_scheduler_config)\n",
        "        if args_decoder_lr_scheduler_config:\n",
        "          args_decoder_lr_scheduler_config = ast.literal_eval(args_decoder_lr_scheduler_config)\n",
        "        if args_segm_decoder_lr_scheduler_config:\n",
        "          args_segm_decoder_lr_scheduler_config = ast.literal_eval(args_segm_decoder_lr_scheduler_config)\n",
        "        args_eval_batch_size = 1\n",
        "        args_batch_size = 16\n",
        "        args_eval_batch_size = args_eval_batch_size if args_eval_batch_size else args_batch_size\n",
        "        args_project = 0\n",
        "        args_decoder_lr = 0.1\n",
        "        args_encoder_lr_scheduler_config = 0\n",
        "        args_encoder_lr_scheduler = 0\n",
        "        args_resume_from_checkpoint = None\n",
        "        args_decoder_lr_scheduler = 0\n",
        "        args_decoder_lr_scheduler_config = 0\n",
        "        if args_encoder_lr > 0:\n",
        "            optimizer_encoder = optim.Adam(\n",
        "                list(self.bert.parameters()) + list(self.fc.parameters() if args_project else list()),\n",
        "                lr=args_encoder_lr,\n",
        "            )\n",
        "            if args_resume_from_checkpoint is not None:\n",
        "                optimizer_encoder.load_state_dict(checkpoint[\"optimizer_dense\"])\n",
        "                optimizer_encoder.param_groups[0][\"lr\"] = args_encoder_lr\n",
        "                optimizer_encoder.param_groups[0][\"weight_decay\"] = args_encoder_weight_decay\n",
        "            optimizers.append(optimizer_encoder)\n",
        "        else:\n",
        "            optimizers.append(DummyOptimizer(self.out.parameters(), defaults={}))\n",
        "\n",
        "        if args_decoder_lr > 0:\n",
        "            if args_sparse:\n",
        "                optimizer_decoder = optim.SparseAdam(self.out.parameters(), lr=args_decoder_lr)\n",
        "            else:\n",
        "                optimizer_decoder = optim.Adam(self.out.parameters(), lr=args_decoder_lr)\n",
        "            if args_resume_from_checkpoint is not None:\n",
        "                optimizer_decoder.load_state_dict(checkpoint[\"optimizer_sparse\"])\n",
        "                if \"weight_decay\" not in optimizer_decoder.param_groups[0]:\n",
        "                    optimizer_decoder.param_groups[0][\"weight_decay\"] = 0\n",
        "                optimizer_decoder.param_groups[0][\"lr\"] = args_decoder_lr\n",
        "                if not args_sparse:\n",
        "                    optimizer_decoder.param_groups[0][\"weight_decay\"] = args_decoder_weight_decay\n",
        "            optimizers.append(optimizer_decoder)\n",
        "        else:\n",
        "            optimizers.append(DummyOptimizer(self.out.parameters(), defaults={}))\n",
        "        optimizer = DummyOptimizer(self.out.parameters(), defaults={})\n",
        "        lr_schedulers = [LRMilestones(optimizer, milestones=[(30, 0.1), (80, 0.2), ])]\n",
        "        #     getattr(LRSchedulers, lr_scheduler)(optimizer=optimizer, **lr_scheduler_config)\n",
        "            # for optimizer, (lr_scheduler, lr_scheduler_config) in zip(\n",
        "            #     optimizers,\n",
        "            #     [\n",
        "            #         (args_encoder_lr_scheduler, args_encoder_lr_scheduler_config),\n",
        "            #         (args_decoder_lr_scheduler, args_decoder_lr_scheduler_config),\n",
        "            #     ],\n",
        "            # )\n",
        "        #     if lr_scheduler is not None  # and not isinstance(optimizer, DummyOptimizer)\n",
        "        # ]\n",
        "            # if lr_scheduler is not None  # and not isinstance(optimizer, DummyOptimizer)\n",
        "    #     ]\n",
        "\n",
        "        return tuple(optimizers), tuple(lr_schedulers)"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHlQjxY_PGqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ast\n",
        "import os\n",
        "import torch.cuda\n",
        "import yaml\n",
        "import argparse\n",
        "# from torchfun import argparse_bool_type"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SJQDd9ZgivK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "from operator import itemgetter\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from tqdm import tqdm\n",
        "class EDLDataset(data.Dataset):\n",
        "    def __init__(self, args, split, vocab, device, label_size=None):\n",
        "        args_train_loc_file = 'train.loc'\n",
        "        args_valid_loc_file = 'valid.loc'\n",
        "        args_test_loc_file = 'valid.loc'\n",
        "        args_train_data_dir = data\n",
        "        args_valid_data_dir = data\n",
        "        args_test_data_dir = data\n",
        "        args_data_workers = 24\n",
        "        args_collect_most_popular_labels_steps = 1\n",
        "        args_label_size = 8192\n",
        "        args_vocab_size = 0\n",
        "\t\n",
        "\t\n",
        "\t\n",
        "\t\n",
        "        if split == \"train\":\n",
        "            loc_file_name = args_train_loc_file\n",
        "            self.data_dir = args_train_data_dir\n",
        "        elif split == \"valid\":\n",
        "            loc_file_name = args_valid_loc_file\n",
        "            self.data_dir = args_valid_data_dir\n",
        "        elif split == \"test\":\n",
        "            loc_file_name = args_test_loc_file\n",
        "            self.data_dir = args_test_data_dir\n",
        "\n",
        "        self.data_path =  f\"/content/drive/My Drive/data/versions/dummy/wiki_training/integerized/enwiki/\"\n",
        "        #self.data_path = f\"data/versions/{args_data_version_name}/wiki_training/integerized/{args_wiki_lang_version}/\"\n",
        "        self.item_locs = None\n",
        "        self.device = device\n",
        "        if os.path.exists(\"{}.pickle\".format(self.data_path + loc_file_name)):\n",
        "            with open(\"{}.pickle\".format(self.data_path + loc_file_name), \"rb\") as f:\n",
        "                self.item_locs = pickle.load(f)\n",
        "        else:\n",
        "            with open(self.data_path + loc_file_name) as f:\n",
        "                self.item_locs = list(map(lambda x: list(map(int, x.strip().split())), tqdm(f.readlines())))\n",
        "            with open(\"{}.pickle\".format(self.data_path + loc_file_name), \"wb\") as f:\n",
        "                pickle.dump(self.item_locs, f)\n",
        "        self.pad_token_id = vocab.PAD_ID\n",
        "        self.label_size = label_size\n",
        "        self.is_training = split == \"train\"\n",
        "\n",
        "    def get_data_iter(\n",
        "        self, args, batch_size, vocab, train,\n",
        "    ):\n",
        "        args_collect_most_popular_labels_steps = 1\n",
        "        args_data_workers = 24\n",
        "        return data.DataLoader(\n",
        "            dataset=self.item_locs,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=train,\n",
        "            num_workers=args_data_workers,\n",
        "            collate_fn=self.collate_func(\n",
        "                args=args,\n",
        "                vocab=vocab,\n",
        "                return_labels=args_collect_most_popular_labels_steps is not None\n",
        "                and args_collect_most_popular_labels_steps > 0\n",
        "                if train\n",
        "                else True,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    # def collate_func(self, args, vocab, return_labels, shards, shards_locks):\n",
        "    def collate_func(\n",
        "        self, args, vocab, return_labels, in_queue=None, out_queue=None,\n",
        "    ):\n",
        "        def collate(batch):\n",
        "            return EDLDataset_collate_func(\n",
        "                batch=batch,\n",
        "                labels_with_high_model_score=None,\n",
        "                args=args,\n",
        "                return_labels=return_labels,\n",
        "                data_path=self.data_path,\n",
        "                vocab=vocab,\n",
        "                is_training=self.is_training,\n",
        "            )\n",
        "\n",
        "        return collate\n",
        "\n",
        "\n",
        "def EDLDataset_collate_func(\n",
        "    batch,\n",
        "    labels_with_high_model_score,\n",
        "    args,\n",
        "    return_labels,\n",
        "    vocab: Vocab,\n",
        "    data_path=None,\n",
        "    is_training=True,\n",
        "    drop_entity_mentions_prob=0.0,\n",
        "    loaded_batch=None,\n",
        "):\n",
        "    args_label_size = 8192\n",
        "    if loaded_batch is None:\n",
        "        batch_dict_list = list()\n",
        "        for shard, offset in batch:\n",
        "            # print('{}/{}.dat'.format(data_path, shard), offset)\n",
        "            with open(\"{}/{}.dat\".format(data_path, shard), \"rb\") as f:\n",
        "                f.seek(offset)\n",
        "                (\n",
        "                    token_ids_chunk,\n",
        "                    mention_entity_ids_chunk,\n",
        "                    mention_entity_probs_chunk,\n",
        "                    mention_probs_chunk,\n",
        "                ) = pickle.load(f)\n",
        "                try:\n",
        "                    eval_mask = list(map(is_a_wikilink_or_keyword, mention_probs_chunk))\n",
        "                    mention_entity_ids_chunk = list(map(itemgetter(0), mention_entity_ids_chunk))\n",
        "                    mention_entity_probs_chunk = list(map(itemgetter(0), mention_entity_probs_chunk))\n",
        "                    batch_dict_list.append(\n",
        "                        {\n",
        "                            \"token_ids\": token_ids_chunk,\n",
        "                            \"entity_ids\": mention_entity_ids_chunk,\n",
        "                            \"entity_probs\": mention_entity_probs_chunk,\n",
        "                            \"eval_mask\": eval_mask,\n",
        "                        }\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"pickle.load(shards[shard]) failed {e}\")\n",
        "                    print(mention_entity_ids_chunk)\n",
        "                    print(mention_entity_probs_chunk)\n",
        "                    raise e\n",
        "\n",
        "        f = lambda x: [sample[x] for sample in batch_dict_list]\n",
        "        # print(batch)\n",
        "        batch_token_ids = f(\"token_ids\")\n",
        "        batch_entity_ids = f(\"entity_ids\")\n",
        "        batch_entity_probs = f(\"entity_probs\")\n",
        "        eval_mask = f(\"eval_mask\")\n",
        "        maxlen = max([len(chunk) for chunk in batch_token_ids])\n",
        "\n",
        "        eval_mask = torch.LongTensor([sample + [0] * (maxlen - len(sample)) for sample in eval_mask])\n",
        "\n",
        "        # create dictionary mapping the vocabulary entity id to a batch label id\n",
        "        #\n",
        "        # e.g.\n",
        "        # all_batch_entity_ids[324] = 0\n",
        "        # all_batch_entity_ids[24]  = 1\n",
        "        # all_batch_entity_ids[2]   = 2\n",
        "        # all_batch_entity_ids[987] = 3\n",
        "        #\n",
        "        all_batch_entity_ids = OrderedDict()\n",
        "\n",
        "        for batch_offset, (batch_item_token_item_entity_ids, batch_item_token_entity_probs) in enumerate(\n",
        "            zip(batch_entity_ids, batch_entity_probs)\n",
        "        ):\n",
        "            for tok_id, (token_entity_ids, token_entity_probs) in enumerate(\n",
        "                zip(batch_item_token_item_entity_ids, batch_item_token_entity_probs)\n",
        "            ):\n",
        "                for eid in token_entity_ids:\n",
        "                    if eid not in all_batch_entity_ids:\n",
        "                        all_batch_entity_ids[eid] = len(all_batch_entity_ids)\n",
        "\n",
        "        loaded_batch = (\n",
        "            batch_token_ids,\n",
        "            batch_entity_ids,\n",
        "            batch_entity_probs,\n",
        "            eval_mask,\n",
        "            all_batch_entity_ids,\n",
        "            maxlen,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        (batch_token_ids, batch_entity_ids, batch_entity_probs, eval_mask, all_batch_entity_ids, maxlen,) = loaded_batch\n",
        "\n",
        "    batch_token_ids = torch.LongTensor([sample + [0] * (maxlen - len(sample)) for sample in batch_token_ids])\n",
        "\n",
        "    if return_labels:\n",
        "\n",
        "        # if labels for each token should be over\n",
        "        # a. the whole entity vocabulary\n",
        "        # b. a reduced set of entities composed of:\n",
        "        #       set of batch's true entities, entities\n",
        "        #       set of entities with the largest logits\n",
        "        #       set of negative samples\n",
        "\n",
        "        if args_label_size is None:\n",
        "\n",
        "            batch_shared_label_ids = list(all_batch_entity_ids.keys())\n",
        "            label_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), args_vocab_size)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # batch_shared_label_ids are constructing by incrementally concatenating\n",
        "            #       set of batch's true entities, entities\n",
        "            #       set of entities with the largest logits\n",
        "            #       set of negative samples\n",
        "\n",
        "            batch_shared_label_ids = list(all_batch_entity_ids.keys())\n",
        "\n",
        "            if len(batch_shared_label_ids) < args_label_size and labels_with_high_model_score is not None:\n",
        "                # print(labels_with_high_model_score)\n",
        "                negative_examples = set(labels_with_high_model_score)\n",
        "                negative_examples.difference_update(batch_shared_label_ids)\n",
        "                batch_shared_label_ids += list(negative_examples)\n",
        "\n",
        "            if len(batch_shared_label_ids) < args_label_size:\n",
        "                negative_samples = set(numpy.random.choice(vocab.OUTSIDE_ID, args_label_size, replace=False))\n",
        "                negative_samples.difference_update(batch_shared_label_ids)\n",
        "                batch_shared_label_ids += list(negative_samples)\n",
        "\n",
        "            batch_shared_label_ids = batch_shared_label_ids[: args_label_size]\n",
        "\n",
        "            label_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), len(batch_shared_label_ids))\n",
        "\n",
        "        drop_probs = None\n",
        "        if drop_entity_mentions_prob > 0 and is_training:\n",
        "            drop_probs = torch.rand((batch_token_ids.size(0), batch_token_ids.size(1)),) < drop_entity_mentions_prob\n",
        "\n",
        "        # loop through the batch x tokens x (label_ids, label_probs)\n",
        "        for batch_offset, (batch_item_token_item_entity_ids, batch_item_token_entity_probs) in enumerate(\n",
        "            zip(batch_entity_ids, batch_entity_probs)\n",
        "        ):\n",
        "            # loop through tokens x (label_ids, label_probs)\n",
        "            for tok_id, (token_entity_ids, token_entity_probs) in enumerate(\n",
        "                zip(batch_item_token_item_entity_ids, batch_item_token_entity_probs)\n",
        "            ):\n",
        "                if drop_entity_mentions_prob > 0 and is_training and drop_probs[batch_offset][tok_id].item() == 1:\n",
        "                    batch_token_ids[batch_offset][tok_id] = vocab.tokenizer.vocab[\"[MASK]\"]\n",
        "\n",
        "                if args_label_size is None:\n",
        "                    label_probs[batch_offset][tok_id][torch.LongTensor(token_entity_ids)] = torch.Tensor(\n",
        "                        batch_item_token_item_entity_ids\n",
        "                    )\n",
        "                else:\n",
        "                    label_probs[batch_offset][tok_id][\n",
        "                        torch.LongTensor(list(map(all_batch_entity_ids.__getitem__, token_entity_ids)))\n",
        "                    ] = torch.Tensor(token_entity_probs)\n",
        "\n",
        "        label_ids = torch.LongTensor(batch_shared_label_ids)\n",
        "\n",
        "        return (\n",
        "            batch_token_ids,\n",
        "            label_ids,\n",
        "            label_probs,\n",
        "            torch.LongTensor(eval_mask),\n",
        "            {v: k for k, v in all_batch_entity_ids.items()},\n",
        "            batch_entity_ids,\n",
        "            batch,\n",
        "            loaded_batch,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "\n",
        "        return batch_token_ids, None, None, None, None, None, batch, loaded_batch\n",
        "\n",
        "# hack to detect if an entity annotation was a\n",
        "# wikilink (== only one entity label) or a\n",
        "# keyword matcher annotation (== multiple entity labels)\n",
        "def is_a_wikilink_or_keyword(item):\n",
        "    if len(item) == 1:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "# from vocab import Vocab\n",
        "# class EDLDataset(data.Dataset):\n",
        "#     def __init__(self, args, split, vocab, device, label_size=None):\n",
        "\n",
        "#         if split == \"train\":\n",
        "#             loc_file_name = args.train_loc_file\n",
        "#             self.data_dir = args.train_data_dir\n",
        "#         elif split == \"valid\":\n",
        "#             loc_file_name = args.valid_loc_file\n",
        "#             self.data_dir = args.valid_data_dir\n",
        "#         elif split == \"test\":\n",
        "#             loc_file_name = args.test_loc_file\n",
        "#             self.data_dir = args.test_data_dir\n",
        "\n",
        "\n",
        "#         self.data_path = f\"data/versions/{args.data_version_name}/wiki_training/integerized/{args.wiki_lang_version}/\"\n",
        "#         self.item_locs = None\n",
        "#         self.device = device\n",
        "#         if os.path.exists(\"{}.pickle\".format(self.data_path + loc_file_name)):\n",
        "#             with open(\"{}.pickle\".format(self.data_path + loc_file_name), \"rb\") as f:\n",
        "#                 self.item_locs = pickle.load(f)\n",
        "#         else:\n",
        "#             with open(self.data_path + loc_file_name) as f:\n",
        "#                 self.item_locs = list(map(lambda x: list(map(int, x.strip().split())), tqdm(f.readlines())))\n",
        "#             with open(\"{}.pickle\".format(self.data_path + loc_file_name), \"wb\") as f:\n",
        "#                 pickle.dump(self.item_locs, f)\n",
        "#         self.pad_token_id = vocab.PAD_ID\n",
        "#         self.label_size = label_size\n",
        "#         self.is_training = split == \"train\"\n",
        "\n",
        "#     def get_data_iter(\n",
        "#         self, args, batch_size, vocab, train,\n",
        "#     ):\n",
        "#         return data.DataLoader(\n",
        "#             dataset=self.item_locs,\n",
        "#             batch_size=batch_size,\n",
        "#             shuffle=train,\n",
        "#             num_workers=args.data_workers,\n",
        "#             collate_fn=self.collate_func(\n",
        "#                 args=args,\n",
        "#                 vocab=vocab,\n",
        "#                 return_labels=args.collect_most_popular_labels_steps is not None\n",
        "#                 and args.collect_most_popular_labels_steps > 0\n",
        "#                 if train\n",
        "#                 else True,\n",
        "#             ),\n",
        "#         )\n",
        "\n",
        "#     # def collate_func(self, args, vocab, return_labels, shards, shards_locks):\n",
        "#     def collate_func(\n",
        "#         self, args, vocab, return_labels, in_queue=None, out_queue=None,\n",
        "#     ):\n",
        "#         def collate(batch):\n",
        "#             return EDLDataset_collate_func(\n",
        "#                 batch=batch,\n",
        "#                 labels_with_high_model_score=None,\n",
        "#                 args=args,\n",
        "#                 return_labels=return_labels,\n",
        "#                 data_path=self.data_path,\n",
        "#                 vocab=vocab,\n",
        "#                 is_training=self.is_training,\n",
        "#             )\n",
        "\n",
        "#         return collate\n",
        "\n",
        "\n",
        "# def EDLDataset_collate_func(\n",
        "#     batch,\n",
        "#     labels_with_high_model_score,\n",
        "#     args,\n",
        "#     return_labels,\n",
        "#     vocab: Vocab,\n",
        "#     data_path=None,\n",
        "#     is_training=True,\n",
        "#     drop_entity_mentions_prob=0.0,\n",
        "#     loaded_batch=None,\n",
        "# ):\n",
        "#     if loaded_batch is None:\n",
        "#         batch_dict_list = list()\n",
        "#         for shard, offset in batch:\n",
        "#             # print('{}/{}.dat'.format(data_path, shard), offset)\n",
        "#             with open(\"{}/{}.dat\".format(data_path, shard), \"rb\") as f:\n",
        "#                 f.seek(offset)\n",
        "#                 (\n",
        "#                     token_ids_chunk,\n",
        "#                     mention_entity_ids_chunk,\n",
        "#                     mention_entity_probs_chunk,\n",
        "#                     mention_probs_chunk,\n",
        "#                 ) = pickle.load(f)\n",
        "#                 try:\n",
        "#                     eval_mask = list(map(is_a_wikilink_or_keyword, mention_probs_chunk))\n",
        "#                     mention_entity_ids_chunk = list(map(itemgetter(0), mention_entity_ids_chunk))\n",
        "#                     mention_entity_probs_chunk = list(map(itemgetter(0), mention_entity_probs_chunk))\n",
        "#                     batch_dict_list.append(\n",
        "#                         {\n",
        "#                             \"token_ids\": token_ids_chunk,\n",
        "#                             \"entity_ids\": mention_entity_ids_chunk,\n",
        "#                             \"entity_probs\": mention_entity_probs_chunk,\n",
        "#                             \"eval_mask\": eval_mask,\n",
        "#                         }\n",
        "#                     )\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"pickle.load(shards[shard]) failed {e}\")\n",
        "#                     print(mention_entity_ids_chunk)\n",
        "#                     print(mention_entity_probs_chunk)\n",
        "#                     raise e\n",
        "\n",
        "#         f = lambda x: [sample[x] for sample in batch_dict_list]\n",
        "#         # print(batch)\n",
        "#         batch_token_ids = f(\"token_ids\")\n",
        "#         batch_entity_ids = f(\"entity_ids\")\n",
        "#         batch_entity_probs = f(\"entity_probs\")\n",
        "#         eval_mask = f(\"eval_mask\")\n",
        "#         maxlen = max([len(chunk) for chunk in batch_token_ids])\n",
        "\n",
        "#         eval_mask = torch.LongTensor([sample + [0] * (maxlen - len(sample)) for sample in eval_mask])\n",
        "\n",
        "#         # create dictionary mapping the vocabulary entity id to a batch label id\n",
        "#         #\n",
        "#         # e.g.\n",
        "#         # all_batch_entity_ids[324] = 0\n",
        "#         # all_batch_entity_ids[24]  = 1\n",
        "#         # all_batch_entity_ids[2]   = 2\n",
        "#         # all_batch_entity_ids[987] = 3\n",
        "#         #\n",
        "#         all_batch_entity_ids = OrderedDict()\n",
        "\n",
        "#         for batch_offset, (batch_item_token_item_entity_ids, batch_item_token_entity_probs) in enumerate(\n",
        "#             zip(batch_entity_ids, batch_entity_probs)\n",
        "#         ):\n",
        "#             for tok_id, (token_entity_ids, token_entity_probs) in enumerate(\n",
        "#                 zip(batch_item_token_item_entity_ids, batch_item_token_entity_probs)\n",
        "#             ):\n",
        "#                 for eid in token_entity_ids:\n",
        "#                     if eid not in all_batch_entity_ids:\n",
        "#                         all_batch_entity_ids[eid] = len(all_batch_entity_ids)\n",
        "\n",
        "#         loaded_batch = (\n",
        "#             batch_token_ids,\n",
        "#             batch_entity_ids,\n",
        "#             batch_entity_probs,\n",
        "#             eval_mask,\n",
        "#             all_batch_entity_ids,\n",
        "#             maxlen,\n",
        "#         )\n",
        "\n",
        "#     else:\n",
        "#         (batch_token_ids, batch_entity_ids, batch_entity_probs, eval_mask, all_batch_entity_ids, maxlen,) = loaded_batch\n",
        "\n",
        "#     batch_token_ids = torch.LongTensor([sample + [0] * (maxlen - len(sample)) for sample in batch_token_ids])\n",
        "\n",
        "#     if return_labels:\n",
        "\n",
        "#         # if labels for each token should be over\n",
        "#         # a. the whole entity vocabulary\n",
        "#         # b. a reduced set of entities composed of:\n",
        "#         #       set of batch's true entities, entities\n",
        "#         #       set of entities with the largest logits\n",
        "#         #       set of negative samples\n",
        "\n",
        "#         if args.label_size is None:\n",
        "\n",
        "#             batch_shared_label_ids = list(all_batch_entity_ids.keys())\n",
        "#             label_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), args.vocab_size)\n",
        "\n",
        "#         else:\n",
        "\n",
        "#             # batch_shared_label_ids are constructing by incrementally concatenating\n",
        "#             #       set of batch's true entities, entities\n",
        "#             #       set of entities with the largest logits\n",
        "#             #       set of negative samples\n",
        "\n",
        "#             batch_shared_label_ids = list(all_batch_entity_ids.keys())\n",
        "\n",
        "#             if len(batch_shared_label_ids) < args.label_size and labels_with_high_model_score is not None:\n",
        "#                 # print(labels_with_high_model_score)\n",
        "#                 negative_examples = set(labels_with_high_model_score)\n",
        "#                 negative_examples.difference_update(batch_shared_label_ids)\n",
        "#                 batch_shared_label_ids += list(negative_examples)\n",
        "\n",
        "#             if len(batch_shared_label_ids) < args.label_size:\n",
        "#                 negative_samples = set(numpy.random.choice(vocab.OUTSIDE_ID, args.label_size, replace=False))\n",
        "#                 negative_samples.difference_update(batch_shared_label_ids)\n",
        "#                 batch_shared_label_ids += list(negative_samples)\n",
        "\n",
        "#             batch_shared_label_ids = batch_shared_label_ids[: args.label_size]\n",
        "\n",
        "#             label_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), len(batch_shared_label_ids))\n",
        "\n",
        "#         drop_probs = None\n",
        "#         if drop_entity_mentions_prob > 0 and is_training:\n",
        "#             drop_probs = torch.rand((batch_token_ids.size(0), batch_token_ids.size(1)),) < drop_entity_mentions_prob\n",
        "\n",
        "#         # loop through the batch x tokens x (label_ids, label_probs)\n",
        "#         for batch_offset, (batch_item_token_item_entity_ids, batch_item_token_entity_probs) in enumerate(\n",
        "#             zip(batch_entity_ids, batch_entity_probs)\n",
        "#         ):\n",
        "#             # loop through tokens x (label_ids, label_probs)\n",
        "#             for tok_id, (token_entity_ids, token_entity_probs) in enumerate(\n",
        "#                 zip(batch_item_token_item_entity_ids, batch_item_token_entity_probs)\n",
        "#             ):\n",
        "#                 if drop_entity_mentions_prob > 0 and is_training and drop_probs[batch_offset][tok_id].item() == 1:\n",
        "#                     batch_token_ids[batch_offset][tok_id] = vocab.tokenizer.vocab[\"[MASK]\"]\n",
        "\n",
        "#                 if args.label_size is None:\n",
        "#                     label_probs[batch_offset][tok_id][torch.LongTensor(token_entity_ids)] = torch.Tensor(\n",
        "#                         batch_item_token_item_entity_ids\n",
        "#                     )\n",
        "#                 else:\n",
        "#                     label_probs[batch_offset][tok_id][\n",
        "#                         torch.LongTensor(list(map(all_batch_entity_ids.__getitem__, token_entity_ids)))\n",
        "#                     ] = torch.Tensor(token_entity_probs)\n",
        "\n",
        "#         label_ids = torch.LongTensor(batch_shared_label_ids)\n",
        "\n",
        "#         return (\n",
        "#             batch_token_ids,\n",
        "#             label_ids,\n",
        "#             label_probs,\n",
        "#             torch.LongTensor(eval_mask),\n",
        "#             {v: k for k, v in all_batch_entity_ids.items()},\n",
        "#             batch_entity_ids,\n",
        "#             batch,\n",
        "#             loaded_batch,\n",
        "#         )\n",
        "\n",
        "#     else:\n",
        "\n",
        "#         return batch_token_ids, None, None, None, None, None, batch, loaded_batch\n",
        "\n",
        "# # hack to detect if an entity annotation was a\n",
        "# # wikilink (== only one entity label) or a\n",
        "# # keyword matcher annotation (== multiple entity labels)\n",
        "# def is_a_wikilink_or_keyword(item):\n",
        "#     if len(item) == 1:\n",
        "#         return 1\n",
        "#     else:\n",
        "#         return 0\n"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExhEAeMWjW0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CONLLEDLDataset(data.Dataset):\n",
        "    def __init__(self, args, split, vocab, device, label_size=None):\n",
        "\n",
        "        if split == \"train\":\n",
        "            train_valid_test_int = 0\n",
        "        if split == \"small_valid\" or split == \"valid\":\n",
        "            train_valid_test_int = 1\n",
        "        if split == \"test\":\n",
        "            train_valid_test_int = 2\n",
        "\n",
        "        chunk_len = args.create_integerized_training_instance_text_length\n",
        "        chunk_overlap = args.create_integerized_training_instance_text_overlap\n",
        "\n",
        "        self.item_locs = None\n",
        "        self.device = device\n",
        "        with open(args.data_path_conll, \"rb\") as f:\n",
        "            train_valid_test = pickle.load(f)\n",
        "            self.conll_docs = torch.LongTensor(\n",
        "                [\n",
        "                    [\n",
        "                        pad_to(\n",
        "                            [tok_id for _, tok_id, _, _, _, _, _ in doc],\n",
        "                            max_len=chunk_len + 2,\n",
        "                            pad_id=0,\n",
        "                            cls_id=101,\n",
        "                            sep_id=102,\n",
        "                        )\n",
        "                        for doc in train_valid_test[train_valid_test_int]\n",
        "                    ],\n",
        "                    [\n",
        "                        pad_to(\n",
        "                            [bio_id for _, _, _, bio_id, _, _, _ in doc],\n",
        "                            max_len=chunk_len + 2,\n",
        "                            pad_id=2,\n",
        "                            cls_id=2,\n",
        "                            sep_id=2,\n",
        "                        )\n",
        "                        for doc in train_valid_test[train_valid_test_int]\n",
        "                    ],\n",
        "                    [\n",
        "                        pad_to(\n",
        "                            [wiki_id for _, _, _, _, _, wiki_id, _ in doc],\n",
        "                            max_len=chunk_len + 2,\n",
        "                            pad_id=vocab.PAD_ID,\n",
        "                            cls_id=vocab.PAD_ID,\n",
        "                            sep_id=vocab.PAD_ID,\n",
        "                        )\n",
        "                        for doc in train_valid_test[train_valid_test_int]\n",
        "                    ],\n",
        "                    [\n",
        "                        pad_to(\n",
        "                            [doc_id for _, _, _, _, _, _, doc_id in doc],\n",
        "                            max_len=chunk_len + 2,\n",
        "                            pad_id=0,\n",
        "                            cls_id=0,\n",
        "                            sep_id=0,\n",
        "                        )\n",
        "                        for doc in train_valid_test[train_valid_test_int]\n",
        "                    ],\n",
        "                ]\n",
        "            ).permute(1, 0, 2)\n",
        "            self.conll_docs[:, 2] = set_out_id(self.conll_docs[:, 2], vocab.OUTSIDE_ID)\n",
        "\n",
        "        self.pad_token_id = vocab.PAD_ID\n",
        "        self.label_size = label_size\n",
        "        self.labels = None\n",
        "        self.train_valid_test_int = train_valid_test_int\n",
        "\n",
        "    def get_data_iter(\n",
        "        self, args, batch_size, vocab, train,\n",
        "    ):\n",
        "        return data.DataLoader(\n",
        "            dataset=self.conll_docs,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=train,\n",
        "            num_workers=args.data_workers,\n",
        "            collate_fn=self.collate_func(\n",
        "                args,\n",
        "                return_labels=args.collect_most_popular_labels_steps is not None\n",
        "                and args.collect_most_popular_labels_steps > 0\n",
        "                if train\n",
        "                else True,\n",
        "                vocab=vocab,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def collate_func(self, args, vocab, return_labels):\n",
        "        def collate(batch):\n",
        "            return CONLLEDLDataset_collate_func(\n",
        "                batch=batch,\n",
        "                labels_with_high_model_score=self.labels,\n",
        "                args=args,\n",
        "                return_labels=return_labels,\n",
        "                vocab=vocab,\n",
        "                is_training=self.train_valid_test_int == 0,\n",
        "            )\n",
        "\n",
        "        return collate\n",
        "\n",
        "\n",
        "def CONLLEDLDataset_collate_func(\n",
        "    batch, labels_with_high_model_score, args, return_labels, vocab: Vocab, is_training=False,\n",
        "):\n",
        "    drop_entity_mentions_prob = args.maskout_entity_prob\n",
        "    # print([b[0] for b in batch])\n",
        "    label_size = args.label_size\n",
        "    batch_token_ids = torch.LongTensor([b[0].tolist() for b in batch])\n",
        "    batch_bio_ids = [b[1].tolist() for b in batch]\n",
        "    batch_entity_ids = [b[2].tolist() for b in batch]\n",
        "    batch_doc_ids = [b[3, 0].item() for b in batch]\n",
        "\n",
        "    if return_labels:\n",
        "\n",
        "        all_batch_entity_ids = OrderedDict()\n",
        "\n",
        "        for batch_offset, one_item_entity_ids in enumerate(batch_entity_ids):\n",
        "            for tok_id, eid in enumerate(one_item_entity_ids):\n",
        "                if eid not in all_batch_entity_ids:\n",
        "                    all_batch_entity_ids[eid] = len(all_batch_entity_ids)\n",
        "\n",
        "        if label_size is not None:\n",
        "\n",
        "            batch_shared_label_ids = all_batch_entity_ids.keys()\n",
        "            negative_samples = set()\n",
        "            if labels_with_high_model_score is not None:\n",
        "                # print(labels_with_high_model_score)\n",
        "                negative_samples = set(labels_with_high_model_score)\n",
        "            # else:\n",
        "            #     negative_samples = set(numpy.random.choice(vocab.OUTSIDE_ID, label_size, replace=False))\n",
        "            if len(negative_samples) < label_size:\n",
        "                random_negative_samples = set(numpy.random.choice(vocab.OUTSIDE_ID, label_size, replace=False))\n",
        "                negative_samples = negative_samples.union(random_negative_samples)\n",
        "\n",
        "            negative_samples.difference_update(batch_shared_label_ids)\n",
        "\n",
        "            if len(batch_shared_label_ids) + len(negative_samples) < label_size:\n",
        "                negative_samples.difference_update(\n",
        "                    set(numpy.random.choice(vocab.OUTSIDE_ID, label_size, replace=False))\n",
        "                )\n",
        "\n",
        "            batch_shared_label_ids = (list(batch_shared_label_ids) + list(negative_samples))[:label_size]\n",
        "            label_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), len(batch_shared_label_ids))\n",
        "            bio_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), 3)\n",
        "\n",
        "        else:\n",
        "\n",
        "            batch_shared_label_ids = list(all_batch_entity_ids.keys())\n",
        "            label_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), args.vocab_size)\n",
        "            bio_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), 3)\n",
        "\n",
        "        drop_probs = None\n",
        "        if drop_entity_mentions_prob > 0 and is_training:\n",
        "            drop_probs = torch.rand((batch_token_ids.size(0), batch_token_ids.size(1)),) < drop_entity_mentions_prob\n",
        "\n",
        "        for batch_offset, (one_item_entity_ids, one_item_bio_ids) in enumerate(zip(batch_entity_ids, batch_bio_ids)):\n",
        "            for tok_id, one_entity_ids in enumerate(one_item_entity_ids):\n",
        "\n",
        "                if (\n",
        "                    is_training\n",
        "                    and vocab.OUTSIDE_ID != one_entity_ids\n",
        "                    and drop_entity_mentions_prob > 0\n",
        "                    and drop_probs[batch_offset][tok_id].item() == 1\n",
        "                ):\n",
        "                    batch_token_ids[batch_offset][tok_id] = vocab.tokenizer.vocab[\"[MASK]\"]\n",
        "\n",
        "                if label_size is not None:\n",
        "                    label_probs[batch_offset][tok_id][torch.LongTensor([all_batch_entity_ids[one_entity_ids]])] = 1.0\n",
        "                else:\n",
        "                    label_probs[batch_offset][tok_id][torch.LongTensor(one_entity_ids)] = 1.0\n",
        "                bio_probs[batch_offset][tok_id][torch.LongTensor(one_item_bio_ids)] = 1.0\n",
        "\n",
        "        label_ids = torch.LongTensor(batch_shared_label_ids)\n",
        "\n",
        "        return (\n",
        "            batch_token_ids,\n",
        "            label_ids,\n",
        "            torch.LongTensor(batch_bio_ids),\n",
        "            torch.FloatTensor(label_probs),\n",
        "            bio_probs,\n",
        "            None,\n",
        "            {v: k for k, v in all_batch_entity_ids.items()},\n",
        "            batch_entity_ids,\n",
        "            batch_doc_ids,\n",
        "            batch,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "\n",
        "        return batch_token_ids, None, None, None, None, None, None, None, None, batch"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNOGSYPVj3QX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "# from metrics import Metrics\n",
        "# from data_loader_conll import CONLLEDLDataset_collate_func\n",
        "# from misc import (\n",
        "#     running_mean,\n",
        "#     get_entity_annotations,\n",
        "#     get_entity_annotations_with_gold_spans,\n",
        "#     DummyOptimizer,\n",
        "#     LRSchedulers,\n",
        "#     create_overlapping_chunks,\n",
        "#     get_topk_ids_aggregated_from_seq_prediction,\n",
        "# )\n",
        "# !pip install transformers\n",
        "from transformers import BertModel\n",
        "\n",
        "bio_id = {\n",
        "    \"B\": 0,\n",
        "    \"I\": 1,\n",
        "    \"O\": 2,\n",
        "}\n",
        "bio_id_inv = {\n",
        "    0: \"B\",\n",
        "    1: \"I\",\n",
        "    2: \"O\",\n",
        "}\n",
        "\n",
        "\n",
        "class ConllNet(nn.Module):\n",
        "    def __init__(\n",
        "        self, args, vocab_size=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if args.uncased:\n",
        "            self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        else:\n",
        "            self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "        self.top_rnns = args.top_rnns\n",
        "        if args.top_rnns:\n",
        "            self.rnn = nn.LSTM(bidirectional=True, num_layers=2, input_size=768, hidden_size=768 // 2, batch_first=True)\n",
        "        self.fc = None\n",
        "        if args.project:\n",
        "            self.fc = nn.Linear(768, args.entity_embedding_size)\n",
        "        self.out = nn.Embedding(num_embeddings=vocab_size, embedding_dim=args.entity_embedding_size, sparse=args.sparse)\n",
        "\n",
        "        self.out_segm = nn.Sequential(nn.Dropout(args.bert_dropout), nn.Linear(768, 768), nn.Tanh(), nn.Linear(768, 3),)\n",
        "\n",
        "        # torch.nn.init.normal_(self.out, std=0.1)\n",
        "\n",
        "        if args.bert_dropout and args.bert_dropout > 0:\n",
        "            for m in self.bert.modules():\n",
        "                if isinstance(m, torch.nn.Dropout):\n",
        "                    m.p = args.bert_dropout\n",
        "\n",
        "        self.device = args.device\n",
        "        self.out_device = args.out_device\n",
        "        self.finetuning = args.finetuning\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def to(self, device, out_device):\n",
        "        self.bert.to(device)\n",
        "        if self.fc:\n",
        "            self.fc.to(device)\n",
        "        self.out.to(out_device)\n",
        "        self.out_segm.to(device)\n",
        "        self.device = device\n",
        "        self.out_device = out_device\n",
        "\n",
        "    def forward(self, x, y=None, probs=None, segm_probs=None, enc=None):\n",
        "        \"\"\"\n",
        "        x: (N, T). int64\n",
        "        y: (N, T). int64\n",
        "        Returns\n",
        "        enc: (N, T, VOCAB)\n",
        "        \"\"\"\n",
        "        if y is not None:\n",
        "            y = y.to(self.out_device)\n",
        "        if probs is not None:\n",
        "            probs = probs.to(self.out_device)\n",
        "        if segm_probs is not None:\n",
        "            segm_probs = segm_probs.to(self.out_device)\n",
        "\n",
        "            # fake_y = torch.Tensor(range(10)).long().to(self.device)\n",
        "\n",
        "        if enc is None:\n",
        "            x = x.to(self.device)\n",
        "            if self.training:\n",
        "                if self.finetuning:\n",
        "                    # print(\"->bert.train()\")\n",
        "                    self.bert.train()\n",
        "                    encoded_layers, _ = self.bert(x)\n",
        "                    enc = encoded_layers\n",
        "                else:\n",
        "                    self.bert.eval()\n",
        "                    with torch.no_grad():\n",
        "                        encoded_layers, _ = self.bert(x)\n",
        "                        enc = encoded_layers\n",
        "            else:\n",
        "                encoded_layers, _ = self.bert(x)\n",
        "                enc = encoded_layers\n",
        "\n",
        "            if self.top_rnns:\n",
        "                enc, _ = self.rnn(enc)\n",
        "\n",
        "            if self.fc:\n",
        "                enc = self.fc(enc)\n",
        "\n",
        "            enc = enc.to(self.out_device)\n",
        "\n",
        "        logits_segm = self.out_segm(enc)\n",
        "\n",
        "        if y is not None:\n",
        "            out = self.out(y)\n",
        "            logits = enc.matmul(out.transpose(0, 1))\n",
        "            y_hat = logits.argmax(-1)\n",
        "            bio_y_hat = logits_segm.argmax(-1)\n",
        "            return logits, y, y_hat, probs, segm_probs, out, enc, logits_segm, bio_y_hat\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                out = self.out.weight\n",
        "                logits = enc.matmul(out.transpose(0, 1))\n",
        "                y_hat = logits.argmax(-1)\n",
        "                bio_y_hat = logits_segm.argmax(-1)\n",
        "                return logits, None, y_hat, None, None, None, enc, None, bio_y_hat\n",
        "\n",
        "    @staticmethod\n",
        "    def train_one_epoch(\n",
        "        args,\n",
        "        model,\n",
        "        train_iter,\n",
        "        optimizers,\n",
        "        criterion,\n",
        "        eval_iter,\n",
        "        vocab,\n",
        "        epoch,\n",
        "        metrics=Metrics(),\n",
        "        loss_aggr=None,\n",
        "    ):\n",
        "\n",
        "        with trange(len(train_iter)) as t:\n",
        "            for iter, batch in enumerate(train_iter):\n",
        "\n",
        "                model.to(\n",
        "                    args.device, args.out_device,\n",
        "                )\n",
        "                model.train()\n",
        "\n",
        "                (\n",
        "                    batch_token_ids,\n",
        "                    label_ids,\n",
        "                    _,\n",
        "                    label_probs,\n",
        "                    batch_bio_probs,\n",
        "                    _,\n",
        "                    label_id_to_entity_id_dict,\n",
        "                    batch_entity_ids,\n",
        "                    batch_doc_ids,\n",
        "                    orig_batch,\n",
        "                ) = batch\n",
        "\n",
        "                enc = None\n",
        "\n",
        "                labels_with_high_model_score = list()\n",
        "                if (\n",
        "                    args.collect_most_popular_labels_steps is not None\n",
        "                    and args.collect_most_popular_labels_steps > 0\n",
        "                    and iter > 0\n",
        "                    and iter % args.collect_most_popular_labels_steps == 0\n",
        "                ):\n",
        "                    model.to(args.device, args.eval_device)\n",
        "                    logits, _, y_hat, _, _, _, enc, segm_logits, segm_pred = model(\n",
        "                        batch_token_ids, None, None, batch_bio_probs\n",
        "                    )  # logits: (N, T, VOCAB), y: (N, T)\n",
        "                    labels_with_high_model_score = get_topk_ids_aggregated_from_seq_prediction(\n",
        "                        logits, topk_from_batch=args.label_size, topk_per_token=args.topk_neg_examples\n",
        "                    )\n",
        "                    (\n",
        "                        batch_token_ids,\n",
        "                        label_ids,\n",
        "                        _,\n",
        "                        label_probs,\n",
        "                        batch_bio_probs,\n",
        "                        _,\n",
        "                        label_id_to_entity_id_dict,\n",
        "                        batch_entity_ids,\n",
        "                        batch_doc_ids,\n",
        "                        orig_batch,\n",
        "                    ) = CONLLEDLDataset_collate_func(\n",
        "                        args=args,\n",
        "                        labels_with_high_model_score=labels_with_high_model_score,\n",
        "                        batch=orig_batch,\n",
        "                        return_labels=True,\n",
        "                        vocab=vocab,\n",
        "                    )\n",
        "\n",
        "                # if args.label_size is not None:\n",
        "                logits, y, y_hat, label_probs, batch_bio_probs, sparse_params, _, segm_logits, segm_pred = model(\n",
        "                    batch_token_ids, label_ids, label_probs, batch_bio_probs, enc=enc\n",
        "                )  # logits: (N, T, VOCAB), y: (N, T)\n",
        "                # else:\n",
        "                #     logits, y, y_hat, label_probs, sparse_params = model(batch_token_ids, None, label_probs) # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "                # logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
        "                logits = logits.view(-1)  # (N*T, VOCAB)\n",
        "                segm_logits = segm_logits.view(-1)  # (N*T, VOCAB)\n",
        "                label_probs = label_probs.view(-1)  # (N*T,)\n",
        "                batch_bio_probs = batch_bio_probs.view(-1)\n",
        "\n",
        "                task_importance_ratio = 0.1\n",
        "\n",
        "                if args.learn_segmentation:\n",
        "                    loss = (1 - task_importance_ratio) * criterion(\n",
        "                        logits, label_probs\n",
        "                    ) + task_importance_ratio * criterion(segm_logits, batch_bio_probs)\n",
        "                else:\n",
        "                    loss = criterion(logits, label_probs)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                if (iter + 1) % args.accumulate_batch_gradients == 0:\n",
        "                    for optimizer in optimizers:\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                if iter == 0:\n",
        "                    logging.debug(\"=====sanity check======\")\n",
        "                    logging.debug(\"x:\", batch_token_ids.cpu().numpy()[0])\n",
        "                    logging.debug(\"tokens:\", vocab.tokenizer.convert_ids_to_tokens(batch_token_ids.cpu().numpy()[0]))\n",
        "                    logging.debug(\"y:\", label_probs.cpu().numpy()[0])\n",
        "                    logging.debug(\"=======================\")\n",
        "\n",
        "                loss_aggr = running_mean(loss.detach().item(), loss_aggr)\n",
        "\n",
        "                if iter > 0 and iter % args.checkpoint_eval_steps == 0:\n",
        "                    metrics = ConllNet.evaluate(\n",
        "                        args=args,\n",
        "                        model=model,\n",
        "                        iterator=eval_iter,\n",
        "                        optimizers=optimizers,\n",
        "                        step=iter,\n",
        "                        epoch=epoch,\n",
        "                        save_checkpoint=iter % args.checkpoint_save_steps == 0,\n",
        "                        sampled_evaluation=False,\n",
        "                        metrics=metrics,\n",
        "                        vocab=vocab,\n",
        "                    )\n",
        "\n",
        "                t.set_postfix(\n",
        "                    loss=loss_aggr,\n",
        "                    # nr_labels=len(label_ids),\n",
        "                    # aggr_labels=len(labels_with_high_model_score) if labels_with_high_model_score else 0,\n",
        "                    last_eval=metrics.report(filter={\"f1\", \"span_f1\", \"lenient_span_f1\", \"epoch\", \"step\"}),\n",
        "                )\n",
        "                t.update()\n",
        "\n",
        "        for optimizer in optimizers:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate(\n",
        "        args,\n",
        "        model,\n",
        "        iterator,\n",
        "        vocab,\n",
        "        optimizers,\n",
        "        step=0,\n",
        "        epoch=0,\n",
        "        save_checkpoint=True,\n",
        "        save_predictions=True,\n",
        "        save_csv=True,\n",
        "        sampled_evaluation=False,\n",
        "        metrics=Metrics(),\n",
        "    ):\n",
        "\n",
        "        logging.info(f\"Start evaluation on split {'test' if args.eval_on_test_only else 'valid'}\")\n",
        "\n",
        "        model.eval()\n",
        "        model.to(args.device, args.eval_device)\n",
        "\n",
        "        chunk_len = args.create_integerized_training_instance_text_length\n",
        "        chunk_overlap = args.create_integerized_training_instance_text_overlap\n",
        "\n",
        "        all_words = list()\n",
        "        all_tags = [0] * (len(iterator) * args.eval_batch_size * (chunk_len))\n",
        "        all_y = [0] * (len(iterator) * args.eval_batch_size * (chunk_len))\n",
        "        all_y_hat = [0] * (len(iterator) * args.eval_batch_size * (chunk_len))\n",
        "        all_segm_preds = [0] * (len(iterator) * args.eval_batch_size * (chunk_len))\n",
        "        all_y_hat_gold_mentions = [0] * (len(iterator) * args.eval_batch_size * (chunk_len))\n",
        "        all_logits = [0] * (len(iterator) * args.eval_batch_size * (chunk_len))\n",
        "        all_predicted = [0] * (len(iterator) * args.eval_batch_size * (chunk_len))\n",
        "        all_token_ids = [0] * (len(iterator) * args.eval_batch_size * (chunk_len))\n",
        "\n",
        "        all_y_hat_scores = torch.ones(len(iterator) * args.eval_batch_size * (chunk_len)) * -1e10\n",
        "        all_y_hat_gold_mentions_scores = torch.ones(len(iterator) * args.eval_batch_size * (chunk_len)) * -1e10\n",
        "\n",
        "        best_scores = torch.ones(len(iterator) * args.eval_batch_size * (chunk_len)) * -1e10\n",
        "\n",
        "        offset = 0\n",
        "        last_doc = -1\n",
        "\n",
        "        # new_best_top1_logit = torch.ones((chunk_len))\n",
        "        # new_best_top2_logit_gold_mentions = torch.ones((chunk_len))\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for iter, batch in enumerate(tqdm(iterator)):\n",
        "\n",
        "                (\n",
        "                    batch_token_ids,\n",
        "                    label_ids,\n",
        "                    batch_bio_ids,\n",
        "                    label_probs,\n",
        "                    batch_bio_probs,\n",
        "                    _,\n",
        "                    label_id_to_entity_id_dict,\n",
        "                    batch_entity_ids,\n",
        "                    batch_doc_ids,\n",
        "                    orig_batch,\n",
        "                ) = batch\n",
        "                eval_mask = batch_bio_ids == 2\n",
        "                logits, y, y_hat, probs, batch_bio_probs, _, _, segm_logits, segm_preds = model(\n",
        "                    batch_token_ids, None, label_probs, batch_bio_probs\n",
        "                )  # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "                logits = logits[:, 1:-1, :]\n",
        "                label_probs = label_probs[:, 1:-1, :]\n",
        "                y_hat = y_hat[:, 1:-1]\n",
        "                segm_preds = segm_preds[:, 1:-1]\n",
        "                eval_mask = eval_mask[:, 1:-1]\n",
        "                batch_token_ids = batch_token_ids[:, 1:-1]\n",
        "\n",
        "                top2_logit, top2 = logits.topk(k=2, dim=-1,)\n",
        "                top2_select = (y_hat >= vocab.OUTSIDE_ID).long()\n",
        "\n",
        "                y_hat_gold_mentions = (\n",
        "                    top2.view(-1, top2.size(-1)).gather(dim=1, index=top2_select.view(-1, 1)).view(y_hat.size())\n",
        "                )\n",
        "                top2_logit_gold_mentions = (\n",
        "                    top2_logit.view(-1, top2_logit.size(-1))\n",
        "                    .gather(dim=1, index=top2_select.view(-1, 1))\n",
        "                    .view(y_hat.size())\n",
        "                    .to(\"cpu\")\n",
        "                )\n",
        "\n",
        "                top1_logit, _ = logits.to(\"cpu\").max(dim=-1,)\n",
        "                top1_probs = torch.sigmoid(top1_logit)\n",
        "\n",
        "                for batch_id, seq in enumerate(label_probs.max(-1)[1]):\n",
        "\n",
        "                    if last_doc >= 0:\n",
        "                        if last_doc == batch_doc_ids[batch_id]:\n",
        "                            next_step = chunk_len - chunk_overlap\n",
        "                        else:\n",
        "                            last_doc = batch_doc_ids[batch_id]\n",
        "                            next_step = chunk_len\n",
        "\n",
        "                        offset += next_step\n",
        "                    else:\n",
        "                        last_doc = batch_doc_ids[batch_id]\n",
        "\n",
        "                    new_best_top1_logit = top1_logit[batch_id] > best_scores[offset : offset + chunk_len]\n",
        "                    new_best_top2_logit_gold_mentions = (\n",
        "                        top2_logit_gold_mentions[batch_id] > best_scores[offset : offset + chunk_len]\n",
        "                    )\n",
        "\n",
        "                    all_y_hat_scores[offset : offset + chunk_len] = (\n",
        "                        new_best_top1_logit.float() * top1_logit[batch_id]\n",
        "                        + (1.0 - new_best_top1_logit.float()) * all_y_hat_scores[offset : offset + chunk_len]\n",
        "                    )\n",
        "                    all_y_hat_gold_mentions_scores[offset : offset + chunk_len] = (\n",
        "                        new_best_top2_logit_gold_mentions.float() * top1_logit[batch_id]\n",
        "                        + (1.0 - new_best_top2_logit_gold_mentions.float())\n",
        "                        * all_y_hat_gold_mentions_scores[offset : offset + chunk_len]\n",
        "                    )\n",
        "\n",
        "                    for tok_id, label_id in enumerate(seq):\n",
        "\n",
        "                        y_resolved = (\n",
        "                            label_ids[label_id].item() if eval_mask[batch_id][tok_id] == 0 else vocab.OUTSIDE_ID\n",
        "                        )\n",
        "                        all_y[offset + tok_id] = y_resolved\n",
        "                        all_tags[offset + tok_id] = vocab.idx2tag[y_resolved]\n",
        "\n",
        "                        y_hat_resolved = (\n",
        "                            new_best_top1_logit[tok_id].item() * y_hat[batch_id][tok_id].item()\n",
        "                            + (1 - new_best_top1_logit[tok_id].item()) * all_y_hat[offset + tok_id]\n",
        "                        )\n",
        "                        all_y_hat[offset + tok_id] = y_hat_resolved\n",
        "                        all_predicted[offset + tok_id] = vocab.idx2tag[y_hat_resolved]\n",
        "\n",
        "                        y_hat_gold_mentions_resolved = (\n",
        "                            new_best_top2_logit_gold_mentions[tok_id].item()\n",
        "                            * y_hat_gold_mentions[batch_id][tok_id].item()\n",
        "                            + (1 - new_best_top2_logit_gold_mentions[tok_id].item()) * all_y_hat[offset + tok_id]\n",
        "                        )\n",
        "                        all_y_hat_gold_mentions[offset + tok_id] = y_hat_gold_mentions_resolved\n",
        "\n",
        "                        all_segm_preds[offset + tok_id] = segm_preds[batch_id][tok_id].item()\n",
        "                        all_token_ids[offset + tok_id] = batch_token_ids[batch_id][tok_id].item()\n",
        "                        all_logits[offset + tok_id] = top1_probs[batch_id][tok_id].item()\n",
        "\n",
        "        all_tags = all_tags[: offset + chunk_len]\n",
        "        all_y = all_y[: offset + chunk_len]\n",
        "        all_y_hat = all_y_hat[: offset + chunk_len]\n",
        "        all_y_hat_gold_mentions = all_y_hat_gold_mentions[: offset + chunk_len]\n",
        "        all_logits = all_logits[: offset + chunk_len]\n",
        "        all_predicted = all_predicted[: offset + chunk_len]\n",
        "        all_token_ids = all_token_ids[: offset + chunk_len]\n",
        "        all_segm_preds = all_segm_preds[: offset + chunk_len]\n",
        "\n",
        "        for chunk in create_overlapping_chunks(all_token_ids, 512, 0):\n",
        "            all_words.extend(vocab.tokenizer.convert_ids_to_tokens(chunk))\n",
        "\n",
        "        ## calc metric\n",
        "        y_true = numpy.array(all_y)\n",
        "        y_pred = numpy.array(all_y_hat)\n",
        "        y_pred_gold_mentions = numpy.array(all_y_hat_gold_mentions)\n",
        "        all_token_ids = numpy.array(all_token_ids)\n",
        "\n",
        "        spans_true = get_entity_annotations(y_true, vocab.OUTSIDE_ID)\n",
        "        spans_pred = get_entity_annotations(y_pred, vocab.OUTSIDE_ID)\n",
        "        spans_pred_gold_mentions = get_entity_annotations_with_gold_spans(\n",
        "            y_pred_gold_mentions, y_true, vocab.OUTSIDE_ID\n",
        "        )\n",
        "\n",
        "        overlaps = list()\n",
        "        for anno in spans_pred:\n",
        "            overlaps.extend(filter(lambda s: len(set(anno[0]) & set(s[0])) > 0 and anno[1] == s[1], spans_true))\n",
        "\n",
        "        overlaps_gold_mentions = list()\n",
        "        for anno in spans_pred_gold_mentions:\n",
        "            overlaps_gold_mentions.extend(\n",
        "                filter(lambda s: len(set(anno[0]) & set(s[0])) > 0 and anno[1] == s[1], spans_true)\n",
        "            )\n",
        "        num_lenient_correct_gold_mentions = len(set(overlaps_gold_mentions))\n",
        "\n",
        "        num_proposed = len(y_pred[(vocab.OUTSIDE_ID > y_pred) & (all_token_ids > 0)])\n",
        "        num_correct = (((y_true == y_pred) & (vocab.OUTSIDE_ID > y_true) & (all_token_ids > 0))).astype(numpy.int).sum()\n",
        "\n",
        "        num_correct_gold_mentions = (\n",
        "            (((y_true == y_pred_gold_mentions) & (vocab.OUTSIDE_ID > y_true) & (all_token_ids > 0)))\n",
        "            .astype(numpy.int)\n",
        "            .sum()\n",
        "        )\n",
        "\n",
        "        num_gold = len(y_true[(vocab.OUTSIDE_ID > y_true) & (all_token_ids > 0)])\n",
        "\n",
        "        num_spans_correct = len(set(spans_true).intersection(set(spans_pred)))\n",
        "        num_spans_true = len(set(spans_true))\n",
        "        num_spans_proposed = len(set(spans_pred)) if len(set(spans_pred)) > 0 else 0\n",
        "\n",
        "        num_lenient_correct_spans = len(set(overlaps))\n",
        "\n",
        "        new_metrics = Metrics(\n",
        "            epoch=epoch,\n",
        "            step=step,\n",
        "            num_correct=num_correct,\n",
        "            num_gold=num_gold,\n",
        "            num_proposed=num_proposed,\n",
        "            # in this setting all gold mentions are scored which is why num_gold == num_proposed\n",
        "            precision_gold_mentions=Metrics.compute_precision(correct=num_correct_gold_mentions, proposed=num_gold),\n",
        "            span_precision=Metrics.compute_precision(correct=num_spans_correct, proposed=num_spans_proposed),\n",
        "            span_recall=Metrics.compute_recall(correct=num_spans_correct, gold=num_spans_true),\n",
        "            span_f1=Metrics.compute_fmeasure(\n",
        "                precision=Metrics.compute_precision(correct=num_spans_correct, proposed=num_spans_proposed),\n",
        "                recall=Metrics.compute_recall(correct=num_spans_correct, gold=num_spans_true),\n",
        "            ),\n",
        "            lenient_span_precision=Metrics.compute_precision(\n",
        "                correct=num_lenient_correct_spans, proposed=num_spans_proposed\n",
        "            ),\n",
        "            lenient_span_recall=Metrics.compute_recall(correct=num_lenient_correct_spans, gold=num_spans_true),\n",
        "            lenient_span_f1=Metrics.compute_fmeasure(\n",
        "                precision=Metrics.compute_precision(correct=num_lenient_correct_spans, proposed=num_spans_proposed),\n",
        "                recall=Metrics.compute_recall(correct=num_lenient_correct_spans, gold=num_spans_true),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if save_predictions:\n",
        "            final = (\n",
        "                args.logdir\n",
        "                + \"/{}-{}-MENTION-S_P_{:.2f}_R_{:.2f}_F1_{:.2f}-LS_P_{:.2f}_R_{:.2f}_F1_{:.2f}-T_P_{:.2f}_R_{:.2f}_F1_{:.2f}-LINK-S_P_{:.2f}.txt\".format(\n",
        "                    epoch,\n",
        "                    step,\n",
        "                    new_metrics.span_precision,\n",
        "                    new_metrics.span_recall,\n",
        "                    new_metrics.span_f1,\n",
        "                    new_metrics.lenient_span_precision,\n",
        "                    new_metrics.lenient_span_recall,\n",
        "                    new_metrics.lenient_span_f1,\n",
        "                    new_metrics.precision,\n",
        "                    new_metrics.recall,\n",
        "                    new_metrics.f1,\n",
        "                    new_metrics.precision_gold_mentions,\n",
        "                )\n",
        "            )\n",
        "            with open(final, \"w\") as fout:\n",
        "\n",
        "                for words, tags, y_hat, preds, segm_pred, logits in zip(\n",
        "                    all_words, all_tags, all_y_hat, all_predicted, all_segm_preds, all_logits\n",
        "                ):\n",
        "                    fout.write(f\"{words}\\t{tags}\\t{preds}\\t{bio_id_inv[segm_pred]}\\t{logits}\\n\")\n",
        "\n",
        "                fout.write(f\"num_proposed:{new_metrics.num_proposed}\\n\")\n",
        "                fout.write(f\"num_correct:{new_metrics.num_correct}\\n\")\n",
        "                fout.write(f\"num_gold:{new_metrics.num_gold}\\n\")\n",
        "                fout.write(f\"precision={new_metrics.precision}\\n\")\n",
        "                fout.write(f\"precision_gold_mentions={new_metrics.precision_gold_mentions}\\n\")\n",
        "                fout.write(f\"recall={new_metrics.recall}\\n\")\n",
        "                fout.write(f\"f1={new_metrics.f1}\\n\")\n",
        "\n",
        "        if not args.dont_save_checkpoints:\n",
        "            if save_checkpoint or metrics.was_improved(new_metrics):\n",
        "                config = {\n",
        "                    \"args\": args,\n",
        "                    \"optimizer_dense\": optimizers[0].state_dict() if optimizers else None,\n",
        "                    \"optimizer_sparse\": optimizers[1].state_dict() if optimizers else None,\n",
        "                    \"model\": model.state_dict(),\n",
        "                    \"epoch\": epoch,\n",
        "                    \"step\": step,\n",
        "                    \"performance\": new_metrics.dict(),\n",
        "                }\n",
        "                fname = os.path.join(args.logdir, \"{}-{}\".format(str(epoch), str(step)))\n",
        "                torch.save(config, f\"{fname}.pt\")\n",
        "                fname = os.path.join(args.logdir, new_metrics.get_best_checkpoint_filename())\n",
        "                torch.save(config, f\"{fname}.pt\")\n",
        "                logging.info(f\"weights were saved to {fname}.pt\")\n",
        "\n",
        "        if save_csv:\n",
        "            new_metrics.to_csv(epoch=epoch, step=step, args=args)\n",
        "\n",
        "        if metrics.was_improved(new_metrics):\n",
        "            metrics.update(new_metrics)\n",
        "\n",
        "        logging.info(\"Finished evaluation\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def get_optimizers(self, args, checkpoint):\n",
        "\n",
        "        optimizers = list()\n",
        "\n",
        "        if args.encoder_lr > 0:\n",
        "            if args.exclude_parameter_names_regex is not None:\n",
        "                bert_parameters = list()\n",
        "                regex = re.compile(args.exclude_parameter_names_regex)\n",
        "                for n, p in list(self.bert.named_parameters()):\n",
        "                    if not len(regex.findall(n)) > 0:\n",
        "                        bert_parameters.append(p)\n",
        "            else:\n",
        "                bert_parameters = list(self.bert.parameters())\n",
        "            optimizer_encoder = optim.Adam(\n",
        "                bert_parameters + list(self.fc.parameters() if args.project else list()), lr=args.encoder_lr\n",
        "            )\n",
        "            # optimizer_encoder = BertAdam(bert_parameters + list(self.fc.parameters() if args.project else list()),\n",
        "            #                      lr=args.encoder_lr,\n",
        "            # )\n",
        "\n",
        "            if args.resume_optimizer_from_checkpoint:\n",
        "                optimizer_encoder.load_state_dict(checkpoint[\"optimizer_dense\"])\n",
        "                optimizer_encoder.param_groups[0][\"lr\"] = args.encoder_lr\n",
        "                optimizer_encoder.param_groups[0][\"weight_decay\"] = args.encoder_weight_decay\n",
        "            optimizers.append(optimizer_encoder)\n",
        "        else:\n",
        "            optimizers.append(DummyOptimizer(self.out.parameters(), defaults={}))\n",
        "\n",
        "        if args.decoder_lr > 0:\n",
        "            if args.sparse:\n",
        "                optimizer_decoder = optim.SparseAdam(self.out.parameters(), lr=args.decoder_lr)\n",
        "            else:\n",
        "                optimizer_decoder = optim.Adam(self.out.parameters(), lr=args.decoder_lr)\n",
        "            if args.resume_from_checkpoint is not None:\n",
        "                optimizer_decoder.load_state_dict(checkpoint[\"optimizer_sparse\"])\n",
        "                if \"weight_decay\" not in optimizer_decoder.param_groups[0]:\n",
        "                    optimizer_decoder.param_groups[0][\"weight_decay\"] = 0\n",
        "                optimizer_decoder.param_groups[0][\"lr\"] = args.decoder_lr\n",
        "                if not args.sparse:\n",
        "                    optimizer_decoder.param_groups[0][\"weight_decay\"] = args.decoder_weight_decay\n",
        "            optimizers.append(optimizer_decoder)\n",
        "        else:\n",
        "            optimizers.append(DummyOptimizer(self.out.parameters(), defaults={}))\n",
        "\n",
        "        if args.segm_decoder_lr > 0:\n",
        "            optimizer_segm_decoder = optim.Adam(self.out_segm.parameters(), lr=args.segm_decoder_lr)\n",
        "            if args.resume_optimizer_from_checkpoint:\n",
        "                optimizer_segm_decoder.param_groups[0][\"lr\"] = args.segm_decoder_lr\n",
        "                optimizer_segm_decoder.param_groups[0][\"weight_decay\"] = args.segm_decoder_weight_decay\n",
        "            optimizers.append(optimizer_segm_decoder)\n",
        "        else:\n",
        "            optimizers.append(DummyOptimizer(self.out.parameters(), defaults={}))\n",
        "\n",
        "        lr_schedulers = [\n",
        "            getattr(LRSchedulers, lr_scheduler)(optimizer=optimizer, **lr_scheduler_config)\n",
        "            for optimizer, (lr_scheduler, lr_scheduler_config) in zip(\n",
        "                optimizers,\n",
        "                [\n",
        "                    (args.encoder_lr_scheduler, args.encoder_lr_scheduler_config),\n",
        "                    (args.decoder_lr_scheduler, args.decoder_lr_scheduler_config),\n",
        "                    (args.segm_decoder_lr_scheduler, args.segm_decoder_lr_scheduler_config),\n",
        "                ],\n",
        "            )\n",
        "            if lr_scheduler is not None  # and not isinstance(optimizer, DummyOptimizer)\n",
        "        ]\n",
        "\n",
        "        return tuple(optimizers), tuple(lr_schedulers)\n"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxox127Ome-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class CONLLEDLDataset(data.Dataset):\n",
        "    def __init__(self, args, split, vocab, device, label_size=None):\n",
        "\n",
        "        if split == \"train\":\n",
        "            train_valid_test_int = 0\n",
        "        if split == \"small_valid\" or split == \"valid\":\n",
        "            train_valid_test_int = 1\n",
        "        if split == \"test\":\n",
        "            train_valid_test_int = 2\n",
        "\n",
        "        chunk_len = args.create_integerized_training_instance_text_length\n",
        "        chunk_overlap = args.create_integerized_training_instance_text_overlap\n",
        "\n",
        "        self.item_locs = None\n",
        "        self.device = device\n",
        "        with open(args.data_path_conll, \"rb\") as f:\n",
        "            train_valid_test = pickle.load(f)\n",
        "            self.conll_docs = torch.LongTensor(\n",
        "                [\n",
        "                    [\n",
        "                        pad_to(\n",
        "                            [tok_id for _, tok_id, _, _, _, _, _ in doc],\n",
        "                            max_len=chunk_len + 2,\n",
        "                            pad_id=0,\n",
        "                            cls_id=101,\n",
        "                            sep_id=102,\n",
        "                        )\n",
        "                        for doc in train_valid_test[train_valid_test_int]\n",
        "                    ],\n",
        "                    [\n",
        "                        pad_to(\n",
        "                            [bio_id for _, _, _, bio_id, _, _, _ in doc],\n",
        "                            max_len=chunk_len + 2,\n",
        "                            pad_id=2,\n",
        "                            cls_id=2,\n",
        "                            sep_id=2,\n",
        "                        )\n",
        "                        for doc in train_valid_test[train_valid_test_int]\n",
        "                    ],\n",
        "                    [\n",
        "                        pad_to(\n",
        "                            [wiki_id for _, _, _, _, _, wiki_id, _ in doc],\n",
        "                            max_len=chunk_len + 2,\n",
        "                            pad_id=vocab.PAD_ID,\n",
        "                            cls_id=vocab.PAD_ID,\n",
        "                            sep_id=vocab.PAD_ID,\n",
        "                        )\n",
        "                        for doc in train_valid_test[train_valid_test_int]\n",
        "                    ],\n",
        "                    [\n",
        "                        pad_to(\n",
        "                            [doc_id for _, _, _, _, _, _, doc_id in doc],\n",
        "                            max_len=chunk_len + 2,\n",
        "                            pad_id=0,\n",
        "                            cls_id=0,\n",
        "                            sep_id=0,\n",
        "                        )\n",
        "                        for doc in train_valid_test[train_valid_test_int]\n",
        "                    ],\n",
        "                ]\n",
        "            ).permute(1, 0, 2)\n",
        "            self.conll_docs[:, 2] = set_out_id(self.conll_docs[:, 2], vocab.OUTSIDE_ID)\n",
        "\n",
        "        self.pad_token_id = vocab.PAD_ID\n",
        "        self.label_size = label_size\n",
        "        self.labels = None\n",
        "        self.train_valid_test_int = train_valid_test_int\n",
        "\n",
        "    def get_data_iter(\n",
        "        self, args, batch_size, vocab, train,\n",
        "    ):\n",
        "        return data.DataLoader(\n",
        "            dataset=self.conll_docs,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=train,\n",
        "            num_workers=args.data_workers,\n",
        "            collate_fn=self.collate_func(\n",
        "                args,\n",
        "                return_labels=args.collect_most_popular_labels_steps is not None\n",
        "                and args.collect_most_popular_labels_steps > 0\n",
        "                if train\n",
        "                else True,\n",
        "                vocab=vocab,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def collate_func(self, args, vocab, return_labels):\n",
        "        def collate(batch):\n",
        "            return CONLLEDLDataset_collate_func(\n",
        "                batch=batch,\n",
        "                labels_with_high_model_score=self.labels,\n",
        "                args=args,\n",
        "                return_labels=return_labels,\n",
        "                vocab=vocab,\n",
        "                is_training=self.train_valid_test_int == 0,\n",
        "            )\n",
        "\n",
        "        return collate\n",
        "\n",
        "\n",
        "def CONLLEDLDataset_collate_func(\n",
        "    batch, labels_with_high_model_score, args, return_labels, vocab: Vocab, is_training=False,\n",
        "):\n",
        "    drop_entity_mentions_prob = args.maskout_entity_prob\n",
        "    # print([b[0] for b in batch])\n",
        "    label_size = args.label_size\n",
        "    batch_token_ids = torch.LongTensor([b[0].tolist() for b in batch])\n",
        "    batch_bio_ids = [b[1].tolist() for b in batch]\n",
        "    batch_entity_ids = [b[2].tolist() for b in batch]\n",
        "    batch_doc_ids = [b[3, 0].item() for b in batch]\n",
        "\n",
        "    if return_labels:\n",
        "\n",
        "        all_batch_entity_ids = OrderedDict()\n",
        "\n",
        "        for batch_offset, one_item_entity_ids in enumerate(batch_entity_ids):\n",
        "            for tok_id, eid in enumerate(one_item_entity_ids):\n",
        "                if eid not in all_batch_entity_ids:\n",
        "                    all_batch_entity_ids[eid] = len(all_batch_entity_ids)\n",
        "\n",
        "        if label_size is not None:\n",
        "\n",
        "            batch_shared_label_ids = all_batch_entity_ids.keys()\n",
        "            negative_samples = set()\n",
        "            if labels_with_high_model_score is not None:\n",
        "                # print(labels_with_high_model_score)\n",
        "                negative_samples = set(labels_with_high_model_score)\n",
        "            # else:\n",
        "            #     negative_samples = set(numpy.random.choice(vocab.OUTSIDE_ID, label_size, replace=False))\n",
        "            if len(negative_samples) < label_size:\n",
        "                random_negative_samples = set(numpy.random.choice(vocab.OUTSIDE_ID, label_size, replace=False))\n",
        "                negative_samples = negative_samples.union(random_negative_samples)\n",
        "\n",
        "            negative_samples.difference_update(batch_shared_label_ids)\n",
        "\n",
        "            if len(batch_shared_label_ids) + len(negative_samples) < label_size:\n",
        "                negative_samples.difference_update(\n",
        "                    set(numpy.random.choice(vocab.OUTSIDE_ID, label_size, replace=False))\n",
        "                )\n",
        "\n",
        "            batch_shared_label_ids = (list(batch_shared_label_ids) + list(negative_samples))[:label_size]\n",
        "            label_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), len(batch_shared_label_ids))\n",
        "            bio_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), 3)\n",
        "\n",
        "        else:\n",
        "\n",
        "            batch_shared_label_ids = list(all_batch_entity_ids.keys())\n",
        "            label_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), args.vocab_size)\n",
        "            bio_probs = torch.zeros(batch_token_ids.size(0), batch_token_ids.size(1), 3)\n",
        "\n",
        "        drop_probs = None\n",
        "        if drop_entity_mentions_prob > 0 and is_training:\n",
        "            drop_probs = torch.rand((batch_token_ids.size(0), batch_token_ids.size(1)),) < drop_entity_mentions_prob\n",
        "\n",
        "        for batch_offset, (one_item_entity_ids, one_item_bio_ids) in enumerate(zip(batch_entity_ids, batch_bio_ids)):\n",
        "            for tok_id, one_entity_ids in enumerate(one_item_entity_ids):\n",
        "\n",
        "                if (\n",
        "                    is_training\n",
        "                    and vocab.OUTSIDE_ID != one_entity_ids\n",
        "                    and drop_entity_mentions_prob > 0\n",
        "                    and drop_probs[batch_offset][tok_id].item() == 1\n",
        "                ):\n",
        "                    batch_token_ids[batch_offset][tok_id] = vocab.tokenizer.vocab[\"[MASK]\"]\n",
        "\n",
        "                if label_size is not None:\n",
        "                    label_probs[batch_offset][tok_id][torch.LongTensor([all_batch_entity_ids[one_entity_ids]])] = 1.0\n",
        "                else:\n",
        "                    label_probs[batch_offset][tok_id][torch.LongTensor(one_entity_ids)] = 1.0\n",
        "                bio_probs[batch_offset][tok_id][torch.LongTensor(one_item_bio_ids)] = 1.0\n",
        "\n",
        "        label_ids = torch.LongTensor(batch_shared_label_ids)\n",
        "\n",
        "        return (\n",
        "            batch_token_ids,\n",
        "            label_ids,\n",
        "            torch.LongTensor(batch_bio_ids),\n",
        "            torch.FloatTensor(label_probs),\n",
        "            bio_probs,\n",
        "            None,\n",
        "            {v: k for k, v in all_batch_entity_ids.items()},\n",
        "            batch_entity_ids,\n",
        "            batch_doc_ids,\n",
        "            batch,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "\n",
        "        return batch_token_ids, None, None, None, None, None, None, None, None, batch"
      ],
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWU6uECXM9kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch.cuda\n",
        "import torch.nn as nn\n",
        "\n",
        "# # from metrics import Metrics\n",
        "# from data_loader_conll import CONLLEDLDataset\n",
        "# from data_loader_wiki import EDLDataset\n",
        "# from model import Net\n",
        "# from model_conll import ConllNet\n",
        "# from train_util import get_args\n",
        "# from vocab import Vocab\n",
        "\n",
        "\n",
        "class Datasets:\n",
        "    EDLDataset = EDLDataset\n",
        "    CONLLEDLDataset = CONLLEDLDataset\n",
        "\n",
        "\n",
        "class Models:\n",
        "    Net = Net\n",
        "    ConllNet = ConllNet\n"
      ],
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFloxOwwepEc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "58412443-acf0-4409-df8e-21c120c771f4"
      },
      "source": [
        "import ast\n",
        "import os\n",
        "!pip install configargparse\n",
        "import configargparse as argparse\n",
        "import torch.cuda\n",
        "import yaml\n",
        "parser = argparse.ArgumentParser()\n",
        "print(parser)\n",
        "parser.add_argument(\"-c\", \"--config\", is_config_file=True, help=\"config file path\")\n",
        "parser.add_argument(\"--debug\", type=argparse_bool_type, default=False)\n",
        "parser.add_argument(\"--device\", default=0)\n",
        "parser.add_argument(\"--eval_device\", default=None)\n",
        "parser.add_argument(\"--dataset\", default=\"EDLDataset\")\n",
        "parser.add_argument(\"--model\", default=\"Net\")\n",
        "parser.add_argument(\"--data_version_name\")\n",
        "parser.add_argument(\"--wiki_lang_version\")\n",
        "parser.add_argument(\"--eval_on_test_only\", type=argparse_bool_type, default=False)\n",
        "parser.add_argument(\"--out_device\", default=None)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
        "parser.add_argument(\"--eval_batch_size\", type=int, default=128)\n",
        "parser.add_argument(\"--accumulate_batch_gradients\", type=int, default=1)\n",
        "parser.add_argument(\"--sparse\", dest=\"sparse\", type=argparse_bool_type)\n",
        "parser.add_argument(\"--encoder_lr\", type=float, default=5e-5)\n",
        "parser.add_argument(\"--decoder_lr\", type=float, default=1e-3)\n",
        "parser.add_argument(\"--maskout_entity_prob\", type=float, default=0)\n",
        "parser.add_argument(\"--segm_decoder_lr\", type=float, default=1e-3)\n",
        "parser.add_argument(\"--encoder_weight_decay\", type=float, default=0)\n",
        "parser.add_argument(\"--decoder_weight_decay\", type=float, default=0)\n",
        "parser.add_argument(\"--segm_decoder_weight_decay\", type=float, default=0)\n",
        "parser.add_argument(\"--learn_segmentation\", type=argparse_bool_type, default=False)\n",
        "parser.add_argument(\"--label_size\", type=int)\n",
        "parser.add_argument(\"--vocab_size\", type=int)\n",
        "parser.add_argument(\"--entity_embedding_size\", type=int, default=768)\n",
        "parser.add_argument(\"--project\", type=argparse_bool_type, default=False)\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=1000)\n",
        "parser.add_argument(\"--collect_most_popular_labels_steps\", type=int, default=100)\n",
        "parser.add_argument(\"--checkpoint_eval_steps\", type=int, default=1000)\n",
        "parser.add_argument(\"--checkpoint_save_steps\", type=int, default=50000)\n",
        "parser.add_argument(\"--finetuning\", dest=\"finetuning\", type=int, default=9999999999)\n",
        "parser.add_argument(\"--top_rnns\", dest=\"top_rnns\", type=argparse_bool_type)\n",
        "parser.add_argument(\"--logdir\", type=str)\n",
        "parser.add_argument(\"--train_loc_file\", type=str, default=\"train.loc\")\n",
        "parser.add_argument(\"--valid_loc_file\", type=str, default=\"valid.loc\")\n",
        "parser.add_argument(\"--test_loc_file\", type=str, default=\"test.loc\")\n",
        "parser.add_argument(\"--resume_from_checkpoint\", type=str)\n",
        "parser.add_argument(\"--resume_reset_epoch\", type=argparse_bool_type, default=False)\n",
        "parser.add_argument(\"--resume_optimizer_from_checkpoint\", type=argparse_bool_type, default=False)\n",
        "parser.add_argument(\"--topk_neg_examples\", type=int, default=3)\n",
        "parser.add_argument(\"--dont_save_checkpoints\", type=argparse_bool_type, default=False)\n",
        "parser.add_argument(\"--data_workers\", type=int, default=8)\n",
        "parser.add_argument(\"--bert_dropout\", type=float, default=None)\n",
        "parser.add_argument(\"--encoder_lr_scheduler\", type=str, default=None)\n",
        "parser.add_argument(\"--encoder_lr_scheduler_config\", default=None)\n",
        "parser.add_argument(\"--decoder_lr_scheduler\", type=str, default=None)\n",
        "parser.add_argument(\"--decoder_lr_scheduler_config\", default=None)\n",
        "parser.add_argument(\"--segm_decoder_lr_scheduler\", type=str, default=None)\n",
        "parser.add_argument(\"--segm_decoder_lr_scheduler_config\", default=None)\n",
        "parser.add_argument(\"--eval_before_training\", type=argparse_bool_type, default=False)\n",
        "parser.add_argument(\"--data_path_conll\", type=str,)\n",
        "parser.add_argument(\"--train_data_dir\", type=str, default=\"data\")\n",
        "parser.add_argument(\"--valid_data_dir\", type=str, default=\"data\")\n",
        "parser.add_argument(\"--test_data_dir\", type=str, default=\"data\")\n",
        "parser.add_argument(\"--exclude_parameter_names_regex\", type=str)\n",
        "print(parser)\n",
        "print('after add arguments to parser')\n",
        "print(argparse_bool_type)\n",
        "# parser.parse_args(['my', 'list', 'of', 'strings']) "
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: configargparse in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "ArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)\n",
            "ArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)\n",
            "after add arguments to parser\n",
            "<function argparse_bool_type at 0x7f1102477d90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzJMHhnGnDM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# from misc import argparse_bool_type\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_args():\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    for k, v in args.__dict__.items():\n",
        "        print(k, \":\", v)\n",
        "        if v == \"None\":\n",
        "            args.__dict__[k] = None\n",
        "\n",
        "    args.device = (\n",
        "        int(args.device) if args.device is not None and args.device != \"cpu\" and torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "    if args.eval_device is not None:\n",
        "        if args.eval_device != \"cpu\":\n",
        "            args.eval_device = int(args.eval_device)\n",
        "        else:\n",
        "            args.eval_device = \"cpu\"\n",
        "    else:\n",
        "        args.eval_device = args.device\n",
        "    if args.out_device is not None:\n",
        "        if args.out_device != \"cpu\":\n",
        "            args.out_device = int(args.out_device)\n",
        "        else:\n",
        "            args.out_device = \"cpu\"\n",
        "    else:\n",
        "        args.out_device = args.device\n",
        "\n",
        "    if args.encoder_lr_scheduler_config:\n",
        "        args.encoder_lr_scheduler_config = ast.literal_eval(args.encoder_lr_scheduler_config)\n",
        "    if args.decoder_lr_scheduler_config:\n",
        "        args.decoder_lr_scheduler_config = ast.literal_eval(args.decoder_lr_scheduler_config)\n",
        "    if args.segm_decoder_lr_scheduler_config:\n",
        "        args.segm_decoder_lr_scheduler_config = ast.literal_eval(args.segm_decoder_lr_scheduler_config)\n",
        "\n",
        "    args.eval_batch_size = args.eval_batch_size if args.eval_batch_size else args.batch_size\n",
        "\n",
        "    if not args.logdir:\n",
        "        raise Exception(\"set args.logdir\")\n",
        "\n",
        "    if not os.path.exists(args.logdir):\n",
        "        os.makedirs(args.logdir)\n",
        "\n",
        "    if not args.eval_on_test_only:\n",
        "        config_fname = os.path.join(args.logdir, \"config\")\n",
        "        with open(f\"{config_fname}.yaml\", \"w\") as f:\n",
        "            f.writelines(\n",
        "                [\n",
        "                    \"{}: {}\\n\".format(k, v)\n",
        "                    for k, v in args.__dict__.items()\n",
        "                    if isinstance(v, str) and len(v.strip()) > 0 or not isinstance(v, str) and v is not None\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    with open(f\"data/versions/{args.data_version_name}/config.yaml\") as f:\n",
        "        dataset = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
        "\n",
        "    for k, v in dataset.items():\n",
        "        if k != \"debug\":\n",
        "            args.__setattr__(k, v)\n",
        "\n",
        "    return args"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxgdXGNlNFjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "150d6b55-3535-45c8-f9cc-e9669f699c10"
      },
      "source": [
        "args = {'logdir': 'data/checkpoints/dummy_wiki_00001',\n",
        "'debug': False,\n",
        "'device': 0,\n",
        "'eval_device': 0,\n",
        "'dataset': EDLDataset,\n",
        "'model': Net,\n",
        "'data_version_name': 'dummy',\n",
        "'wiki_lang_version': 'enwiki',\n",
        "'eval_on_test_only': False,\n",
        "'out_device': 0,\n",
        "'batch_size': 16,\n",
        "'eval_batch_size': 1,\n",
        "'accumulate_batch_gradients': 8,\n",
        "'sparse': True,\n",
        "'encoder_lr': 5e-05,\n",
        "'decoder_lr': 0.1,\n",
        "'maskout_entity_prob': 0.0,\n",
        "'encoder_weight_decay': 0.0,\n",
        "'decoder_weight_decay': 0.0,\n",
        "'segm_decoder_weight_decay': 0.0,\n",
        "'learn_segmentation': False,\n",
        "'label_size': 8192,\n",
        "'entity_embedding_size': 768,\n",
        "'project': False,\n",
        "'n_epochs': 100,\n",
        "'collect_most_popular_labels_steps': 1,\n",
        "'checkpoint_eval_steps': 1000,\n",
        "'checkpoint_save_steps': 100000,\n",
        "'finetuning': 3,\n",
        "'train_loc_file': 'train.loc',\n",
        "'resume_reset_epoch': False,\n",
        "'resume_optimizer_from_checkpoint': False,\n",
        "'topk_neg_examples': 20,\n",
        "'dont_save_checkpoints': False,\n",
        "'data_workers': 24,\n",
        "'uncased': True,\n",
        "'eval_before_training': False,\n",
        "'train_data_dir': 'data',\n",
        "'valid_data_dir': 'data',\n",
        "'test_data_dir': 'data',\n",
        "'vocab_size': 0,\n",
        "'label_size': 0,\n",
        " 'top_rnns': False,\n",
        " 'project': False\n",
        "}\n",
        "print(args)"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'logdir': 'data/checkpoints/dummy_wiki_00001', 'debug': False, 'device': 0, 'eval_device': 0, 'dataset': <class '__main__.EDLDataset'>, 'model': <class '__main__.Net'>, 'data_version_name': 'dummy', 'wiki_lang_version': 'enwiki', 'eval_on_test_only': False, 'out_device': 0, 'batch_size': 16, 'eval_batch_size': 1, 'accumulate_batch_gradients': 8, 'sparse': True, 'encoder_lr': 5e-05, 'decoder_lr': 0.1, 'maskout_entity_prob': 0.0, 'encoder_weight_decay': 0.0, 'decoder_weight_decay': 0.0, 'segm_decoder_weight_decay': 0.0, 'learn_segmentation': False, 'label_size': 0, 'entity_embedding_size': 768, 'project': False, 'n_epochs': 100, 'collect_most_popular_labels_steps': 1, 'checkpoint_eval_steps': 1000, 'checkpoint_save_steps': 100000, 'finetuning': 3, 'train_loc_file': 'train.loc', 'resume_reset_epoch': False, 'resume_optimizer_from_checkpoint': False, 'topk_neg_examples': 20, 'dont_save_checkpoints': False, 'data_workers': 24, 'uncased': True, 'eval_before_training': False, 'train_data_dir': 'data', 'valid_data_dir': 'data', 'test_data_dir': 'data', 'vocab_size': 0, 'top_rnns': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvQ3gsdtnQ-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# if args.debug:\n",
        "#     logging.basicConfig(level=logging.DEBUG)\n",
        "# else:\n",
        "#     logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# logging.info(str((\"Devices\", args.device, args.eval_device, args.out_device)))\n",
        "# args = 'config/dummy__train_on_wiki.yaml'\n",
        "# set up the model\n",
        "vocab = Vocab(args)\n",
        "model_class = Net\n",
        "model = model_class(args=args, vocab_size=vocab.size())\n"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVNDiMYh7nWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16357a5f-4a6a-498e-fd78-0322a79b67a4"
      },
      "source": [
        "checkpoint = None\n",
        "# if args.resume_from_checkpoint is not None:\n",
        "resume_from_checkpoint = \"/content/drive/My Drive/data/checkpoints/dummy_wiki_00001\"\n",
        "# checkpoint = torch.load(args.resume_from_checkpoint, map_location=\"cpu\")\n",
        "# model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "if device != \"cpu\":\n",
        "    torch.cuda.empty_cache()\n",
        "    model.to(device, 0)\n",
        "print(model)"
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (out): Embedding(50002, 768, sparse=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CiBxglLM_XI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b74255d9-52b3-47f0-ed79-4db4197a987e"
      },
      "source": [
        "\n",
        "# model_class = getattr(Models, args.model)\n",
        "# model = model_class(args=args, vocab_size=vocab.size())\n",
        "# checkpoint = None\n",
        "# if args.resume_from_checkpoint is not None:\n",
        "#     checkpoint = torch.load(args.resume_from_checkpoint, map_location=\"cpu\")\n",
        "#     model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "# if args.device != \"cpu\":\n",
        "#     torch.cuda.empty_cache()\n",
        "#     model.to(args.device, args.out_device)\n",
        "# print(model)\n",
        "\n",
        "# set up the optimizers and the loss\n",
        "# optimizers, lr_schedulers = model.get_optimizers(args, checkpoint=checkpoint)\n",
        "\n",
        "\n",
        "# else:\n",
        "#     eval_dataset = getattr(Datasets, args.dataset)(args, split=\"test\", vocab=vocab, device=args.eval_device)\n",
        "#     eval_iter = eval_dataset.get_data_iter(args=args, batch_size=args.eval_batch_size, vocab=vocab, train=False)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = getattr(Datasets, 'EDLDataset')(\n",
        "    args, split=\"train\", vocab=vocab, device=0, label_size=8192\n",
        ")\n",
        "batch_size = 16\n",
        "eval_batch_size = 1\n",
        "train_iter = train_dataset.get_data_iter(args=args, batch_size=batch_size, vocab=vocab, train=True)\n",
        "eval_dataset = getattr(Datasets, 'EDLDataset')(args, split=\"valid\", vocab=vocab, device=0)\n",
        "eval_iter = eval_dataset.get_data_iter(args=args, batch_size=eval_batch_size, vocab=vocab, train=False)\n",
        "print('eval_iter')"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eval_iter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDUqF4VNta09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizers = []\n",
        "optimizer1 = optim.Adam(model.out.parameters(), lr=0.05)\n",
        "optimizers.append(optimizer1)\n",
        "lr_schedulers = []\n",
        "scheduler = ReduceLROnPlateau\n",
        "#LRMilestones(optimizer1,milestones= [(30, 0.1), (80, 0.2)])\n",
        "lr_schedulers.append(scheduler)\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ],
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GViYQ4Yb9x7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # set up the datasets and dataloaders\n",
        "# if not args.eval_on_test_only:\n",
        "# train_dataset = getattr(Datasets, args.dataset)(\n",
        "#     args, split=\"train\", vocab=vocab, device=args.device, label_size=args.label_size\n",
        "# )\n",
        "# train_iter = train_dataset.get_data_iter(args=args, batch_size=args.batch_size, vocab=vocab, train=True)\n",
        "# eval_dataset = getattr(Datasets, args.dataset)(args, split=\"valid\", vocab=vocab, device=args.eval_device)\n",
        "# eval_iter = eval_dataset.get_data_iter(args=args, batch_size=args.eval_batch_size, vocab=vocab, train=False)"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biDujsmDOWTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4228701-c5d3-4696-ad1f-ab54cb3d659c"
      },
      "source": [
        "start_epoch = 1\n",
        "if checkpoint and not args.resume_reset_epoch:\n",
        "    start_epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "metrics = Metrics()\n",
        "args_eval_on_test_only = False\n",
        "# if args.eval_before_training or args.eval_on_test_only:\n",
        "cloned_args = copy.deepcopy(args)\n",
        "# cloned_args.dont_save_checkpoints = True\n",
        "metrics = model_class.evaluate(\n",
        "    cloned_args,\n",
        "    model,\n",
        "    eval_iter,\n",
        "    optimizers=optimizers,\n",
        "    step=0,\n",
        "    epoch=0,\n",
        "    save_checkpoint=False,\n",
        "    save_csv=args_eval_on_test_only,\n",
        "    vocab=vocab,\n",
        "    metrics=metrics,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 1/1000 [00:02<35:20,  2.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 3/1000 [00:02<25:06,  1.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 5/1000 [00:02<17:56,  1.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 7/1000 [00:02<12:56,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 9/1000 [00:02<09:21,  1.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 11/1000 [00:02<06:55,  2.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|▏         | 13/1000 [00:03<05:22,  3.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 15/1000 [00:03<04:05,  4.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 17/1000 [00:03<03:08,  5.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 19/1000 [00:03<02:30,  6.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 21/1000 [00:03<02:04,  7.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 23/1000 [00:03<01:47,  9.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▎         | 25/1000 [00:04<02:13,  7.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 27/1000 [00:04<01:51,  8.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 29/1000 [00:04<01:38,  9.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 31/1000 [00:04<01:25, 11.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 33/1000 [00:04<01:19, 12.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▎         | 35/1000 [00:04<01:15, 12.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▎         | 37/1000 [00:04<01:11, 13.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 39/1000 [00:05<01:09, 13.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 41/1000 [00:05<01:07, 14.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 43/1000 [00:05<01:06, 14.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 45/1000 [00:05<01:06, 14.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 47/1000 [00:05<01:05, 14.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 49/1000 [00:05<01:06, 14.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 51/1000 [00:05<01:04, 14.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 53/1000 [00:06<01:04, 14.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 55/1000 [00:06<01:03, 14.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 57/1000 [00:06<01:05, 14.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 59/1000 [00:06<01:03, 14.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 61/1000 [00:06<01:03, 14.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▋         | 63/1000 [00:06<01:09, 13.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▋         | 65/1000 [00:06<01:05, 14.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 67/1000 [00:07<01:03, 14.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 69/1000 [00:07<01:03, 14.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 71/1000 [00:07<01:02, 14.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 73/1000 [00:07<01:06, 13.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 75/1000 [00:07<01:14, 12.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 77/1000 [00:07<01:09, 13.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 79/1000 [00:07<01:07, 13.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 81/1000 [00:08<01:05, 14.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 83/1000 [00:08<01:03, 14.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 85/1000 [00:08<01:03, 14.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▊         | 87/1000 [00:08<01:02, 14.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 89/1000 [00:08<01:02, 14.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 91/1000 [00:08<01:01, 14.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 93/1000 [00:08<01:00, 14.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 95/1000 [00:08<01:00, 14.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 97/1000 [00:09<01:00, 14.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 99/1000 [00:09<01:00, 14.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 101/1000 [00:09<01:00, 14.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 103/1000 [00:09<00:59, 15.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 105/1000 [00:09<01:00, 14.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 107/1000 [00:09<00:59, 15.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 109/1000 [00:09<00:59, 15.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 111/1000 [00:10<00:58, 15.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|█▏        | 113/1000 [00:10<00:57, 15.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 115/1000 [00:10<00:58, 15.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 117/1000 [00:10<01:00, 14.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 119/1000 [00:10<00:57, 15.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 121/1000 [00:10<00:59, 14.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 123/1000 [00:11<01:29,  9.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▎        | 125/1000 [00:11<01:18, 11.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 127/1000 [00:11<01:12, 12.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 129/1000 [00:11<01:07, 12.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 131/1000 [00:11<01:03, 13.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 133/1000 [00:11<01:02, 13.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▎        | 135/1000 [00:11<01:01, 14.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▎        | 137/1000 [00:11<01:00, 14.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 139/1000 [00:12<00:59, 14.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 141/1000 [00:12<00:58, 14.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 143/1000 [00:12<00:59, 14.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 145/1000 [00:12<00:57, 14.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▍        | 147/1000 [00:12<00:57, 14.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▍        | 149/1000 [00:12<00:57, 14.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▌        | 151/1000 [00:12<00:56, 14.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▌        | 153/1000 [00:13<00:56, 14.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 155/1000 [00:13<00:56, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 157/1000 [00:13<00:56, 15.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 159/1000 [00:13<00:56, 14.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 161/1000 [00:13<00:55, 15.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▋        | 163/1000 [00:13<00:52, 15.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▋        | 165/1000 [00:13<00:52, 15.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 167/1000 [00:13<00:53, 15.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 169/1000 [00:14<00:53, 15.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 171/1000 [00:14<00:54, 15.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 173/1000 [00:14<00:55, 15.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 175/1000 [00:14<00:54, 15.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 177/1000 [00:14<00:54, 15.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 179/1000 [00:14<00:58, 14.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 181/1000 [00:14<00:53, 15.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 183/1000 [00:15<00:50, 16.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 185/1000 [00:15<00:51, 15.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▊        | 187/1000 [00:15<00:52, 15.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 189/1000 [00:15<00:52, 15.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 191/1000 [00:15<00:53, 15.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 193/1000 [00:15<00:53, 15.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|█▉        | 195/1000 [00:15<00:53, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|█▉        | 197/1000 [00:15<00:53, 15.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|█▉        | 199/1000 [00:16<00:53, 15.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 201/1000 [00:16<00:53, 15.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 203/1000 [00:16<00:52, 15.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 205/1000 [00:16<00:52, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 21%|██        | 207/1000 [00:16<00:53, 14.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 21%|██        | 209/1000 [00:16<00:52, 14.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 21%|██        | 211/1000 [00:16<00:52, 15.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 21%|██▏       | 213/1000 [00:17<00:52, 15.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 215/1000 [00:17<00:52, 15.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 217/1000 [00:17<00:51, 15.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 219/1000 [00:17<00:48, 16.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 221/1000 [00:17<00:47, 16.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 223/1000 [00:17<00:49, 15.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▎       | 225/1000 [00:17<00:49, 15.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 227/1000 [00:17<00:49, 15.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 229/1000 [00:18<00:50, 15.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 231/1000 [00:18<00:50, 15.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 233/1000 [00:18<00:50, 15.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▎       | 235/1000 [00:18<00:51, 14.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▎       | 237/1000 [00:18<00:50, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 239/1000 [00:18<00:52, 14.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 241/1000 [00:18<00:49, 15.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 243/1000 [00:18<00:49, 15.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 245/1000 [00:19<00:49, 15.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▍       | 247/1000 [00:19<00:49, 15.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▍       | 249/1000 [00:19<00:49, 15.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▌       | 251/1000 [00:19<00:50, 14.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▌       | 253/1000 [00:19<00:49, 15.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 255/1000 [00:19<00:50, 14.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 257/1000 [00:19<00:49, 14.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 259/1000 [00:20<00:49, 14.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 261/1000 [00:20<00:49, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▋       | 263/1000 [00:20<00:49, 14.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▋       | 265/1000 [00:20<00:49, 14.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 267/1000 [00:20<00:49, 14.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 269/1000 [00:20<00:50, 14.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 271/1000 [00:20<00:48, 14.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 273/1000 [00:20<00:48, 14.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 275/1000 [00:21<00:48, 14.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 277/1000 [00:21<00:48, 15.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 279/1000 [00:21<00:47, 15.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 281/1000 [00:21<00:48, 14.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 283/1000 [00:21<00:47, 15.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 285/1000 [00:21<00:47, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▊       | 287/1000 [00:21<00:48, 14.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▉       | 289/1000 [00:22<00:47, 15.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▉       | 291/1000 [00:22<00:47, 15.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▉       | 293/1000 [00:22<00:47, 14.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|██▉       | 295/1000 [00:22<00:47, 14.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|██▉       | 297/1000 [00:22<00:47, 14.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|██▉       | 299/1000 [00:22<00:46, 14.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 301/1000 [00:22<00:46, 15.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 303/1000 [00:22<00:46, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 305/1000 [00:23<00:46, 14.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 31%|███       | 307/1000 [00:23<00:46, 15.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 31%|███       | 309/1000 [00:23<00:46, 14.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 31%|███       | 311/1000 [00:23<00:46, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 31%|███▏      | 313/1000 [00:23<00:46, 14.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 315/1000 [00:23<00:45, 14.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 317/1000 [00:23<00:46, 14.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 319/1000 [00:24<00:45, 14.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 321/1000 [00:24<00:42, 15.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 323/1000 [00:24<00:43, 15.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▎      | 325/1000 [00:24<00:43, 15.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 327/1000 [00:24<00:45, 14.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 329/1000 [00:24<00:44, 15.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 331/1000 [00:24<00:44, 15.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 333/1000 [00:24<00:43, 15.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▎      | 335/1000 [00:25<00:43, 15.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▎      | 337/1000 [00:25<00:43, 15.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 339/1000 [00:25<00:43, 15.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 341/1000 [00:25<00:43, 15.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 343/1000 [00:25<00:43, 15.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 345/1000 [00:25<00:43, 15.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▍      | 347/1000 [00:25<00:43, 14.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▍      | 349/1000 [00:26<00:43, 14.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▌      | 351/1000 [00:26<00:43, 15.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▌      | 353/1000 [00:26<00:42, 15.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▌      | 355/1000 [00:26<00:42, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▌      | 357/1000 [00:26<00:42, 15.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▌      | 359/1000 [00:26<00:42, 15.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▌      | 361/1000 [00:26<00:43, 14.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▋      | 363/1000 [00:26<00:42, 14.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▋      | 365/1000 [00:27<00:55, 11.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 37%|███▋      | 367/1000 [00:27<00:50, 12.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 37%|███▋      | 369/1000 [00:27<00:46, 13.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 37%|███▋      | 371/1000 [00:27<00:45, 13.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 37%|███▋      | 373/1000 [00:27<00:44, 14.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 375/1000 [00:27<00:43, 14.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 377/1000 [00:28<00:43, 14.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 379/1000 [00:28<00:42, 14.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 381/1000 [00:28<00:41, 14.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 383/1000 [00:28<00:41, 14.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 385/1000 [00:28<00:41, 14.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 39%|███▊      | 387/1000 [00:28<00:41, 14.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 39%|███▉      | 389/1000 [00:28<00:53, 11.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 39%|███▉      | 391/1000 [00:29<00:49, 12.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 39%|███▉      | 393/1000 [00:29<00:46, 13.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|███▉      | 395/1000 [00:29<00:44, 13.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|███▉      | 397/1000 [00:29<00:42, 14.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|███▉      | 399/1000 [00:29<00:41, 14.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 401/1000 [00:29<00:41, 14.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 403/1000 [00:29<00:41, 14.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 405/1000 [00:30<00:40, 14.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 41%|████      | 407/1000 [00:30<00:40, 14.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 41%|████      | 409/1000 [00:30<00:39, 14.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 41%|████      | 411/1000 [00:30<00:39, 14.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 41%|████▏     | 413/1000 [00:30<00:50, 11.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 415/1000 [00:30<00:46, 12.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 417/1000 [00:30<00:43, 13.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 419/1000 [00:31<00:42, 13.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 421/1000 [00:31<00:40, 14.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 423/1000 [00:31<00:45, 12.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▎     | 425/1000 [00:31<00:43, 13.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 43%|████▎     | 427/1000 [00:31<00:41, 13.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 43%|████▎     | 429/1000 [00:31<00:39, 14.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 43%|████▎     | 431/1000 [00:31<00:39, 14.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 43%|████▎     | 433/1000 [00:32<00:39, 14.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▎     | 435/1000 [00:32<00:38, 14.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▎     | 437/1000 [00:32<00:38, 14.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▍     | 439/1000 [00:32<00:38, 14.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▍     | 441/1000 [00:32<00:37, 14.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▍     | 443/1000 [00:32<00:37, 14.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▍     | 445/1000 [00:32<00:37, 14.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▍     | 447/1000 [00:33<00:44, 12.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▍     | 449/1000 [00:33<00:40, 13.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▌     | 451/1000 [00:33<00:39, 14.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▌     | 453/1000 [00:33<00:38, 14.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▌     | 455/1000 [00:33<00:37, 14.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▌     | 457/1000 [00:33<00:37, 14.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▌     | 459/1000 [00:33<00:36, 14.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▌     | 461/1000 [00:34<00:40, 13.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▋     | 463/1000 [00:34<00:38, 14.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▋     | 465/1000 [00:34<00:37, 14.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 47%|████▋     | 467/1000 [00:34<00:36, 14.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 47%|████▋     | 469/1000 [00:34<00:35, 14.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 47%|████▋     | 471/1000 [00:34<00:37, 14.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 47%|████▋     | 473/1000 [00:34<00:35, 14.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 475/1000 [00:34<00:35, 14.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 477/1000 [00:35<00:35, 14.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 479/1000 [00:35<00:34, 14.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 481/1000 [00:35<00:32, 15.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 483/1000 [00:35<00:33, 15.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 485/1000 [00:35<00:36, 14.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 49%|████▊     | 487/1000 [00:35<00:34, 14.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 49%|████▉     | 489/1000 [00:35<00:34, 14.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 49%|████▉     | 491/1000 [00:36<00:33, 15.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 49%|████▉     | 493/1000 [00:36<00:34, 14.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|████▉     | 495/1000 [00:36<00:37, 13.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|████▉     | 497/1000 [00:36<00:34, 14.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|████▉     | 499/1000 [00:36<00:34, 14.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 501/1000 [00:36<00:32, 15.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 503/1000 [00:36<00:34, 14.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 505/1000 [00:37<00:33, 14.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 51%|█████     | 507/1000 [00:37<00:33, 14.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 51%|█████     | 509/1000 [00:37<00:33, 14.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 51%|█████     | 511/1000 [00:37<00:32, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 51%|█████▏    | 513/1000 [00:37<00:32, 15.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 515/1000 [00:37<00:32, 14.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 517/1000 [00:37<00:32, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 519/1000 [00:37<00:34, 13.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 521/1000 [00:38<00:31, 14.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 523/1000 [00:38<00:31, 15.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▎    | 525/1000 [00:38<00:30, 15.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 53%|█████▎    | 527/1000 [00:38<00:33, 14.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 53%|█████▎    | 529/1000 [00:38<00:31, 15.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 53%|█████▎    | 531/1000 [00:38<00:30, 15.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 53%|█████▎    | 533/1000 [00:38<00:31, 14.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▎    | 535/1000 [00:39<00:30, 15.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▎    | 537/1000 [00:39<00:30, 15.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▍    | 539/1000 [00:39<00:30, 14.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▍    | 541/1000 [00:39<00:30, 15.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▍    | 543/1000 [00:39<00:45, 10.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▍    | 545/1000 [00:39<00:40, 11.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▍    | 547/1000 [00:40<00:36, 12.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▍    | 549/1000 [00:40<00:33, 13.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▌    | 551/1000 [00:40<00:34, 12.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▌    | 553/1000 [00:40<00:31, 13.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▌    | 555/1000 [00:40<00:31, 14.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▌    | 557/1000 [00:40<00:30, 14.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▌    | 559/1000 [00:40<00:30, 14.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▌    | 561/1000 [00:40<00:29, 14.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▋    | 563/1000 [00:41<00:29, 14.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▋    | 565/1000 [00:41<00:29, 14.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 57%|█████▋    | 567/1000 [00:41<00:33, 12.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 57%|█████▋    | 569/1000 [00:41<00:32, 13.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 57%|█████▋    | 571/1000 [00:41<00:30, 14.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 57%|█████▋    | 573/1000 [00:41<00:29, 14.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 57%|█████▊    | 575/1000 [00:41<00:31, 13.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 577/1000 [00:42<00:29, 14.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 579/1000 [00:42<00:28, 14.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 581/1000 [00:42<00:30, 13.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 583/1000 [00:42<00:28, 14.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 585/1000 [00:42<00:27, 14.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 59%|█████▊    | 587/1000 [00:42<00:27, 15.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 59%|█████▉    | 589/1000 [00:42<00:27, 15.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 59%|█████▉    | 591/1000 [00:43<00:31, 13.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 59%|█████▉    | 593/1000 [00:43<00:28, 14.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|█████▉    | 595/1000 [00:43<00:27, 14.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|█████▉    | 597/1000 [00:43<00:27, 14.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|█████▉    | 599/1000 [00:43<00:29, 13.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 601/1000 [00:43<00:27, 14.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 603/1000 [00:43<00:27, 14.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 605/1000 [00:44<00:27, 14.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 61%|██████    | 607/1000 [00:44<00:26, 14.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 61%|██████    | 609/1000 [00:44<00:26, 14.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 61%|██████    | 611/1000 [00:44<00:26, 14.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 61%|██████▏   | 613/1000 [00:44<00:26, 14.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 615/1000 [00:44<00:25, 14.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 617/1000 [00:44<00:25, 14.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 619/1000 [00:44<00:25, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 621/1000 [00:45<00:25, 15.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 623/1000 [00:45<00:27, 13.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▎   | 625/1000 [00:45<00:25, 14.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 63%|██████▎   | 627/1000 [00:45<00:25, 14.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 63%|██████▎   | 629/1000 [00:45<00:24, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 63%|██████▎   | 631/1000 [00:45<00:24, 15.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 63%|██████▎   | 633/1000 [00:45<00:24, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▎   | 635/1000 [00:46<00:24, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▎   | 637/1000 [00:46<00:24, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▍   | 639/1000 [00:46<00:24, 15.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▍   | 641/1000 [00:46<00:23, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▍   | 643/1000 [00:46<00:23, 15.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▍   | 645/1000 [00:46<00:23, 15.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 65%|██████▍   | 647/1000 [00:46<00:27, 12.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 65%|██████▍   | 649/1000 [00:47<00:26, 13.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 65%|██████▌   | 651/1000 [00:47<00:24, 14.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 65%|██████▌   | 653/1000 [00:47<00:22, 15.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 655/1000 [00:47<00:22, 15.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 657/1000 [00:47<00:22, 15.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 659/1000 [00:47<00:22, 15.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 661/1000 [00:47<00:22, 15.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▋   | 663/1000 [00:48<00:27, 12.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▋   | 665/1000 [00:48<00:25, 13.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 667/1000 [00:48<00:24, 13.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 669/1000 [00:48<00:21, 15.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 671/1000 [00:48<00:22, 14.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 673/1000 [00:48<00:21, 15.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 675/1000 [00:48<00:21, 15.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 677/1000 [00:48<00:21, 15.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 679/1000 [00:49<00:21, 15.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 681/1000 [00:49<00:21, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 683/1000 [00:49<00:21, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 685/1000 [00:49<00:20, 15.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 69%|██████▊   | 687/1000 [00:49<00:25, 12.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 69%|██████▉   | 689/1000 [00:49<00:23, 13.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 69%|██████▉   | 691/1000 [00:49<00:22, 13.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 69%|██████▉   | 693/1000 [00:50<00:21, 14.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|██████▉   | 695/1000 [00:50<00:21, 14.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|██████▉   | 697/1000 [00:50<00:20, 14.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|██████▉   | 699/1000 [00:50<00:20, 14.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 701/1000 [00:50<00:20, 14.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 703/1000 [00:50<00:20, 14.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 705/1000 [00:50<00:19, 15.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 71%|███████   | 707/1000 [00:51<00:19, 15.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 71%|███████   | 709/1000 [00:51<00:19, 14.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 71%|███████   | 711/1000 [00:51<00:25, 11.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 71%|███████▏  | 713/1000 [00:51<00:23, 12.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 715/1000 [00:51<00:21, 13.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 717/1000 [00:51<00:20, 13.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 719/1000 [00:51<00:19, 14.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 721/1000 [00:52<00:19, 14.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 723/1000 [00:52<00:19, 14.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▎  | 725/1000 [00:52<00:19, 14.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 73%|███████▎  | 727/1000 [00:52<00:19, 14.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 73%|███████▎  | 729/1000 [00:52<00:18, 14.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 73%|███████▎  | 731/1000 [00:52<00:18, 14.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 73%|███████▎  | 733/1000 [00:52<00:17, 14.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 74%|███████▎  | 735/1000 [00:53<00:22, 11.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfs_PzmSORxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if not args.eval_on_test_only\n",
        "args_n_epochs = 100\n",
        "args_finetuning = False\n",
        "for epoch in range(start_epoch, args_n_epochs + 1):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    model.finetuning = epoch >= args_finetuning if args_finetuning >= 0 else False\n",
        "\n",
        "    metrics = model_class.train_one_epoch(\n",
        "        args=args,\n",
        "        model=model,\n",
        "        train_iter=train_iter,\n",
        "        optimizers=optimizers,\n",
        "        criterion=criterion,\n",
        "        vocab=vocab,\n",
        "        eval_iter=eval_iter,\n",
        "        epoch=epoch,\n",
        "        metrics=metrics,\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Evaluate in epoch {epoch}\")\n",
        "    metrics = model_class.evaluate(\n",
        "        args, model, eval_iter, optimizers=optimizers, step=0, epoch=epoch, vocab=vocab, metrics=metrics,\n",
        "    )\n",
        "\n",
        "    logging.info(f\"{time.time() - start} per epoch\")\n",
        "\n",
        "    if lr_schedulers:\n",
        "        for lr_scheduler in lr_schedulers:\n",
        "                lr_scheduler.step(metrics.get_model_selection_metric())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEGSQdeUkTP8",
        "colab_type": "text"
      },
      "source": [
        "**Task 4:** write a training loop, which takes a model (instance of NERNet) and number of epochs to train on. The loss is always CrossEntropyLoss and the optimizer is always Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avkHfjT3k0HM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def train_loop(model, n_epochs):\n",
        "#   # Loss function\n",
        "#   criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#   # Optimizer (ADAM is a fancy version of SGD)\n",
        "#   optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "  \n",
        "#   for e in range(1, n_epochs + 1):\n",
        "#     # TODO - your code goes here...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baN1c_B7lTjb",
        "colab_type": "text"
      },
      "source": [
        "**Task 5:** write an evaluation loop on a trained model, using the dev and test datasets. This function print the true positive rate (TPR), also known as Recall and the opposite to false positive rate (FPR), also known as precision, of each label seperately (7 labels in total), and for all the 6 labels (except O) together. The caption argument for the function should be served for printing, so that when you print include it as a prefix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyQAjGaqmd8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def evaluate(model, caption):\n",
        "#   # TODO - your code goes here\n",
        "#   print(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQSXqWNOmqG4",
        "colab_type": "text"
      },
      "source": [
        "**Task 6:** Train and evaluate a few models, all with embedding_size=300, and with the following hyper parameters (you may use that as captions for the models as well):\n",
        "\n",
        "Model 1: (hidden_size: 500, n_layers: 1, directions: 1)\n",
        "\n",
        "Model 2: (hidden_size: 500, n_layers: 2, directions: 1)\n",
        "\n",
        "Model 3: (hidden_size: 500, n_layers: 3, directions: 1)\n",
        "\n",
        "Model 4: (hidden_size: 500, n_layers: 1, directions: 2)\n",
        "\n",
        "Model 5: (hidden_size: 500, n_layers: 2, directions: 2)\n",
        "\n",
        "Model 6: (hidden_size: 500, n_layers: 3, directions: 2)\n",
        "\n",
        "Model 4: (hidden_size: 800, n_layers: 1, directions: 2)\n",
        "\n",
        "Model 5: (hidden_size: 800, n_layers: 2, directions: 2)\n",
        "\n",
        "Model 6: (hidden_size: 800, n_layers: 3, directions: 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTNmBU6hycZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - your code goes here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM74r0_8nk5s",
        "colab_type": "text"
      },
      "source": [
        "**Task 6:** Download the GloVe embeddings from https://nlp.stanford.edu/projects/glove/ (use the 300-dim vectors from glove.6B.zip). Then intialize the nn.Embedding module in your NERNet with these embeddings, so that you can start your training with pre-trained vectors. Repeat Task 6 and print the results for each model.\n",
        "\n",
        "Note: make sure that vectors are aligned with the IDs in your Vocab, in other words, make sure that for example the word with ID 0 is the first vector in the GloVe matrix of vectors that you initialize nn.Embedding with. For a dicussion on how to do that, check it this link:\n",
        "https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRiMbvx9o5Rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - your code goes here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxaESRoco6bV",
        "colab_type": "text"
      },
      "source": [
        "**Good luck!**"
      ]
    }
  ]
}