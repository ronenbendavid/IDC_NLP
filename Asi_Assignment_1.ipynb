{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asi Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronenbendavid/IDC_NLP/blob/master/Asi_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM",
        "colab_type": "text"
      },
      "source": [
        "*As a preparation for this task, place the data files somewhere in your drive so that you can access the files from this notebook. The files are available to download from the Moodle assignment activity*\n",
        "\n",
        "The relevant files are:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z92NTjzV5zFc",
        "colab_type": "code",
        "outputId": "b14f1ffa-012c-467e-d609-9f84689be9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfzsITW8Yaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.chdir('/content/drive/My Drive/idc/nlp/ex1/')\n",
        "\n",
        "def preprocess():\n",
        "  vocabulary = set()\n",
        "  pathlist = glob('*.csv')\n",
        "  for data_file_path in pathlist:\n",
        "      data_file = pd.read_csv(data_file_path)\n",
        "      for data in data_file['tweet_text'].values:\n",
        "          vocabulary.update(list(data))\n",
        "\n",
        "  vocabulary.add('<s>')\n",
        "  vocabulary.add('</s>')\n",
        "\n",
        "  # vocabulary.discard(' ')\n",
        "  # vocabulary.discard('\\t')\n",
        "  # vocabulary.discard('\\r')\n",
        "  # vocabulary.discard('\\n')\n",
        "\n",
        "  return sorted(list(vocabulary))\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q0-OHfeo2og",
        "colab_type": "code",
        "outputId": "88bba8aa-4463-4906-f4d8-9f6c545f710d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "%%time\n",
        "vocabulary = preprocess()\n",
        "print(len(vocabulary))\n",
        "print(vocabulary)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1861\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '</s>', '<s>', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x97', '\\x9d', '¡', '£', '¤', '¥', '§', '¨', '©', 'ª', '«', '\\xad', '®', '¯', '°', '²', '³', '´', '¶', '·', '¸', 'º', '»', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Å', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ù', 'Ú', 'Ü', 'à', 'á', 'â', 'ã', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ė', 'Ğ', 'ğ', 'İ', 'ı', 'ń', 'ō', 'Œ', 'œ', 'Ş', 'ş', 'Š', 'Ÿ', 'ƒ', 'ʔ', 'ʕ', 'ʖ', 'ʰ', 'ʳ', 'ʷ', 'ʸ', 'ˍ', '˖', '˘', '˚', '˛', 'ˡ', 'ˢ', '̀', '́', '̃', '̈', '̥', '̮', '̯', '͜', '͡', 'Δ', 'Θ', 'Ω', 'υ', 'ω', 'А', 'И', 'М', 'Н', 'О', 'П', 'Р', 'Ф', 'Ц', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'з', 'и', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'х', 'ы', 'э', 'ю', 'я', 'Ғ', 'ү', '،', 'آ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ض', 'ط', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ی', '۶', 'ं', 'क', 'ग', 'प', 'ब', 'र', 'स', 'ा', 'े', '्', 'ೃ', '෴', 'ก', 'ข', 'ง', 'จ', 'ญ', 'ด', 'ต', 'ถ', 'ท', 'น', 'บ', 'ป', 'พ', 'ภ', 'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ส', 'อ', 'ะ', 'ั', 'า', 'ิ', 'ี', 'ุ', 'ู', 'เ', 'แ', '่', '้', '๐', '๑', 'ຶ', '༎', '༺', '༻', '༼', '༽', 'ღ', 'ᙓ', 'ᴗ', 'ᴬ', 'ᴰ', 'ᵃ', 'ᵇ', 'ᵈ', 'ᵉ', 'ᵍ', 'ᵐ', 'ᵒ', 'ᵖ', 'ᵗ', 'ᵘ', 'ᵛ', 'ᶜ', 'ᶠ', 'ᶦ', 'ᶰ', '\\u2009', '\\u200a', '\\u200b', '\\u200d', '–', '—', '―', '‘', '’', '“', '”', '„', '†', '•', '․', '…', '‰', '′', '‹', '›', '※', '‼', '‿', '⁉', '\\u2066', '\\u2067', '\\u2069', 'ⁱ', '⁷', 'ⁿ', '€', '₹', '⃣', '℃', '℅', '™', 'Ⅰ', 'Ⅱ', 'Ⅲ', 'Ⅳ', 'Ⅴ', '←', '↑', '→', '↓', '↔', '↕', '↗', '↘', '↚', '↛', '↩', '↪', '↯', '↺', '⇘', '⇨', '∀', '∆', '∇', '√', '∞', '∴', '∵', '≤', '≥', '≦', '≧', '⊙', '⋅', '⋪', '⋭', '⌚', '⌛', '⌣', '⎋', '⏩', '⏰', '⏱', '⏳', '⏸', '①', '⑥', '⒈', '⒉', '⒊', '⒋', '⒌', '⒍', '⒎', '⒏', '⒐', '⒑', 'Ⓜ', 'ⓘ', 'ⓙ', 'ⓢ', 'ⓦ', '─', '━', '┃', '┄', '┆', '┌', '┏', '┐', '┓', '└', '┗', '┘', '┛', '┳', '┻', '║', '╔', '╗', '╚', '╝', '╦', '╩', '╬', '╭', '╮', '╯', '╰', '╱', '╲', '╴', '█', '▊', '▏', '▒', '▔', '▕', '▙', '▝', '▣', '▦', '▪', '▲', '△', '▶', '▸', '►', '▼', '▽', '▿', '◀', '◁', '◄', '◆', '◇', '◈', '○', '◎', '●', '◑', '◕', '◡', '◻', '◼', '◽', '◾', '☀', '☁', '☃', '☄', '★', '☆', '☉', '☎', '☑', '☓', '☔', '☕', '☘', '☙', '☚', '☛', '☜', '☝', '☞', '☠', '☣', '☪', '☮', '☯', '☰', '☹', '☺', '☼', '☽', '☾', '♀', '♂', '♊', '♋', '♍', '♎', '♏', '♐', '♓', '♛', '♡', '♣', '♤', '♥', '♦', '♩', '♪', '♫', '♬', '♯', '♻', '⚒', '⚓', '⚔', '⚕', '⚖', '⚘', '⚜', '⚝', '⚠', '⚡', '⚪', '⚫', '⚰', '⚽', '⚾', '⛄', '⛅', '⛈', '⛓', '⛔', '⛩', '⛪', '⛳', '⛷', '⛽', '✁', '✂', '✃', '✅', '✈', '✉', '✊', '✋', '✌', '✍', '✏', '✓', '✔', '✖', '✝', '✡', '✧', '✨', '✩', '✪', '✭', '✰', '✳', '✴', '✵', '✶', '✷', '✿', '❀', '❁', '❄', '❅', '❈', '❋', '❌', '❎', '❓', '❔', '❗', '❝', '❞', '❣', '❤', '❥', '➊', '➋', '➌', '➍', '➎', '➏', '➔', '➖', '➗', '➙', '➛', '➜', '➞', '➟', '➠', '➡', '➢', '➤', '➰', '➵', '⠀', '⤵', '⦑', '⦒', '⬅', '⬇', '⭐', '⸄', '⸅', '\\u3000', '、', '。', '〆', '《', '》', '「', '」', '『', '』', '【', '】', '〜', '〡', '〰', 'い', 'う', 'え', 'お', 'か', 'が', 'き', 'く', 'し', 'せ', 'ぜ', 'た', 'っ', 'づ', 'て', 'で', 'と', 'な', 'に', 'ね', 'の', 'は', 'ひ', 'み', 'む', 'ょ', 'ら', 'り', 'る', 'れ', 'わ', 'を', '゜', 'イ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'キ', 'ク', 'グ', 'コ', 'ゴ', 'サ', 'ジ', 'ス', 'セ', 'タ', 'チ', 'ッ', 'ツ', 'テ', 'ト', 'ド', 'ナ', 'ニ', 'ネ', 'ノ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ポ', 'ム', 'メ', 'ュ', 'ユ', 'ョ', 'ラ', 'レ', 'ロ', 'ヮ', 'ワ', 'ン', '・', 'ー', 'ヽ', 'ㅅ', 'ㅈ', 'ㅋ', 'ㅏ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ', 'ㅤ', '\\u31ef', '世', '中', '主', '互', '人', '付', '会', '像', '儿', '允', '先', '入', '写', '分', '利', '制', '刹', '力', '努', '動', '午', '卒', '南', '合', '呟', '嘉', '増', '好', '姿', '嫌', '学', '尔', '希', '彡', '影', '彼', '後', '悪', '手', '投', '拶', '挨', '撃', '撮', '文', '映', '時', '曲', '月', '有', '服', '本', '林', '柱', '業', '機', '歌', '歳', '毅', '気', '洲', '洸', '王', '生', '用', '画', '界', '相', '真', '瞬', '知', '社', '稿', '空', '糟', '終', '結', '繋', '者', '花', '菜', '行', '许', '赫', '踊', '込', '通', '那', '間', '限', '風', '魏', 'ꠎ', '가', '간', '갓', '강', '걸', '검', '게', '격', '결', '경', '고', '곡', '과', '구', '국', '규', '그', '근', '금', '기', '김', '꺽', '꼼', '나', '날', '남', '내', '너', '널', '네', '넷', '녀', '년', '노', '논', '누', '는', '늘', '니', '다', '단', '당', '닿', '대', '더', '도', '동', '두', '둑', '듀', '드', '등', '디', '라', '락', '랑', '랙', '랜', '램', '러', '런', '레', '렛', '로', '롱', '료', '루', '룰', '룸', '를', '름', '릉', '리', '림', '링', '마', '맞', '매', '맨', '몬', '무', '미', '민', '밀', '바', '박', '방', '배', '백', '뱀', '버', '벅', '법', '베', '벨', '벳', '보', '복', '본', '봄', '봉', '뷔', '브', '븐', '블', '비', '빅', '빼', '사', '살', '삼', '상', '생', '샤', '샵', '서', '석', '선', '성', '세', '섹', '셔', '션', '셩', '소', '송', '수', '슈', '스', '슨', '슬', '승', '시', '식', '신', '실', '싸', '아', '안', '압', '애', '야', '양', '어', '에', '엑', '엘', '엠', '엣', '여', '역', '연', '영', '예', '오', '온', '와', '왕', '외', '요', '용', '우', '울', '워', '원', '위', '유', '윤', '의', '이', '인', '일', '임', '잘', '장', '재', '잭', '전', '정', '제', '젤', '종', '주', '쥔', '즈', '지', '직', '진', '집', '쩜', '찌', '찰', '채', '천', '철', '초', '최', '추', '출', '츠', '치', '카', '커', '코', '콘', '콤', '쿱', '크', '키', '킹', '타', '탄', '탑', '태', '터', '텐', '토', '톡', '트', '티', '틴', '팁', '파', '패', '펀', '포', '풀', '프', '플', '피', '핑', '하', '한', '해', '핸', '헌', '헤', '헨', '혁', '현', '형', '호', '화', '환', '훈', '힐', 'ﷻ', '︎', '️', '︵', '﹏', '﹪', '！', '＂', '＃', '（', '）', '＊', '．', '３', '６', '７', '８', '？', '＠', 'Ａ', 'Ｂ', 'Ｃ', 'Ｄ', 'Ｅ', 'Ｆ', 'Ｇ', 'Ｈ', 'Ｉ', 'Ｋ', 'Ｌ', 'Ｍ', 'Ｎ', 'Ｏ', 'Ｐ', 'Ｑ', 'Ｒ', 'Ｓ', 'Ｔ', 'Ｕ', 'Ｖ', 'Ｗ', 'Ｙ', '［', '］', '＿', '｀', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｇ', 'ｉ', 'ｌ', 'ｍ', 'ｎ', 'ｏ', 'ｐ', 'ｑ', 'ｒ', 'ｓ', 'ｔ', 'ｕ', 'ｖ', 'ｗ', 'ｘ', 'ｙ', 'ｚ', '｜', '｡', '･', 'ﾉ', 'ﾒ', '￣', '￼', '�', '🃏', '🅰', '🅱', '🅾', '🅿', '🆑', '🆒', '🆓', '🆔', '🆕', '🆖', '🆗', '🆘', '🆙', '🆚', '🇦', '🇧', '🇨', '🇩', '🇪', '🇫', '🇬', '🇭', '🇮', '🇯', '🇰', '🇱', '🇲', '🇳', '🇴', '🇵', '🇷', '🇸', '🇹', '🇺', '🇻', '🇼', '🇽', '🇾', '🇿', '🈴', '🈵', '🈶', '🈷', '🌀', '🌃', '🌄', '🌅', '🌆', '🌇', '🌈', '🌊', '🌋', '🌌', '🌍', '🌎', '🌏', '🌐', '🌒', '🌓', '🌗', '🌙', '🌚', '🌛', '🌜', '🌝', '🌞', '🌟', '🌠', '🌤', '🌥', '🌧', '🌨', '🌪', '🌫', '🌬', '🌭', '🌮', '🌯', '🌰', '🌱', '🌲', '🌳', '🌴', '🌵', '🌶', '🌷', '🌸', '🌹', '🌺', '🌻', '🌼', '🌽', '🌾', '🌿', '🍀', '🍁', '🍂', '🍃', '🍅', '🍆', '🍇', '🍉', '🍊', '🍋', '🍌', '🍍', '🍎', '🍏', '🍑', '🍒', '🍓', '🍔', '🍕', '🍖', '🍗', '🍚', '🍛', '🍜', '🍝', '🍞', '🍟', '🍣', '🍤', '🍥', '🍦', '🍨', '🍩', '🍪', '🍫', '🍬', '🍭', '🍯', '🍰', '🍱', '🍳', '🍴', '🍵', '🍶', '🍷', '🍸', '🍹', '🍺', '🍻', '🍼', '🍽', '🍾', '🍿', '🎀', '🎁', '🎂', '🎅', '🎆', '🎇', '🎈', '🎉', '🎊', '🎋', '🎍', '🎒', '🎓', '🎗', '🎙', '🎞', '🎟', '🎠', '🎡', '🎢', '🎤', '🎥', '🎦', '🎧', '🎨', '🎩', '🎪', '🎫', '🎬', '🎭', '🎮', '🎯', '🎰', '🎱', '🎲', '🎵', '🎶', '🎷', '🎸', '🎹', '🎺', '🎻', '🎼', '🎾', '🎿', '🏀', '🏁', '🏃', '🏄', '🏅', '🏆', '🏇', '🏈', '🏉', '🏊', '🏋', '🏌', '🏒', '🏓', '🏔', '🏖', '🏘', '🏙', '🏚', '🏟', '🏠', '🏡', '🏢', '🏩', '🏫', '🏰', '🏳', '🏴', '🏹', '🏻', '🏼', '🏽', '🏾', '🏿', '🐀', '🐁', '🐂', '🐄', '🐆', '🐇', '🐈', '🐉', '🐊', '🐍', '🐎', '🐐', '🐑', '🐒', '🐓', '🐔', '🐕', '🐖', '🐘', '🐙', '🐚', '🐜', '🐝', '🐞', '🐟', '🐠', '🐡', '🐢', '🐣', '🐥', '🐦', '🐧', '🐨', '🐩', '🐫', '🐬', '🐭', '🐮', '🐯', '🐰', '🐱', '🐲', '🐳', '🐴', '🐶', '🐷', '🐸', '🐹', '🐺', '🐻', '🐼', '🐽', '🐾', '🐿', '👀', '👁', '👂', '👄', '👅', '👆', '👇', '👈', '👉', '👊', '👋', '👌', '👍', '👎', '👏', '👐', '👑', '👓', '👕', '👖', '👗', '👙', '👞', '👟', '👠', '👡', '👣', '👤', '👥', '👦', '👧', '👨', '👩', '👪', '👫', '👭', '👮', '👯', '👰', '👱', '👲', '👳', '👵', '👶', '👷', '👸', '👹', '👺', '👻', '👼', '👽', '👿', '💀', '💁', '💂', '💃', '💄', '💅', '💆', '💈', '💉', '💊', '💋', '💌', '💍', '💎', '💏', '💐', '💑', '💒', '💓', '💔', '💕', '💖', '💗', '💘', '💙', '💚', '💛', '💜', '💝', '💞', '💟', '💡', '💢', '💣', '💤', '💥', '💦', '💧', '💨', '💩', '💪', '💫', '💬', '💭', '💮', '💯', '💰', '💲', '💳', '💵', '💶', '💸', '💻', '💼', '💽', '💿', '📀', '📂', '📅', '📆', '📈', '📊', '📋', '📌', '📍', '📏', '📓', '📖', '📚', '📛', '📝', '📞', '📡', '📢', '📣', '📦', '📧', '📩', '📬', '📯', '📰', '📱', '📲', '📴', '📷', '📸', '📹', '📺', '📻', '📼', '📽', '📿', '🔁', '🔂', '🔃', '🔄', '🔅', '🔉', '🔊', '🔋', '🔌', '🔐', '🔑', '🔒', '🔓', '🔔', '🔘', '🔙', '🔛', '🔜', '🔝', '🔞', '🔥', '🔨', '🔩', '🔪', '🔫', '🔮', '🔰', '🔱', '🔲', '🔴', '🔵', '🔶', '🔸', '🔹', '🔺', '🔻', '🔼', '🔽', '🕊', '🕋', '🕌', '🕎', '🕐', '🕒', '🕘', '🕛', '🕜', '🕟', '🕤', '🕪', '🕯', '🕵', '🕶', '🕷', '🕺', '🖐', '🖒', '🖕', '🖖', '🖤', '🖥', '🖲', '🖼', '🗂', '🗓', '🗝', '🗞', '🗡', '🗣', '🗨', '🗳', '🗻', '🗼', '🗽', '🗾', '😀', '😁', '😂', '😃', '😄', '😅', '😆', '😇', '😈', '😉', '😊', '😋', '😌', '😍', '😎', '😏', '😐', '😑', '😒', '😓', '😔', '😕', '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😦', '😧', '😨', '😩', '😪', '😫', '😬', '😭', '😮', '😯', '😰', '😱', '😲', '😳', '😴', '😵', '😶', '😷', '😸', '😹', '😺', '😻', '😼', '😽', '😿', '🙀', '🙁', '🙂', '🙃', '🙄', '🙅', '🙆', '🙇', '🙈', '🙉', '🙊', '🙋', '🙌', '🙍', '🙎', '🙏', '🚀', '🚁', '🚇', '🚈', '🚌', '🚑', '🚓', '🚔', '🚖', '🚗', '🚘', '🚙', '🚢', '🚣', '🚦', '🚧', '🚨', '🚩', '🚫', '🚬', '🚮', '🚲', '🚴', '🚵', '🚶', '🚻', '🚼', '🚿', '🛀', '🛁', '🛃', '🛄', '🛐', '🛩', '🛫', '🛬', '🛰', '🛳', '🛴', '🤐', '🤑', '🤒', '🤓', '🤔', '🤕', '🤖', '🤗', '🤘', '🤙', '🤚', '🤛', '🤜', '🤝', '🤞', '🤠', '🤡', '🤢', '🤣', '🤤', '🤥', '🤦', '🤧', '🤳', '🤴', '🤷', '🥀', '🥁', '🥂', '🥃', '🥄', '🥅', '🥇', '🥊', '🥐', '🥒', '🥓', '🥔', '🥘', '🥙', '🥞', '🦀', '🦁', '🦃', '🦄', '🦅', '🦇', '🦉', '🦋', '🦑', '\\U000fe4e6']\n",
            "CPU times: user 274 ms, sys: 17.8 ms, total: 291 ms\n",
            "Wall time: 310 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMC_u8eQbVvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  voc_size = len(vocabulary)\n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "  data_file = pd.read_csv(data_file_path)\n",
        "  for data in data_file['tweet_text'].values:\n",
        "      data = [\"<s>\"] + list(data) + [\"</s>\"]\n",
        "      for i in range(len(data) - n + 1):\n",
        "          word, char = ''.join(data[i:i + n - 1]), data[i + n - 1]\n",
        "          model[word][char] += 1\n",
        "\n",
        "  if add_one:\n",
        "      pmodel = defaultdict(lambda: defaultdict(lambda: 1/voc_size))\n",
        "  else:\n",
        "      pmodel = defaultdict(lambda: defaultdict(lambda: 1e-08))\n",
        "  for word, counts in model.items():\n",
        "      if add_one:\n",
        "          total_count = sum(counts.values()) + voc_size\n",
        "          pmodel[word] = defaultdict(lambda: 1/total_count)\n",
        "          for char in counts:\n",
        "              pmodel[word][char] = (counts[char] + 1) / total_count\n",
        "      else:\n",
        "          total_count = sum(counts.values())\n",
        "          for char in counts:\n",
        "              pmodel[word][char] = counts[char] / total_count\n",
        "\n",
        "  return pmodel\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU5gq5sCbUaR",
        "colab_type": "code",
        "outputId": "90875ec8-e268-4982-eccd-28e6e32556f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "%%time\n",
        "vocabulary = preprocess()\n",
        "model = lm(3, vocabulary, 'en.csv', False)\n",
        "print(model['Ab'])\n",
        "print(sum(v for v in model['Ab'].values()))\n",
        "print(sum(model['Ab'][v] for v in vocabulary))\n",
        "\n",
        "model = lm(3, vocabulary, 'en.csv', True)\n",
        "print(model['Ab'])\n",
        "print(sum(v for v in model['Ab'].values()))\n",
        "print(sum(model['Ab'][v] for v in vocabulary))\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function lm.<locals>.<lambda>.<locals>.<lambda> at 0x7fe40841ce18>, {'o': 0.2571428571428571, 's': 0.1, 'e': 0.05714285714285714, 'U': 0.04285714285714286, 'u': 0.08571428571428572, 'D': 0.014285714285714285, 'd': 0.1, 'a': 0.014285714285714285, 'b': 0.1, '7': 0.014285714285714285, 'y': 0.014285714285714285, 'l': 0.02857142857142857, 'j': 0.014285714285714285, 'q': 0.014285714285714285, '2': 0.014285714285714285, 'i': 0.02857142857142857, '</s>': 0.014285714285714285, 'r': 0.05714285714285714, 'n': 0.014285714285714285, 'Z': 0.014285714285714285})\n",
            "0.9999999999999994\n",
            "1.0000184099998928\n",
            "defaultdict(<function lm.<locals>.<lambda>.<locals>.<lambda> at 0x7fe4088c8488>, {'o': 18, 's': 7, 'e': 4, 'U': 3, 'u': 6, 'D': 1, 'd': 7, 'a': 1, 'b': 7, '7': 1, 'y': 1, 'l': 2, 'j': 1, 'q': 1, '2': 1, 'i': 2, '</s>': 1, 'r': 4, 'n': 1, 'Z': 1})\n",
            "defaultdict(<function lm.<locals>.<lambda> at 0x7fe407809e18>, {'<s>': 0.0005178663904712584, 'o': 0.00983946141895391, 's': 0.004142931123770067, 'e': 0.002589331952356292, 'U': 0.0020714655618850335, 'u': 0.0036250647332988087, 'D': 0.0010357327809425167, 'd': 0.004142931123770067, 'a': 0.0010357327809425167, 'b': 0.004142931123770067, '7': 0.0010357327809425167, 'y': 0.0010357327809425167, 'l': 0.0015535991714137752, 'j': 0.0010357327809425167, 'q': 0.0010357327809425167, '2': 0.0010357327809425167, 'i': 0.0015535991714137752, '</s>': 0.0010357327809425167, 'r': 0.002589331952356292, 'n': 0.0010357327809425167, 'Z': 0.0010357327809425167})\n",
            "0.04712584153288451\n",
            "1.0353105891161487\n",
            "CPU times: user 1.56 s, sys: 11 ms, total: 1.57 s\n",
            "Wall time: 1.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3",
        "colab_type": "text"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kkMn328-lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def eval(n, model, data_file_path):\n",
        "  # n - the n-gram that you used to build your model (must be the same number)\n",
        "  # model - the dictionary (model) to use for calculating perplexity\n",
        "  # data_file - the tweets file that you wish to claculate a perplexity score for\n",
        "\n",
        "  data_file = pd.read_csv(data_file_path)\n",
        "  count = 0\n",
        "  total = 0.0\n",
        "  for data in data_file['tweet_text'].values:\n",
        "      data = ['<s>'] + list(data) + ['</s>']\n",
        "      for i in range(len(data) - n + 1):\n",
        "          count += 1\n",
        "          word, char = ''.join(data[i:i + n - 1]), data[i + n - 1]\n",
        "          total += math.log2(model[word][char])\n",
        "\n",
        "  ent = -1 / count * total\n",
        "  per = 2 ** ent\n",
        "\n",
        "  return per"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRJFGceKAZ7j",
        "colab_type": "code",
        "outputId": "96175be3-7b7a-4d14-935d-6254510b2fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "model = lm(3, vocabulary, 'en.csv', False)\n",
        "print(eval(3, model, 'en.csv'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.965438001439026\n",
            "CPU times: user 1.21 s, sys: 7.97 ms, total: 1.22 s\n",
            "Wall time: 1.23 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCK_py9HYV6-",
        "colab_type": "code",
        "outputId": "3cf1a21c-cf1f-4fbe-fac9-daf5cb35b14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "model = lm(3, vocabulary, 'en.csv', True)\n",
        "print(eval(3, model, 'en.csv'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26.318871163182674\n",
            "CPU times: user 1.14 s, sys: 2.99 ms, total: 1.15 s\n",
            "Wall time: 1.15 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p",
        "colab_type": "text"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAxLE9s_fvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match(n, add_one):\n",
        "  # n - the n-gram to use for creating n-gram models\n",
        "  # add_one - use add_one smoothing or not\n",
        "\n",
        "  file_path = '{}.csv'\n",
        "\n",
        "  lang = ['en' ,'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "  df = pd.DataFrame(columns=lang, index=lang)\n",
        "  for l1 in lang:\n",
        "    l1_model = lm(n, vocabulary, file_path.format(l1), add_one)\n",
        "    for l2 in lang:\n",
        "      df.at[l1, l2] = eval(n, l1_model, file_path.format(l2))\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdQ-_2dMYbvQ",
        "colab_type": "code",
        "outputId": "696a727b-4475-45fa-a22d-d3876bd702bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "%%time\n",
        "pd.options.display.max_columns = None\n",
        "print(match(3, False))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  8.96544  78.0531  109.272  93.1641  65.1218  87.6969   106.41  76.1182\n",
            "es  78.8428   8.5912  101.603  134.955  52.6994  143.156  57.7033  101.265\n",
            "fr  58.3061  66.1363  8.56056  106.203  56.6941  99.7183  86.5802  98.8763\n",
            "in  57.6622  86.8521  145.839  9.87273  74.3799  89.9082    108.3   59.971\n",
            "it  73.2088  59.7344  95.8546  144.067  8.60361  144.459  74.6154  93.2924\n",
            "nl  49.1501   86.677  95.5061  88.6947  76.5365  9.16546  110.144  79.1393\n",
            "pt  97.6712  55.0793  124.818  178.722  69.2821  185.273  8.05634  117.905\n",
            "tl  50.5246  74.3102   154.86  64.0306    69.47  112.556  98.4439  8.50025\n",
            "CPU times: user 39.6 s, sys: 106 ms, total: 39.7 s\n",
            "Wall time: 39.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZYPxdpQasBZ",
        "colab_type": "code",
        "outputId": "9147aeee-9830-42ca-9952-5bd2b9048e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "%%time\n",
        "pd.options.display.max_columns = None\n",
        "print(match(3, True))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  26.3189  56.1303  56.5069  75.7786   58.678  59.4674  65.8466  69.4008\n",
            "es  70.2417  24.2023  58.3679  88.1664   47.765  81.1119  45.7008  86.5626\n",
            "fr   57.368  47.4652   24.232  84.2093  52.9249  67.8738   58.516  86.7851\n",
            "in  61.7705  65.3927  75.3658  30.4108  66.3883  70.5887  74.1277  56.7802\n",
            "it  68.2684  45.9564  59.4491  90.6819  24.9038  84.7244  52.3253  79.6418\n",
            "nl  53.0435  67.0519  63.0558  77.1002  70.7973  27.1713  76.7761  76.1951\n",
            "pt   79.311  44.7339  66.8505  101.437  56.6742  95.7371  25.4544  92.4071\n",
            "tl  53.6397  62.1633  75.2583  58.1438  63.0929  75.8147  71.3543  28.5867\n",
            "CPU times: user 41 s, sys: 92 ms, total: 41.1 s\n",
            "Wall time: 41.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17",
        "colab_type": "text"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk32naXyAMdl",
        "colab_type": "code",
        "outputId": "f0e505d8-544c-433f-8530-14f016c7c09f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "pd.options.display.max_columns = None\n",
        "vocabulary = preprocess()\n",
        "for n in range(0, 4):\n",
        "  print(\"\\n\\nn: {}. add_one: False\".format(n + 1))\n",
        "  print(match(n + 1, False))\n",
        "  print(\"\\n\\nn: {}. add_one: True\".format(n + 1))\n",
        "  print(match(n + 1, True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "n: 1. add_one: False\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  38.5819  41.7284  43.9312  41.7317  41.2459  39.7133  45.2394  45.0282\n",
            "es  42.1904  36.2134  41.5859  43.8699  40.4491  41.5543  41.0399  47.5528\n",
            "fr   41.802   40.208  37.5004  44.6866  39.9451  40.9627  42.4084   49.471\n",
            "in  42.6906  45.4138  48.1969  37.3912  43.8748  41.9243  48.3191  42.9053\n",
            "it  41.6723  39.0687  40.9179  43.6638  37.5525  41.0965  42.6782  46.7371\n",
            "nl  40.9808  41.1384  42.1104   41.815  41.1531  37.4997  43.9915  46.7566\n",
            "pt  42.7496  37.6343  41.0913  43.2408  41.0032  41.7144  37.0763   47.514\n",
            "tl  42.4023  43.1284   48.603  39.3321  43.3317  42.8451   45.963  40.7645\n",
            "\n",
            "\n",
            "n: 1. add_one: True\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  38.6302  39.7953  41.1775  41.2137  39.5433  39.4878  41.0819  44.5881\n",
            "es  41.7911  36.2646  39.9556   43.276  39.3732  41.1705  38.0581  47.0439\n",
            "fr   41.334   39.384   37.546  44.1159  39.5241  40.6741  39.7526  49.0089\n",
            "in  42.3472  40.8326  45.9093  37.4421  42.3289  41.6147  42.5383  42.5591\n",
            "it  41.1327  38.7656  39.8175   43.068  37.6103  40.7713  39.5173   46.219\n",
            "nl   40.445   40.217  41.5154  41.2038  40.4064  37.5528  41.4904  46.2216\n",
            "pt   42.241  37.3908  40.0325  42.6152  39.6902  41.2996  37.1431  46.9012\n",
            "tl  41.8673  42.4975  46.4135  38.8072  41.8065   42.558  44.2155  40.8298\n",
            "\n",
            "\n",
            "n: 2. add_one: False\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  18.2886  30.2626  33.4835  27.5662     27.6  25.3983  37.8465  27.0262\n",
            "es  29.1031  16.2653  28.9506  30.9691  23.4261  30.0798  26.2961  30.5896\n",
            "fr  26.1245  27.6946  17.1228  30.1329  24.6027  27.1802  31.9095  31.4539\n",
            "in  26.6907  34.0945    41.14  18.1508  28.3149  27.0107  41.3162  24.0628\n",
            "it  28.7876   23.751  30.4275  30.4894  16.7155  30.1302  29.7926  29.8152\n",
            "nl   25.054  31.7616  29.6374  28.1347  28.8831  17.9473  37.0871  29.2788\n",
            "pt  29.8409  21.4945  28.7374  32.3514  24.7088  31.2407   16.596  31.7168\n",
            "tl  24.9965  28.4714  41.4229  23.4979  27.2717  28.1747  35.6522  17.9873\n",
            "\n",
            "\n",
            "n: 2. add_one: True\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  21.1462    27.37  28.6679  29.9039  27.6788  27.7599  30.6839  29.6472\n",
            "es  32.2397  18.9511  28.4346   34.457   24.622  33.3454   25.095  34.5817\n",
            "fr  28.5709  25.0576  19.8275    33.05  26.3281  29.8666  28.0505  35.1141\n",
            "in  29.8012  29.8618  33.5594  21.2968  28.9791  29.9599  33.1593  26.9741\n",
            "it  31.8034  24.3839  29.0322   33.762  19.4968  33.3769  26.8671  33.4563\n",
            "nl  27.4539  30.5522  30.5669   30.627  30.7585  20.6685  33.4576  32.3588\n",
            "pt  33.6352  23.5871  29.4191   36.207  26.0445  35.1767  19.9711  36.1806\n",
            "tl  27.7583  28.5018  33.4332  25.9164   27.835  31.2773  31.6653  21.4416\n",
            "\n",
            "\n",
            "n: 3. add_one: False\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  8.96544  78.0531  109.272  93.1641  65.1218  87.6969   106.41  76.1182\n",
            "es  78.8428   8.5912  101.603  134.955  52.6994  143.156  57.7033  101.265\n",
            "fr  58.3061  66.1363  8.56056  106.203  56.6941  99.7183  86.5802  98.8763\n",
            "in  57.6622  86.8521  145.839  9.87273  74.3799  89.9082    108.3   59.971\n",
            "it  73.2088  59.7344  95.8546  144.067  8.60361  144.459  74.6154  93.2924\n",
            "nl  49.1501   86.677  95.5061  88.6947  76.5365  9.16546  110.144  79.1393\n",
            "pt  97.6712  55.0793  124.818  178.722  69.2821  185.273  8.05634  117.905\n",
            "tl  50.5246  74.3102   154.86  64.0306    69.47  112.556  98.4439  8.50025\n",
            "\n",
            "\n",
            "n: 3. add_one: True\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  26.3189  56.1303  56.5069  75.7786   58.678  59.4674  65.8466  69.4008\n",
            "es  70.2417  24.2023  58.3679  88.1664   47.765  81.1119  45.7008  86.5626\n",
            "fr   57.368  47.4652   24.232  84.2093  52.9249  67.8738   58.516  86.7851\n",
            "in  61.7705  65.3927  75.3658  30.4108  66.3883  70.5887  74.1277  56.7802\n",
            "it  68.2684  45.9564  59.4491  90.6819  24.9038  84.7244  52.3253  79.6418\n",
            "nl  53.0435  67.0519  63.0558  77.1002  70.7973  27.1713  76.7761  76.1951\n",
            "pt   79.311  44.7339  66.8505  101.437  56.6742  95.7371  25.4544  92.4071\n",
            "tl  53.6397  62.1633  75.2583  58.1438  63.0929  75.8147  71.3543  28.5867\n",
            "\n",
            "\n",
            "n: 4. add_one: False\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  4.47687  1851.24  1936.52  8087.42  1930.68  2816.88  3842.61  2742.95\n",
            "es  1876.37  4.72036  2203.77  12640.1  719.916  8345.32  642.929  4174.94\n",
            "fr  902.754  1081.19   4.4721  9630.12  1224.51  4648.37  2209.71  5498.47\n",
            "in  673.634   2420.4  3754.28  5.05066  2223.99  3867.59  3896.38  918.538\n",
            "it  1324.93  567.367  1920.34  12567.3  4.62153  9413.43  876.523  3363.66\n",
            "nl  515.704  2700.54  1984.78  6305.62  2756.34  4.55145  5012.31  3585.72\n",
            "pt  2932.08  634.171  3904.28  22803.1  1327.34  16368.8  4.34192  5934.23\n",
            "tl  356.619  1179.84  3639.21  1375.08  1269.73  4800.99  2129.38  4.27057\n",
            "\n",
            "\n",
            "n: 4. add_one: True\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  59.9832  258.865  223.544  364.203  264.892   237.45  297.274  288.104\n",
            "es  277.795  54.9381  218.041  384.731  183.309  312.089  153.752  348.384\n",
            "fr  223.542  192.676  52.8425  390.933  227.388  268.278  243.723  364.615\n",
            "in  246.572  308.572  324.973  76.3116  304.768  306.004  338.322  221.492\n",
            "it  264.379  170.621  233.153  388.325  56.8371  332.089  193.525  310.583\n",
            "nl  203.184    292.9  245.144  363.092   317.78  61.7214  347.663  327.844\n",
            "pt  319.365  159.206  274.569   446.56  233.211  392.684  57.4256  370.966\n",
            "tl  196.174  275.441  319.861  254.055  281.077  325.115  306.211  66.0217\n",
            "CPU times: user 5min 11s, sys: 1.89 s, total: 5min 12s\n",
            "Wall time: 5min 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW",
        "colab_type": "text"
      },
      "source": [
        "# **Good luck!**"
      ]
    }
  ]
}