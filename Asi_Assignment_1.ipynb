{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asi Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronenbendavid/IDC_NLP/blob/master/Asi_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM",
        "colab_type": "text"
      },
      "source": [
        "*As a preparation for this task, place the data files somewhere in your drive so that you can access the files from this notebook. The files are available to download from the Moodle assignment activity*\n",
        "\n",
        "The relevant files are:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z92NTjzV5zFc",
        "colab_type": "code",
        "outputId": "e02e0e05-b640-4253-b411-2d8e24805d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfzsITW8Yaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.chdir('/content/drive/My Drive/idc/nlp/ex1/')\n",
        "\n",
        "def preprocess():\n",
        "  vocabulary = set()\n",
        "  pathlist = glob('*.csv')\n",
        "  for data_file_path in pathlist:\n",
        "      data_file = pd.read_csv(data_file_path)\n",
        "      for data in data_file['tweet_text'].values:\n",
        "          vocabulary.update(list(data))\n",
        "\n",
        "  vocabulary.add('<s>')\n",
        "  vocabulary.add('</s>')\n",
        "\n",
        "  # vocabulary.discard(' ')\n",
        "  # vocabulary.discard('\\t')\n",
        "  # vocabulary.discard('\\r')\n",
        "  # vocabulary.discard('\\n')\n",
        "\n",
        "  return sorted(list(vocabulary))\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q0-OHfeo2og",
        "colab_type": "code",
        "outputId": "bc7071cf-239d-4a99-9d58-a97f0c30105b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "%%time\n",
        "vocabulary = preprocess()\n",
        "print(len(vocabulary))\n",
        "print(vocabulary)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1861\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '</s>', '<s>', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x97', '\\x9d', '¡', '£', '¤', '¥', '§', '¨', '©', 'ª', '«', '\\xad', '®', '¯', '°', '²', '³', '´', '¶', '·', '¸', 'º', '»', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Å', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ù', 'Ú', 'Ü', 'à', 'á', 'â', 'ã', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ė', 'Ğ', 'ğ', 'İ', 'ı', 'ń', 'ō', 'Œ', 'œ', 'Ş', 'ş', 'Š', 'Ÿ', 'ƒ', 'ʔ', 'ʕ', 'ʖ', 'ʰ', 'ʳ', 'ʷ', 'ʸ', 'ˍ', '˖', '˘', '˚', '˛', 'ˡ', 'ˢ', '̀', '́', '̃', '̈', '̥', '̮', '̯', '͜', '͡', 'Δ', 'Θ', 'Ω', 'υ', 'ω', 'А', 'И', 'М', 'Н', 'О', 'П', 'Р', 'Ф', 'Ц', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'з', 'и', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'х', 'ы', 'э', 'ю', 'я', 'Ғ', 'ү', '،', 'آ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ض', 'ط', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ی', '۶', 'ं', 'क', 'ग', 'प', 'ब', 'र', 'स', 'ा', 'े', '्', 'ೃ', '෴', 'ก', 'ข', 'ง', 'จ', 'ญ', 'ด', 'ต', 'ถ', 'ท', 'น', 'บ', 'ป', 'พ', 'ภ', 'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ส', 'อ', 'ะ', 'ั', 'า', 'ิ', 'ี', 'ุ', 'ู', 'เ', 'แ', '่', '้', '๐', '๑', 'ຶ', '༎', '༺', '༻', '༼', '༽', 'ღ', 'ᙓ', 'ᴗ', 'ᴬ', 'ᴰ', 'ᵃ', 'ᵇ', 'ᵈ', 'ᵉ', 'ᵍ', 'ᵐ', 'ᵒ', 'ᵖ', 'ᵗ', 'ᵘ', 'ᵛ', 'ᶜ', 'ᶠ', 'ᶦ', 'ᶰ', '\\u2009', '\\u200a', '\\u200b', '\\u200d', '–', '—', '―', '‘', '’', '“', '”', '„', '†', '•', '․', '…', '‰', '′', '‹', '›', '※', '‼', '‿', '⁉', '\\u2066', '\\u2067', '\\u2069', 'ⁱ', '⁷', 'ⁿ', '€', '₹', '⃣', '℃', '℅', '™', 'Ⅰ', 'Ⅱ', 'Ⅲ', 'Ⅳ', 'Ⅴ', '←', '↑', '→', '↓', '↔', '↕', '↗', '↘', '↚', '↛', '↩', '↪', '↯', '↺', '⇘', '⇨', '∀', '∆', '∇', '√', '∞', '∴', '∵', '≤', '≥', '≦', '≧', '⊙', '⋅', '⋪', '⋭', '⌚', '⌛', '⌣', '⎋', '⏩', '⏰', '⏱', '⏳', '⏸', '①', '⑥', '⒈', '⒉', '⒊', '⒋', '⒌', '⒍', '⒎', '⒏', '⒐', '⒑', 'Ⓜ', 'ⓘ', 'ⓙ', 'ⓢ', 'ⓦ', '─', '━', '┃', '┄', '┆', '┌', '┏', '┐', '┓', '└', '┗', '┘', '┛', '┳', '┻', '║', '╔', '╗', '╚', '╝', '╦', '╩', '╬', '╭', '╮', '╯', '╰', '╱', '╲', '╴', '█', '▊', '▏', '▒', '▔', '▕', '▙', '▝', '▣', '▦', '▪', '▲', '△', '▶', '▸', '►', '▼', '▽', '▿', '◀', '◁', '◄', '◆', '◇', '◈', '○', '◎', '●', '◑', '◕', '◡', '◻', '◼', '◽', '◾', '☀', '☁', '☃', '☄', '★', '☆', '☉', '☎', '☑', '☓', '☔', '☕', '☘', '☙', '☚', '☛', '☜', '☝', '☞', '☠', '☣', '☪', '☮', '☯', '☰', '☹', '☺', '☼', '☽', '☾', '♀', '♂', '♊', '♋', '♍', '♎', '♏', '♐', '♓', '♛', '♡', '♣', '♤', '♥', '♦', '♩', '♪', '♫', '♬', '♯', '♻', '⚒', '⚓', '⚔', '⚕', '⚖', '⚘', '⚜', '⚝', '⚠', '⚡', '⚪', '⚫', '⚰', '⚽', '⚾', '⛄', '⛅', '⛈', '⛓', '⛔', '⛩', '⛪', '⛳', '⛷', '⛽', '✁', '✂', '✃', '✅', '✈', '✉', '✊', '✋', '✌', '✍', '✏', '✓', '✔', '✖', '✝', '✡', '✧', '✨', '✩', '✪', '✭', '✰', '✳', '✴', '✵', '✶', '✷', '✿', '❀', '❁', '❄', '❅', '❈', '❋', '❌', '❎', '❓', '❔', '❗', '❝', '❞', '❣', '❤', '❥', '➊', '➋', '➌', '➍', '➎', '➏', '➔', '➖', '➗', '➙', '➛', '➜', '➞', '➟', '➠', '➡', '➢', '➤', '➰', '➵', '⠀', '⤵', '⦑', '⦒', '⬅', '⬇', '⭐', '⸄', '⸅', '\\u3000', '、', '。', '〆', '《', '》', '「', '」', '『', '』', '【', '】', '〜', '〡', '〰', 'い', 'う', 'え', 'お', 'か', 'が', 'き', 'く', 'し', 'せ', 'ぜ', 'た', 'っ', 'づ', 'て', 'で', 'と', 'な', 'に', 'ね', 'の', 'は', 'ひ', 'み', 'む', 'ょ', 'ら', 'り', 'る', 'れ', 'わ', 'を', '゜', 'イ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'キ', 'ク', 'グ', 'コ', 'ゴ', 'サ', 'ジ', 'ス', 'セ', 'タ', 'チ', 'ッ', 'ツ', 'テ', 'ト', 'ド', 'ナ', 'ニ', 'ネ', 'ノ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ポ', 'ム', 'メ', 'ュ', 'ユ', 'ョ', 'ラ', 'レ', 'ロ', 'ヮ', 'ワ', 'ン', '・', 'ー', 'ヽ', 'ㅅ', 'ㅈ', 'ㅋ', 'ㅏ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ', 'ㅤ', '\\u31ef', '世', '中', '主', '互', '人', '付', '会', '像', '儿', '允', '先', '入', '写', '分', '利', '制', '刹', '力', '努', '動', '午', '卒', '南', '合', '呟', '嘉', '増', '好', '姿', '嫌', '学', '尔', '希', '彡', '影', '彼', '後', '悪', '手', '投', '拶', '挨', '撃', '撮', '文', '映', '時', '曲', '月', '有', '服', '本', '林', '柱', '業', '機', '歌', '歳', '毅', '気', '洲', '洸', '王', '生', '用', '画', '界', '相', '真', '瞬', '知', '社', '稿', '空', '糟', '終', '結', '繋', '者', '花', '菜', '行', '许', '赫', '踊', '込', '通', '那', '間', '限', '風', '魏', 'ꠎ', '가', '간', '갓', '강', '걸', '검', '게', '격', '결', '경', '고', '곡', '과', '구', '국', '규', '그', '근', '금', '기', '김', '꺽', '꼼', '나', '날', '남', '내', '너', '널', '네', '넷', '녀', '년', '노', '논', '누', '는', '늘', '니', '다', '단', '당', '닿', '대', '더', '도', '동', '두', '둑', '듀', '드', '등', '디', '라', '락', '랑', '랙', '랜', '램', '러', '런', '레', '렛', '로', '롱', '료', '루', '룰', '룸', '를', '름', '릉', '리', '림', '링', '마', '맞', '매', '맨', '몬', '무', '미', '민', '밀', '바', '박', '방', '배', '백', '뱀', '버', '벅', '법', '베', '벨', '벳', '보', '복', '본', '봄', '봉', '뷔', '브', '븐', '블', '비', '빅', '빼', '사', '살', '삼', '상', '생', '샤', '샵', '서', '석', '선', '성', '세', '섹', '셔', '션', '셩', '소', '송', '수', '슈', '스', '슨', '슬', '승', '시', '식', '신', '실', '싸', '아', '안', '압', '애', '야', '양', '어', '에', '엑', '엘', '엠', '엣', '여', '역', '연', '영', '예', '오', '온', '와', '왕', '외', '요', '용', '우', '울', '워', '원', '위', '유', '윤', '의', '이', '인', '일', '임', '잘', '장', '재', '잭', '전', '정', '제', '젤', '종', '주', '쥔', '즈', '지', '직', '진', '집', '쩜', '찌', '찰', '채', '천', '철', '초', '최', '추', '출', '츠', '치', '카', '커', '코', '콘', '콤', '쿱', '크', '키', '킹', '타', '탄', '탑', '태', '터', '텐', '토', '톡', '트', '티', '틴', '팁', '파', '패', '펀', '포', '풀', '프', '플', '피', '핑', '하', '한', '해', '핸', '헌', '헤', '헨', '혁', '현', '형', '호', '화', '환', '훈', '힐', 'ﷻ', '︎', '️', '︵', '﹏', '﹪', '！', '＂', '＃', '（', '）', '＊', '．', '３', '６', '７', '８', '？', '＠', 'Ａ', 'Ｂ', 'Ｃ', 'Ｄ', 'Ｅ', 'Ｆ', 'Ｇ', 'Ｈ', 'Ｉ', 'Ｋ', 'Ｌ', 'Ｍ', 'Ｎ', 'Ｏ', 'Ｐ', 'Ｑ', 'Ｒ', 'Ｓ', 'Ｔ', 'Ｕ', 'Ｖ', 'Ｗ', 'Ｙ', '［', '］', '＿', '｀', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｇ', 'ｉ', 'ｌ', 'ｍ', 'ｎ', 'ｏ', 'ｐ', 'ｑ', 'ｒ', 'ｓ', 'ｔ', 'ｕ', 'ｖ', 'ｗ', 'ｘ', 'ｙ', 'ｚ', '｜', '｡', '･', 'ﾉ', 'ﾒ', '￣', '￼', '�', '🃏', '🅰', '🅱', '🅾', '🅿', '🆑', '🆒', '🆓', '🆔', '🆕', '🆖', '🆗', '🆘', '🆙', '🆚', '🇦', '🇧', '🇨', '🇩', '🇪', '🇫', '🇬', '🇭', '🇮', '🇯', '🇰', '🇱', '🇲', '🇳', '🇴', '🇵', '🇷', '🇸', '🇹', '🇺', '🇻', '🇼', '🇽', '🇾', '🇿', '🈴', '🈵', '🈶', '🈷', '🌀', '🌃', '🌄', '🌅', '🌆', '🌇', '🌈', '🌊', '🌋', '🌌', '🌍', '🌎', '🌏', '🌐', '🌒', '🌓', '🌗', '🌙', '🌚', '🌛', '🌜', '🌝', '🌞', '🌟', '🌠', '🌤', '🌥', '🌧', '🌨', '🌪', '🌫', '🌬', '🌭', '🌮', '🌯', '🌰', '🌱', '🌲', '🌳', '🌴', '🌵', '🌶', '🌷', '🌸', '🌹', '🌺', '🌻', '🌼', '🌽', '🌾', '🌿', '🍀', '🍁', '🍂', '🍃', '🍅', '🍆', '🍇', '🍉', '🍊', '🍋', '🍌', '🍍', '🍎', '🍏', '🍑', '🍒', '🍓', '🍔', '🍕', '🍖', '🍗', '🍚', '🍛', '🍜', '🍝', '🍞', '🍟', '🍣', '🍤', '🍥', '🍦', '🍨', '🍩', '🍪', '🍫', '🍬', '🍭', '🍯', '🍰', '🍱', '🍳', '🍴', '🍵', '🍶', '🍷', '🍸', '🍹', '🍺', '🍻', '🍼', '🍽', '🍾', '🍿', '🎀', '🎁', '🎂', '🎅', '🎆', '🎇', '🎈', '🎉', '🎊', '🎋', '🎍', '🎒', '🎓', '🎗', '🎙', '🎞', '🎟', '🎠', '🎡', '🎢', '🎤', '🎥', '🎦', '🎧', '🎨', '🎩', '🎪', '🎫', '🎬', '🎭', '🎮', '🎯', '🎰', '🎱', '🎲', '🎵', '🎶', '🎷', '🎸', '🎹', '🎺', '🎻', '🎼', '🎾', '🎿', '🏀', '🏁', '🏃', '🏄', '🏅', '🏆', '🏇', '🏈', '🏉', '🏊', '🏋', '🏌', '🏒', '🏓', '🏔', '🏖', '🏘', '🏙', '🏚', '🏟', '🏠', '🏡', '🏢', '🏩', '🏫', '🏰', '🏳', '🏴', '🏹', '🏻', '🏼', '🏽', '🏾', '🏿', '🐀', '🐁', '🐂', '🐄', '🐆', '🐇', '🐈', '🐉', '🐊', '🐍', '🐎', '🐐', '🐑', '🐒', '🐓', '🐔', '🐕', '🐖', '🐘', '🐙', '🐚', '🐜', '🐝', '🐞', '🐟', '🐠', '🐡', '🐢', '🐣', '🐥', '🐦', '🐧', '🐨', '🐩', '🐫', '🐬', '🐭', '🐮', '🐯', '🐰', '🐱', '🐲', '🐳', '🐴', '🐶', '🐷', '🐸', '🐹', '🐺', '🐻', '🐼', '🐽', '🐾', '🐿', '👀', '👁', '👂', '👄', '👅', '👆', '👇', '👈', '👉', '👊', '👋', '👌', '👍', '👎', '👏', '👐', '👑', '👓', '👕', '👖', '👗', '👙', '👞', '👟', '👠', '👡', '👣', '👤', '👥', '👦', '👧', '👨', '👩', '👪', '👫', '👭', '👮', '👯', '👰', '👱', '👲', '👳', '👵', '👶', '👷', '👸', '👹', '👺', '👻', '👼', '👽', '👿', '💀', '💁', '💂', '💃', '💄', '💅', '💆', '💈', '💉', '💊', '💋', '💌', '💍', '💎', '💏', '💐', '💑', '💒', '💓', '💔', '💕', '💖', '💗', '💘', '💙', '💚', '💛', '💜', '💝', '💞', '💟', '💡', '💢', '💣', '💤', '💥', '💦', '💧', '💨', '💩', '💪', '💫', '💬', '💭', '💮', '💯', '💰', '💲', '💳', '💵', '💶', '💸', '💻', '💼', '💽', '💿', '📀', '📂', '📅', '📆', '📈', '📊', '📋', '📌', '📍', '📏', '📓', '📖', '📚', '📛', '📝', '📞', '📡', '📢', '📣', '📦', '📧', '📩', '📬', '📯', '📰', '📱', '📲', '📴', '📷', '📸', '📹', '📺', '📻', '📼', '📽', '📿', '🔁', '🔂', '🔃', '🔄', '🔅', '🔉', '🔊', '🔋', '🔌', '🔐', '🔑', '🔒', '🔓', '🔔', '🔘', '🔙', '🔛', '🔜', '🔝', '🔞', '🔥', '🔨', '🔩', '🔪', '🔫', '🔮', '🔰', '🔱', '🔲', '🔴', '🔵', '🔶', '🔸', '🔹', '🔺', '🔻', '🔼', '🔽', '🕊', '🕋', '🕌', '🕎', '🕐', '🕒', '🕘', '🕛', '🕜', '🕟', '🕤', '🕪', '🕯', '🕵', '🕶', '🕷', '🕺', '🖐', '🖒', '🖕', '🖖', '🖤', '🖥', '🖲', '🖼', '🗂', '🗓', '🗝', '🗞', '🗡', '🗣', '🗨', '🗳', '🗻', '🗼', '🗽', '🗾', '😀', '😁', '😂', '😃', '😄', '😅', '😆', '😇', '😈', '😉', '😊', '😋', '😌', '😍', '😎', '😏', '😐', '😑', '😒', '😓', '😔', '😕', '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😦', '😧', '😨', '😩', '😪', '😫', '😬', '😭', '😮', '😯', '😰', '😱', '😲', '😳', '😴', '😵', '😶', '😷', '😸', '😹', '😺', '😻', '😼', '😽', '😿', '🙀', '🙁', '🙂', '🙃', '🙄', '🙅', '🙆', '🙇', '🙈', '🙉', '🙊', '🙋', '🙌', '🙍', '🙎', '🙏', '🚀', '🚁', '🚇', '🚈', '🚌', '🚑', '🚓', '🚔', '🚖', '🚗', '🚘', '🚙', '🚢', '🚣', '🚦', '🚧', '🚨', '🚩', '🚫', '🚬', '🚮', '🚲', '🚴', '🚵', '🚶', '🚻', '🚼', '🚿', '🛀', '🛁', '🛃', '🛄', '🛐', '🛩', '🛫', '🛬', '🛰', '🛳', '🛴', '🤐', '🤑', '🤒', '🤓', '🤔', '🤕', '🤖', '🤗', '🤘', '🤙', '🤚', '🤛', '🤜', '🤝', '🤞', '🤠', '🤡', '🤢', '🤣', '🤤', '🤥', '🤦', '🤧', '🤳', '🤴', '🤷', '🥀', '🥁', '🥂', '🥃', '🥄', '🥅', '🥇', '🥊', '🥐', '🥒', '🥓', '🥔', '🥘', '🥙', '🥞', '🦀', '🦁', '🦃', '🦄', '🦅', '🦇', '🦉', '🦋', '🦑', '\\U000fe4e6']\n",
            "CPU times: user 287 ms, sys: 21.7 ms, total: 309 ms\n",
            "Wall time: 331 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMC_u8eQbVvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict, UserDict\n",
        "\n",
        "class dict_value(UserDict):\n",
        "    def __init__(self, value=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.value = value\n",
        "\n",
        "    def __missing__(self, key):\n",
        "        return self.value\n",
        "\n",
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  voc_size = len(vocabulary)\n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "  data_file = pd.read_csv(data_file_path)\n",
        "  for data in data_file['tweet_text'].values:\n",
        "      data = [\"<s>\"] + list(data) + [\"</s>\"]\n",
        "      for i in range(len(data) - n + 1):\n",
        "          word, char = ''.join(data[i:i + n - 1]), data[i + n - 1]\n",
        "          model[word][char] += 1\n",
        "\n",
        "  if add_one:\n",
        "      pmodel = defaultdict(lambda: defaultdict(lambda: 1 / voc_size))\n",
        "  else:\n",
        "      pmodel = defaultdict(lambda: defaultdict(lambda: 1e-08))\n",
        "  for word, counts in model.items():\n",
        "      if add_one:\n",
        "          total_count = sum(counts.values()) + voc_size\n",
        "          pmodel[word] = dict_value(1/total_count)\n",
        "          for char in counts:\n",
        "              pmodel[word][char] = (counts[char] + 1) / total_count\n",
        "      else:\n",
        "          total_count = sum(counts.values())\n",
        "          for char in counts:\n",
        "              pmodel[word][char] = counts[char] / total_count\n",
        "\n",
        "  return pmodel\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU5gq5sCbUaR",
        "colab_type": "code",
        "outputId": "98939877-ef96-4a51-f156-d68cb35770f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "%%time\n",
        "vocabulary = preprocess()\n",
        "model = lm(3, vocabulary, 'en.csv', False)\n",
        "print(model['Ab'])\n",
        "print(sum(v for v in model['Ab'].values()))\n",
        "print(sum(model['Ab'][v] for v in vocabulary))\n",
        "\n",
        "model = lm(3, vocabulary, 'en.csv', True)\n",
        "print(model['Ab'])\n",
        "print(sum(v for v in model['Ab'].values()))\n",
        "print(sum(model['Ab'][v] for v in vocabulary))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function lm.<locals>.<lambda>.<locals>.<lambda> at 0x7f2dab1d3bf8>, {'o': 0.2571428571428571, 's': 0.1, 'e': 0.05714285714285714, 'U': 0.04285714285714286, 'u': 0.08571428571428572, 'D': 0.014285714285714285, 'd': 0.1, 'a': 0.014285714285714285, 'b': 0.1, '7': 0.014285714285714285, 'y': 0.014285714285714285, 'l': 0.02857142857142857, 'j': 0.014285714285714285, 'q': 0.014285714285714285, '2': 0.014285714285714285, 'i': 0.02857142857142857, '</s>': 0.014285714285714285, 'r': 0.05714285714285714, 'n': 0.014285714285714285, 'Z': 0.014285714285714285})\n",
            "0.9999999999999994\n",
            "1.0000184099998928\n",
            "{'o': 0.00983946141895391, 's': 0.004142931123770067, 'e': 0.002589331952356292, 'U': 0.0020714655618850335, 'u': 0.0036250647332988087, 'D': 0.0010357327809425167, 'd': 0.004142931123770067, 'a': 0.0010357327809425167, 'b': 0.004142931123770067, '7': 0.0010357327809425167, 'y': 0.0010357327809425167, 'l': 0.0015535991714137752, 'j': 0.0010357327809425167, 'q': 0.0010357327809425167, '2': 0.0010357327809425167, 'i': 0.0015535991714137752, '</s>': 0.0010357327809425167, 'r': 0.002589331952356292, 'n': 0.0010357327809425167, 'Z': 0.0010357327809425167}\n",
            "0.04660797514241325\n",
            "0.999999999999983\n",
            "CPU times: user 1.49 s, sys: 36.6 ms, total: 1.53 s\n",
            "Wall time: 1.55 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3",
        "colab_type": "text"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kkMn328-lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def eval(n, model, data_file_path):\n",
        "  # n - the n-gram that you used to build your model (must be the same number)\n",
        "  # model - the dictionary (model) to use for calculating perplexity\n",
        "  # data_file - the tweets file that you wish to claculate a perplexity score for\n",
        "\n",
        "  data_file = pd.read_csv(data_file_path)\n",
        "  count = 0\n",
        "  total = 0.0\n",
        "  for data in data_file['tweet_text'].values:\n",
        "      data = ['<s>'] + list(data) + ['</s>']\n",
        "      for i in range(len(data) - n + 1):\n",
        "          count += 1\n",
        "          word, char = ''.join(data[i:i + n - 1]), data[i + n - 1]\n",
        "          total += math.log2(model[word][char])\n",
        "\n",
        "  ent = -1 / count * total\n",
        "  per = 2 ** ent\n",
        "\n",
        "  return per"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRJFGceKAZ7j",
        "colab_type": "code",
        "outputId": "bf96513f-5a15-4e1e-c162-34ff91381b2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "model = lm(3, vocabulary, 'en.csv', False)\n",
        "print(eval(3, model, 'en.csv'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.965438001439026\n",
            "CPU times: user 1.18 s, sys: 8.87 ms, total: 1.19 s\n",
            "Wall time: 1.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCK_py9HYV6-",
        "colab_type": "code",
        "outputId": "8ecabcab-9ca2-4d74-c501-6d4e1c004dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "model = lm(3, vocabulary, 'en.csv', True)\n",
        "print(eval(3, model, 'en.csv'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26.318871163182674\n",
            "CPU times: user 1.47 s, sys: 10 ms, total: 1.48 s\n",
            "Wall time: 1.48 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p",
        "colab_type": "text"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAxLE9s_fvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match(n, add_one):\n",
        "  # n - the n-gram to use for creating n-gram models\n",
        "  # add_one - use add_one smoothing or not\n",
        "\n",
        "  file_path = '{}.csv'\n",
        "  vocabulary = preprocess()\n",
        "\n",
        "  lang = ['en' ,'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "  df = pd.DataFrame(columns=lang, index=lang)\n",
        "  for l1 in lang:\n",
        "    l1_model = lm(n, vocabulary, file_path.format(l1), add_one)\n",
        "    for l2 in lang:\n",
        "      df.at[l1, l2] = eval(n, l1_model, file_path.format(l2))\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdQ-_2dMYbvQ",
        "colab_type": "code",
        "outputId": "ac1d1af7-595c-43fb-89c8-b7b469c0efb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "%%time\n",
        "pd.options.display.max_columns = None\n",
        "print(match(3, False))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  8.96544  78.0531  109.272  93.1641  65.1218  87.6969   106.41  76.1182\n",
            "es  78.8428   8.5912  101.603  134.955  52.6994  143.156  57.7033  101.265\n",
            "fr  58.3061  66.1363  8.56056  106.203  56.6941  99.7183  86.5802  98.8763\n",
            "in  57.6622  86.8521  145.839  9.87273  74.3799  89.9082    108.3   59.971\n",
            "it  73.2088  59.7344  95.8546  144.067  8.60361  144.459  74.6154  93.2924\n",
            "nl  49.1501   86.677  95.5061  88.6947  76.5365  9.16546  110.144  79.1393\n",
            "pt  97.6712  55.0793  124.818  178.722  69.2821  185.273  8.05634  117.905\n",
            "tl  50.5246  74.3102   154.86  64.0306    69.47  112.556  98.4439  8.50025\n",
            "CPU times: user 41.8 s, sys: 155 ms, total: 41.9 s\n",
            "Wall time: 42.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZYPxdpQasBZ",
        "colab_type": "code",
        "outputId": "efd18922-3227-4d1a-c39d-56a4e3a3bfb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "%%time\n",
        "pd.options.display.max_columns = None\n",
        "print(match(3, True))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  26.3189  57.2445  57.8933  77.0167    59.52  60.3326  67.5199  70.3138\n",
            "es  71.0303  24.2023  59.6484  89.7969  48.3866  82.4741  46.4629  88.0705\n",
            "fr  57.9458  48.3237   24.232  85.3929  53.6109  68.9781  59.6998  88.1311\n",
            "in  62.2854  66.4444  77.0803  30.4108  67.3064  71.3141  75.6373  57.3578\n",
            "it  69.0617  46.6899  60.6116  92.5048  24.9038   86.205  53.3553   80.954\n",
            "nl  53.5112   68.309   64.354  78.1774   71.831  27.1713   78.419  77.2418\n",
            "pt  80.3243  45.3119  68.1943  103.263  57.5052  97.3543  25.4544  93.9566\n",
            "tl  54.0615  63.0378  76.8159  58.7234   63.866  76.7101  72.8183  28.5867\n",
            "CPU times: user 58.6 s, sys: 138 ms, total: 58.7 s\n",
            "Wall time: 58.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17",
        "colab_type": "text"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk32naXyAMdl",
        "colab_type": "code",
        "outputId": "5c324e75-0009-4ccd-88f2-f6df8f575f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "pd.options.display.max_columns = None\n",
        "vocabulary = preprocess()\n",
        "for n in range(0, 4):\n",
        "  print(\"\\n\\nn: {}. add_one: False\".format(n + 1))\n",
        "  print(match(n + 1, False))\n",
        "  print(\"\\n\\nn: {}. add_one: True\".format(n + 1))\n",
        "  print(match(n + 1, True))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "n: 1. add_one: False\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  38.5819  41.7284  43.9312  41.7317  41.2459  39.7133  45.2394  45.0282\n",
            "es  42.1904  36.2134  41.5859  43.8699  40.4491  41.5543  41.0399  47.5528\n",
            "fr   41.802   40.208  37.5004  44.6866  39.9451  40.9627  42.4084   49.471\n",
            "in  42.6906  45.4138  48.1969  37.3912  43.8748  41.9243  48.3191  42.9053\n",
            "it  41.6723  39.0687  40.9179  43.6638  37.5525  41.0965  42.6782  46.7371\n",
            "nl  40.9808  41.1384  42.1104   41.815  41.1531  37.4997  43.9915  46.7566\n",
            "pt  42.7496  37.6343  41.0913  43.2408  41.0032  41.7144  37.0763   47.514\n",
            "tl  42.4023  43.1284   48.603  39.3321  43.3317  42.8451   45.963  40.7645\n",
            "\n",
            "\n",
            "n: 1. add_one: True\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  38.6302  40.8316  42.7346  41.5393  40.5401  39.6455   43.361  44.8704\n",
            "es  42.0466  36.2646  40.8816  43.6463  39.9983  41.4236  39.7485  47.3664\n",
            "fr   41.632  39.7507   37.546  44.4726  39.7949  40.8721   41.219  49.3011\n",
            "in  42.5701  43.3595  47.0202  37.4421  43.2093  41.8203  45.6565  42.7824\n",
            "it  41.4699  38.9478  40.4681  43.4362  37.6103  40.9902  41.2952  46.5431\n",
            "nl  40.7786  40.7343  41.8562   41.581   40.854  37.5528  42.9046  46.5545\n",
            "pt  42.5553  37.5536  40.6418  42.9975  40.4434  41.5704  37.1431  47.2778\n",
            "tl  42.2049  42.8348   47.579  39.1358  42.6815  42.7598  45.1095  40.8298\n",
            "\n",
            "\n",
            "n: 2. add_one: False\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  18.2886  30.2626  33.4835  27.5662     27.6  25.3983  37.8465  27.0262\n",
            "es  29.1031  16.2653  28.9506  30.9691  23.4261  30.0798  26.2961  30.5896\n",
            "fr  26.1245  27.6946  17.1228  30.1329  24.6027  27.1802  31.9095  31.4539\n",
            "in  26.6907  34.0945    41.14  18.1508  28.3149  27.0107  41.3162  24.0628\n",
            "it  28.7876   23.751  30.4275  30.4894  16.7155  30.1302  29.7926  29.8152\n",
            "nl   25.054  31.7616  29.6374  28.1347  28.8831  17.9473  37.0871  29.2788\n",
            "pt  29.8409  21.4945  28.7374  32.3514  24.7088  31.2407   16.596  31.7168\n",
            "tl  24.9965  28.4714  41.4229  23.4979  27.2717  28.1747  35.6522  17.9873\n",
            "\n",
            "\n",
            "n: 2. add_one: True\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  21.1462  28.3807  29.8763  30.0756  28.2237  27.8786  32.1634  29.8516\n",
            "es  32.4003  18.9511  29.2135  34.6239  24.9484   33.499  25.6359  34.7233\n",
            "fr   28.699  25.7852  19.8275   33.232  26.5594  29.9936  28.8392  35.3014\n",
            "in  29.9125  30.8552  35.1012  21.2968  29.4623  30.0865  34.6331  27.0657\n",
            "it  31.9355  24.7688  29.6797  33.9374  19.4968  33.5134  27.6708  33.6078\n",
            "nl  27.5579  31.3748  31.0523  30.7723   31.096  20.6685  34.5285  32.5127\n",
            "pt  33.8049  23.7428  29.8242  36.4087  26.4334   35.337  19.9711  36.3668\n",
            "tl  27.8663  29.0723   34.797  26.0213  28.2779  31.4099  32.9608  21.4416\n",
            "\n",
            "\n",
            "n: 3. add_one: False\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  8.96544  78.0531  109.272  93.1641  65.1218  87.6969   106.41  76.1182\n",
            "es  78.8428   8.5912  101.603  134.955  52.6994  143.156  57.7033  101.265\n",
            "fr  58.3061  66.1363  8.56056  106.203  56.6941  99.7183  86.5802  98.8763\n",
            "in  57.6622  86.8521  145.839  9.87273  74.3799  89.9082    108.3   59.971\n",
            "it  73.2088  59.7344  95.8546  144.067  8.60361  144.459  74.6154  93.2924\n",
            "nl  49.1501   86.677  95.5061  88.6947  76.5365  9.16546  110.144  79.1393\n",
            "pt  97.6712  55.0793  124.818  178.722  69.2821  185.273  8.05634  117.905\n",
            "tl  50.5246  74.3102   154.86  64.0306    69.47  112.556  98.4439  8.50025\n",
            "\n",
            "\n",
            "n: 3. add_one: True\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  26.3189  57.2445  57.8933  77.0167    59.52  60.3326  67.5199  70.3138\n",
            "es  71.0303  24.2023  59.6484  89.7969  48.3866  82.4741  46.4629  88.0705\n",
            "fr  57.9458  48.3237   24.232  85.3929  53.6109  68.9781  59.6998  88.1311\n",
            "in  62.2854  66.4444  77.0803  30.4108  67.3064  71.3141  75.6373  57.3578\n",
            "it  69.0617  46.6899  60.6116  92.5048  24.9038   86.205  53.3553   80.954\n",
            "nl  53.5112   68.309   64.354  78.1774   71.831  27.1713   78.419  77.2418\n",
            "pt  80.3243  45.3119  68.1943  103.263  57.5052  97.3543  25.4544  93.9566\n",
            "tl  54.0615  63.0378  76.8159  58.7234   63.866  76.7101  72.8183  28.5867\n",
            "\n",
            "\n",
            "n: 4. add_one: False\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  4.47687  1851.24  1936.52  8087.42  1930.68  2816.88  3842.61  2742.95\n",
            "es  1876.37  4.72036  2203.77  12640.1  719.916  8345.32  642.929  4174.94\n",
            "fr  902.754  1081.19   4.4721  9630.12  1224.51  4648.37  2209.71  5498.47\n",
            "in  673.634   2420.4  3754.28  5.05066  2223.99  3867.59  3896.38  918.538\n",
            "it  1324.93  567.367  1920.34  12567.3  4.62153  9413.43  876.523  3363.66\n",
            "nl  515.704  2700.54  1984.78  6305.62  2756.34  4.55145  5012.31  3585.72\n",
            "pt  2932.08  634.171  3904.28  22803.1  1327.34  16368.8  4.34192  5934.23\n",
            "tl  356.619  1179.84  3639.21  1375.08  1269.73  4800.99  2129.38  4.27057\n",
            "\n",
            "\n",
            "n: 4. add_one: True\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  59.9832  261.593  225.948  368.315  267.493  240.111  300.956  290.451\n",
            "es   279.99  54.9381  220.514  389.911  184.963  315.653  155.091  352.288\n",
            "fr  225.194  194.503  52.8425  395.801  229.371  271.397  246.274  368.269\n",
            "in   247.77  311.172  327.639  76.3116  307.172  308.152  341.189  222.878\n",
            "it  266.291   171.87  235.732   393.26  56.8371  335.511  195.118  313.649\n",
            "nl  204.413  295.791  247.441  366.381  320.293  61.7214  351.143  330.608\n",
            "pt  322.145  160.522  277.388  451.831  235.337  396.568  57.4256  374.527\n",
            "tl  197.062  277.368  322.365  255.844   282.99  327.617  308.552  66.0217\n",
            "CPU times: user 6min 29s, sys: 2.13 s, total: 6min 32s\n",
            "Wall time: 6min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW",
        "colab_type": "text"
      },
      "source": [
        "# **Good luck!**"
      ]
    }
  ]
}