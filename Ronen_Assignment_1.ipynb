{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ronen_Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronenbendavid/IDC_NLP/blob/master/Ronen_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM",
        "colab_type": "text"
      },
      "source": [
        "*As a preparation for this task, place the data files somewhere in your drive so that you can access the files from this notebook. The files are available to download from the Moodle assignment activity*\n",
        "\n",
        "The relevant files are:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGtqPrCPnLUn",
        "colab_type": "code",
        "outputId": "951684b4-427a-468b-c111-88ae9e32d106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfzsITW8Yaj",
        "colab_type": "code",
        "outputId": "18f866e7-51d4-43a8-acad-176eb475b615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess():\n",
        "  # TODO\n",
        " vocabulary = set()\n",
        " pathlist = glob('/content/drive/My Drive/IDC/NLP/Assignment1/*.csv')\n",
        " for path in pathlist:\n",
        "    with open(path, encoding='utf-8', newline='') as csv_file:\n",
        "        csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "        for text in csv_reader['tweet_text']:\n",
        "            list_text = list(text)\n",
        "            vocabulary.update(list_text)\n",
        "            line_count += 1\n",
        "        #print(line_count)\n",
        " return sorted(list(vocabulary))\n",
        "\n",
        "vocabulary = preprocess()\n",
        "print(len(vocabulary))\n",
        "print(vocabulary)\n",
        "  "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1859\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x97', '\\x9d', '¡', '£', '¤', '¥', '§', '¨', '©', 'ª', '«', '\\xad', '®', '¯', '°', '²', '³', '´', '¶', '·', '¸', 'º', '»', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Å', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ù', 'Ú', 'Ü', 'à', 'á', 'â', 'ã', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ė', 'Ğ', 'ğ', 'İ', 'ı', 'ń', 'ō', 'Œ', 'œ', 'Ş', 'ş', 'Š', 'Ÿ', 'ƒ', 'ʔ', 'ʕ', 'ʖ', 'ʰ', 'ʳ', 'ʷ', 'ʸ', 'ˍ', '˖', '˘', '˚', '˛', 'ˡ', 'ˢ', '̀', '́', '̃', '̈', '̥', '̮', '̯', '͜', '͡', 'Δ', 'Θ', 'Ω', 'υ', 'ω', 'А', 'И', 'М', 'Н', 'О', 'П', 'Р', 'Ф', 'Ц', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'з', 'и', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'х', 'ы', 'э', 'ю', 'я', 'Ғ', 'ү', '،', 'آ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ض', 'ط', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ی', '۶', 'ं', 'क', 'ग', 'प', 'ब', 'र', 'स', 'ा', 'े', '्', 'ೃ', '෴', 'ก', 'ข', 'ง', 'จ', 'ญ', 'ด', 'ต', 'ถ', 'ท', 'น', 'บ', 'ป', 'พ', 'ภ', 'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ส', 'อ', 'ะ', 'ั', 'า', 'ิ', 'ี', 'ุ', 'ู', 'เ', 'แ', '่', '้', '๐', '๑', 'ຶ', '༎', '༺', '༻', '༼', '༽', 'ღ', 'ᙓ', 'ᴗ', 'ᴬ', 'ᴰ', 'ᵃ', 'ᵇ', 'ᵈ', 'ᵉ', 'ᵍ', 'ᵐ', 'ᵒ', 'ᵖ', 'ᵗ', 'ᵘ', 'ᵛ', 'ᶜ', 'ᶠ', 'ᶦ', 'ᶰ', '\\u2009', '\\u200a', '\\u200b', '\\u200d', '–', '—', '―', '‘', '’', '“', '”', '„', '†', '•', '․', '…', '‰', '′', '‹', '›', '※', '‼', '‿', '⁉', '\\u2066', '\\u2067', '\\u2069', 'ⁱ', '⁷', 'ⁿ', '€', '₹', '⃣', '℃', '℅', '™', 'Ⅰ', 'Ⅱ', 'Ⅲ', 'Ⅳ', 'Ⅴ', '←', '↑', '→', '↓', '↔', '↕', '↗', '↘', '↚', '↛', '↩', '↪', '↯', '↺', '⇘', '⇨', '∀', '∆', '∇', '√', '∞', '∴', '∵', '≤', '≥', '≦', '≧', '⊙', '⋅', '⋪', '⋭', '⌚', '⌛', '⌣', '⎋', '⏩', '⏰', '⏱', '⏳', '⏸', '①', '⑥', '⒈', '⒉', '⒊', '⒋', '⒌', '⒍', '⒎', '⒏', '⒐', '⒑', 'Ⓜ', 'ⓘ', 'ⓙ', 'ⓢ', 'ⓦ', '─', '━', '┃', '┄', '┆', '┌', '┏', '┐', '┓', '└', '┗', '┘', '┛', '┳', '┻', '║', '╔', '╗', '╚', '╝', '╦', '╩', '╬', '╭', '╮', '╯', '╰', '╱', '╲', '╴', '█', '▊', '▏', '▒', '▔', '▕', '▙', '▝', '▣', '▦', '▪', '▲', '△', '▶', '▸', '►', '▼', '▽', '▿', '◀', '◁', '◄', '◆', '◇', '◈', '○', '◎', '●', '◑', '◕', '◡', '◻', '◼', '◽', '◾', '☀', '☁', '☃', '☄', '★', '☆', '☉', '☎', '☑', '☓', '☔', '☕', '☘', '☙', '☚', '☛', '☜', '☝', '☞', '☠', '☣', '☪', '☮', '☯', '☰', '☹', '☺', '☼', '☽', '☾', '♀', '♂', '♊', '♋', '♍', '♎', '♏', '♐', '♓', '♛', '♡', '♣', '♤', '♥', '♦', '♩', '♪', '♫', '♬', '♯', '♻', '⚒', '⚓', '⚔', '⚕', '⚖', '⚘', '⚜', '⚝', '⚠', '⚡', '⚪', '⚫', '⚰', '⚽', '⚾', '⛄', '⛅', '⛈', '⛓', '⛔', '⛩', '⛪', '⛳', '⛷', '⛽', '✁', '✂', '✃', '✅', '✈', '✉', '✊', '✋', '✌', '✍', '✏', '✓', '✔', '✖', '✝', '✡', '✧', '✨', '✩', '✪', '✭', '✰', '✳', '✴', '✵', '✶', '✷', '✿', '❀', '❁', '❄', '❅', '❈', '❋', '❌', '❎', '❓', '❔', '❗', '❝', '❞', '❣', '❤', '❥', '➊', '➋', '➌', '➍', '➎', '➏', '➔', '➖', '➗', '➙', '➛', '➜', '➞', '➟', '➠', '➡', '➢', '➤', '➰', '➵', '⠀', '⤵', '⦑', '⦒', '⬅', '⬇', '⭐', '⸄', '⸅', '\\u3000', '、', '。', '〆', '《', '》', '「', '」', '『', '』', '【', '】', '〜', '〡', '〰', 'い', 'う', 'え', 'お', 'か', 'が', 'き', 'く', 'し', 'せ', 'ぜ', 'た', 'っ', 'づ', 'て', 'で', 'と', 'な', 'に', 'ね', 'の', 'は', 'ひ', 'み', 'む', 'ょ', 'ら', 'り', 'る', 'れ', 'わ', 'を', '゜', 'イ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'キ', 'ク', 'グ', 'コ', 'ゴ', 'サ', 'ジ', 'ス', 'セ', 'タ', 'チ', 'ッ', 'ツ', 'テ', 'ト', 'ド', 'ナ', 'ニ', 'ネ', 'ノ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ポ', 'ム', 'メ', 'ュ', 'ユ', 'ョ', 'ラ', 'レ', 'ロ', 'ヮ', 'ワ', 'ン', '・', 'ー', 'ヽ', 'ㅅ', 'ㅈ', 'ㅋ', 'ㅏ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ', 'ㅤ', '\\u31ef', '世', '中', '主', '互', '人', '付', '会', '像', '儿', '允', '先', '入', '写', '分', '利', '制', '刹', '力', '努', '動', '午', '卒', '南', '合', '呟', '嘉', '増', '好', '姿', '嫌', '学', '尔', '希', '彡', '影', '彼', '後', '悪', '手', '投', '拶', '挨', '撃', '撮', '文', '映', '時', '曲', '月', '有', '服', '本', '林', '柱', '業', '機', '歌', '歳', '毅', '気', '洲', '洸', '王', '生', '用', '画', '界', '相', '真', '瞬', '知', '社', '稿', '空', '糟', '終', '結', '繋', '者', '花', '菜', '行', '许', '赫', '踊', '込', '通', '那', '間', '限', '風', '魏', 'ꠎ', '가', '간', '갓', '강', '걸', '검', '게', '격', '결', '경', '고', '곡', '과', '구', '국', '규', '그', '근', '금', '기', '김', '꺽', '꼼', '나', '날', '남', '내', '너', '널', '네', '넷', '녀', '년', '노', '논', '누', '는', '늘', '니', '다', '단', '당', '닿', '대', '더', '도', '동', '두', '둑', '듀', '드', '등', '디', '라', '락', '랑', '랙', '랜', '램', '러', '런', '레', '렛', '로', '롱', '료', '루', '룰', '룸', '를', '름', '릉', '리', '림', '링', '마', '맞', '매', '맨', '몬', '무', '미', '민', '밀', '바', '박', '방', '배', '백', '뱀', '버', '벅', '법', '베', '벨', '벳', '보', '복', '본', '봄', '봉', '뷔', '브', '븐', '블', '비', '빅', '빼', '사', '살', '삼', '상', '생', '샤', '샵', '서', '석', '선', '성', '세', '섹', '셔', '션', '셩', '소', '송', '수', '슈', '스', '슨', '슬', '승', '시', '식', '신', '실', '싸', '아', '안', '압', '애', '야', '양', '어', '에', '엑', '엘', '엠', '엣', '여', '역', '연', '영', '예', '오', '온', '와', '왕', '외', '요', '용', '우', '울', '워', '원', '위', '유', '윤', '의', '이', '인', '일', '임', '잘', '장', '재', '잭', '전', '정', '제', '젤', '종', '주', '쥔', '즈', '지', '직', '진', '집', '쩜', '찌', '찰', '채', '천', '철', '초', '최', '추', '출', '츠', '치', '카', '커', '코', '콘', '콤', '쿱', '크', '키', '킹', '타', '탄', '탑', '태', '터', '텐', '토', '톡', '트', '티', '틴', '팁', '파', '패', '펀', '포', '풀', '프', '플', '피', '핑', '하', '한', '해', '핸', '헌', '헤', '헨', '혁', '현', '형', '호', '화', '환', '훈', '힐', 'ﷻ', '︎', '️', '︵', '﹏', '﹪', '！', '＂', '＃', '（', '）', '＊', '．', '３', '６', '７', '８', '？', '＠', 'Ａ', 'Ｂ', 'Ｃ', 'Ｄ', 'Ｅ', 'Ｆ', 'Ｇ', 'Ｈ', 'Ｉ', 'Ｋ', 'Ｌ', 'Ｍ', 'Ｎ', 'Ｏ', 'Ｐ', 'Ｑ', 'Ｒ', 'Ｓ', 'Ｔ', 'Ｕ', 'Ｖ', 'Ｗ', 'Ｙ', '［', '］', '＿', '｀', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｇ', 'ｉ', 'ｌ', 'ｍ', 'ｎ', 'ｏ', 'ｐ', 'ｑ', 'ｒ', 'ｓ', 'ｔ', 'ｕ', 'ｖ', 'ｗ', 'ｘ', 'ｙ', 'ｚ', '｜', '｡', '･', 'ﾉ', 'ﾒ', '￣', '￼', '�', '🃏', '🅰', '🅱', '🅾', '🅿', '🆑', '🆒', '🆓', '🆔', '🆕', '🆖', '🆗', '🆘', '🆙', '🆚', '🇦', '🇧', '🇨', '🇩', '🇪', '🇫', '🇬', '🇭', '🇮', '🇯', '🇰', '🇱', '🇲', '🇳', '🇴', '🇵', '🇷', '🇸', '🇹', '🇺', '🇻', '🇼', '🇽', '🇾', '🇿', '🈴', '🈵', '🈶', '🈷', '🌀', '🌃', '🌄', '🌅', '🌆', '🌇', '🌈', '🌊', '🌋', '🌌', '🌍', '🌎', '🌏', '🌐', '🌒', '🌓', '🌗', '🌙', '🌚', '🌛', '🌜', '🌝', '🌞', '🌟', '🌠', '🌤', '🌥', '🌧', '🌨', '🌪', '🌫', '🌬', '🌭', '🌮', '🌯', '🌰', '🌱', '🌲', '🌳', '🌴', '🌵', '🌶', '🌷', '🌸', '🌹', '🌺', '🌻', '🌼', '🌽', '🌾', '🌿', '🍀', '🍁', '🍂', '🍃', '🍅', '🍆', '🍇', '🍉', '🍊', '🍋', '🍌', '🍍', '🍎', '🍏', '🍑', '🍒', '🍓', '🍔', '🍕', '🍖', '🍗', '🍚', '🍛', '🍜', '🍝', '🍞', '🍟', '🍣', '🍤', '🍥', '🍦', '🍨', '🍩', '🍪', '🍫', '🍬', '🍭', '🍯', '🍰', '🍱', '🍳', '🍴', '🍵', '🍶', '🍷', '🍸', '🍹', '🍺', '🍻', '🍼', '🍽', '🍾', '🍿', '🎀', '🎁', '🎂', '🎅', '🎆', '🎇', '🎈', '🎉', '🎊', '🎋', '🎍', '🎒', '🎓', '🎗', '🎙', '🎞', '🎟', '🎠', '🎡', '🎢', '🎤', '🎥', '🎦', '🎧', '🎨', '🎩', '🎪', '🎫', '🎬', '🎭', '🎮', '🎯', '🎰', '🎱', '🎲', '🎵', '🎶', '🎷', '🎸', '🎹', '🎺', '🎻', '🎼', '🎾', '🎿', '🏀', '🏁', '🏃', '🏄', '🏅', '🏆', '🏇', '🏈', '🏉', '🏊', '🏋', '🏌', '🏒', '🏓', '🏔', '🏖', '🏘', '🏙', '🏚', '🏟', '🏠', '🏡', '🏢', '🏩', '🏫', '🏰', '🏳', '🏴', '🏹', '🏻', '🏼', '🏽', '🏾', '🏿', '🐀', '🐁', '🐂', '🐄', '🐆', '🐇', '🐈', '🐉', '🐊', '🐍', '🐎', '🐐', '🐑', '🐒', '🐓', '🐔', '🐕', '🐖', '🐘', '🐙', '🐚', '🐜', '🐝', '🐞', '🐟', '🐠', '🐡', '🐢', '🐣', '🐥', '🐦', '🐧', '🐨', '🐩', '🐫', '🐬', '🐭', '🐮', '🐯', '🐰', '🐱', '🐲', '🐳', '🐴', '🐶', '🐷', '🐸', '🐹', '🐺', '🐻', '🐼', '🐽', '🐾', '🐿', '👀', '👁', '👂', '👄', '👅', '👆', '👇', '👈', '👉', '👊', '👋', '👌', '👍', '👎', '👏', '👐', '👑', '👓', '👕', '👖', '👗', '👙', '👞', '👟', '👠', '👡', '👣', '👤', '👥', '👦', '👧', '👨', '👩', '👪', '👫', '👭', '👮', '👯', '👰', '👱', '👲', '👳', '👵', '👶', '👷', '👸', '👹', '👺', '👻', '👼', '👽', '👿', '💀', '💁', '💂', '💃', '💄', '💅', '💆', '💈', '💉', '💊', '💋', '💌', '💍', '💎', '💏', '💐', '💑', '💒', '💓', '💔', '💕', '💖', '💗', '💘', '💙', '💚', '💛', '💜', '💝', '💞', '💟', '💡', '💢', '💣', '💤', '💥', '💦', '💧', '💨', '💩', '💪', '💫', '💬', '💭', '💮', '💯', '💰', '💲', '💳', '💵', '💶', '💸', '💻', '💼', '💽', '💿', '📀', '📂', '📅', '📆', '📈', '📊', '📋', '📌', '📍', '📏', '📓', '📖', '📚', '📛', '📝', '📞', '📡', '📢', '📣', '📦', '📧', '📩', '📬', '📯', '📰', '📱', '📲', '📴', '📷', '📸', '📹', '📺', '📻', '📼', '📽', '📿', '🔁', '🔂', '🔃', '🔄', '🔅', '🔉', '🔊', '🔋', '🔌', '🔐', '🔑', '🔒', '🔓', '🔔', '🔘', '🔙', '🔛', '🔜', '🔝', '🔞', '🔥', '🔨', '🔩', '🔪', '🔫', '🔮', '🔰', '🔱', '🔲', '🔴', '🔵', '🔶', '🔸', '🔹', '🔺', '🔻', '🔼', '🔽', '🕊', '🕋', '🕌', '🕎', '🕐', '🕒', '🕘', '🕛', '🕜', '🕟', '🕤', '🕪', '🕯', '🕵', '🕶', '🕷', '🕺', '🖐', '🖒', '🖕', '🖖', '🖤', '🖥', '🖲', '🖼', '🗂', '🗓', '🗝', '🗞', '🗡', '🗣', '🗨', '🗳', '🗻', '🗼', '🗽', '🗾', '😀', '😁', '😂', '😃', '😄', '😅', '😆', '😇', '😈', '😉', '😊', '😋', '😌', '😍', '😎', '😏', '😐', '😑', '😒', '😓', '😔', '😕', '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😦', '😧', '😨', '😩', '😪', '😫', '😬', '😭', '😮', '😯', '😰', '😱', '😲', '😳', '😴', '😵', '😶', '😷', '😸', '😹', '😺', '😻', '😼', '😽', '😿', '🙀', '🙁', '🙂', '🙃', '🙄', '🙅', '🙆', '🙇', '🙈', '🙉', '🙊', '🙋', '🙌', '🙍', '🙎', '🙏', '🚀', '🚁', '🚇', '🚈', '🚌', '🚑', '🚓', '🚔', '🚖', '🚗', '🚘', '🚙', '🚢', '🚣', '🚦', '🚧', '🚨', '🚩', '🚫', '🚬', '🚮', '🚲', '🚴', '🚵', '🚶', '🚻', '🚼', '🚿', '🛀', '🛁', '🛃', '🛄', '🛐', '🛩', '🛫', '🛬', '🛰', '🛳', '🛴', '🤐', '🤑', '🤒', '🤓', '🤔', '🤕', '🤖', '🤗', '🤘', '🤙', '🤚', '🤛', '🤜', '🤝', '🤞', '🤠', '🤡', '🤢', '🤣', '🤤', '🤥', '🤦', '🤧', '🤳', '🤴', '🤷', '🥀', '🥁', '🥂', '🥃', '🥄', '🥅', '🥇', '🥊', '🥐', '🥒', '🥓', '🥔', '🥘', '🥙', '🥞', '🦀', '🦁', '🦃', '🦄', '🦅', '🦇', '🦉', '🦋', '🦑', '\\U000fe4e6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMC_u8eQbVvZ",
        "colab_type": "code",
        "outputId": "ddc1ea82-7a82-4786-b17f-b638965d161d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import nltk.data\n",
        "import nltk.tokenize \n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "def find_ngrams(text, n):\n",
        "    counter = Counter()\n",
        "    ngram = {}\n",
        "\n",
        "    num_tokens, tokens = split_into_tokens(text.lower())\n",
        "    for token in tokens:\n",
        "        sequence  = {}\n",
        "        for i,_ in enumerate(token):\n",
        "            if(i>=n - 1):\n",
        "                word, char = token[i - (n - 1):i], token[i]\n",
        "                nchar_word = ''.join(word) + char\n",
        "                counter[nchar_word] = counter.get(nchar_word, 0) + 1\n",
        "                sequence[char] = counter[nchar_word]\n",
        "                ngram[word] = sequence\n",
        "                #print({word:ngram[word]})\n",
        "    return ngram\n",
        "def token_to_unigram(token):\n",
        "    token = token.strip().strip(\",.!|&-_()[]<>{}/\\\"'\").strip()\n",
        "\n",
        "    def has_no_chars(token):\n",
        "        for char in token:\n",
        "            if char.isalpha():\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(token) == 1 or token.isdigit() or has_no_chars(token):\n",
        "        return None\n",
        "    return token\n",
        "def split_into_tokens(text):\n",
        "    tokens = []\n",
        "    for token in nltk.tokenize.WhitespaceTokenizer().tokenize(text):\n",
        "    #for token in tokenize(text):\n",
        "        unigram = token_to_unigram(token)\n",
        "        if unigram:\n",
        "            tokens.append(unigram)\n",
        "    return len(tokens), tokens\n",
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  # TODO\n",
        "  voc_size = len(vocabulary)\n",
        "  if add_one:\n",
        "      model = defaultdict(lambda: defaultdict(lambda: 1/voc_size))\n",
        "  else:\n",
        "      model = defaultdict(lambda: defaultdict(lambda: 1e-08))\n",
        "  with open(data_file_path, encoding='utf-8', newline='') as csv_file:\n",
        "    csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "    corpus = []\n",
        "    for tweet_text in csv_reader['tweet_text']:\n",
        "            corpus.append(tweet_text)\n",
        "    data = find_ngrams(\" \".join(corpus),n)\n",
        "\n",
        "    for word,chars in data.items():\n",
        "        temp_dict = {}\n",
        "        words_total = float(sum(chars.values()))\n",
        "        for char, count in chars.items():\n",
        "            if (add_one == True):\n",
        "                p = float(count + 1)/(words_total + len(vocabulary))\n",
        "            else:\n",
        "                p = float(count)/words_total\n",
        "            temp_dict[char] = p\n",
        "        model[word] = temp_dict\n",
        "  return model\n",
        "n =3\n",
        "file_name = '/content/drive/My Drive/IDC/NLP/Assignment1/en.csv'\n",
        "model = lm(n, vocabulary,file_name , False)\n",
        "print({k: v for i, (k, v) in enumerate(model.items()) if i < 50})\n",
        "print(sum(v for v in model['abc'].values()))\n",
        "print(model['abc'])\n",
        "model['abc']\n",
        "# print({k: v for i, (k, v) in enumerate(model.items()) if i < 50})\n",
        "# for j, key in enumerate(model.keys()):\n",
        "#     if j < 50:\n",
        "#         print(\"ngram:{:s} {:f}\".format(key,sum(model[key].values())))  "
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'@h': {'i': 0.06206896551724138, 'l': 0.02666666666666667, 'a': 0.09793103448275862, 'r': 0.1103448275862069, 'y': 0.0910344827586207, 'c': 0.027126436781609194, 'n': 0.17471264367816092, 't': 0.2381609195402299, 'o': 0.0993103448275862, ':': 0.07264367816091954}, 'ha': {'u': 0.023801967629324024, 's': 0.18946366232941922, 't': 0.25261821643922566, 'i': 0.18406854966677244, 'n': 0.06569343065693431, '_': 0.00856870834655665, 'h': 0.0050777530942557915, 'a': 0.008251348778165662, 'l': 0.05553792446842272, 'p': 0.013329101872421454, 'e': 0.008886067914947636, 'r': 0.17613456045699777, ':': 0.00856870834655665}, 'an': {'d': 1.0}, 'nn': {'v': 0.025537634408602152, 'e': 0.29973118279569894, 'b': 0.008064516129032258, 'n': 0.14381720430107528, 't': 0.5161290322580645, 'x': 0.004032258064516129, ':': 0.002688172043010753}, 'na': {'n': 0.10649819494584838, 'a': 0.10613718411552346, 'l': 0.5790613718411552, 'y': 0.20830324909747291}, 'ah': {'h': 0.11515151515151516, 'c': 0.19393939393939394, 'a': 0.6909090909090909}, 'hm': {'h': 0.02666666666666667, 'm': 0.03, 'e': 0.08666666666666667, 'd': 0.6633333333333333, 'g': 0.10666666666666667, 'i': 0.02, 'y': 0.016666666666666666, '1': 0.006666666666666667, '8': 0.013333333333333334, '3': 0.006666666666666667, '9': 0.0033333333333333335, ':': 0.02}, 'me': {'m': 0.3762765121759623, 'e': 0.3692065985860173, 's': 0.2545168892380204}, 'em': {'m': 0.43717277486910994, 'o': 0.56282722513089}, 'mo': {'n': 0.21775147928994082, 'e': 0.6550295857988165, 'y': 0.12721893491124261}, 'or': {'o': 0.4260869565217391, 'r': 0.40347826086956523, 'y': 0.17043478260869566}, 'ri': {'r': 0.03614457831325301, 'p': 0.060240963855421686, 'i': 0.3192771084337349, 's': 0.3506024096385542, 'e': 0.23373493975903614}, 'ie': {'l': 0.11983223487118035, 'i': 0.17555422408627921, 'e': 0.6590772917914919, 'v': 0.04553624925104853}, 'es': {'s': 0.6453433678269049, 'a': 0.1317027281279398, 'y': 0.22295390404515522}, 's1': {'a': 0.1023027557568894, 'n': 0.14571536428841073, 'h': 0.02491506228765572, 'm': 0.0604001510003775, 'e': 0.15175537938844846, 'o': 0.08040770101925254, 'r': 0.18610796526991316, 'i': 0.0928652321630804, 's': 0.15024537561343904, '1': 0.0026425066062665155, ':': 0.0026425066062665155}, 'th': {'e': 0.8972718086897945, 'n': 0.10272819131020546}, 'hi': {'i': 0.577304964539007, 't': 0.4226950354609929}, 'wa': {'n': 0.3510747185261003, 't': 0.6489252814738997}, 'no': {'t': 1.0}, 'on': {'o': 0.0069381598793363496, 'n': 0.2015082956259427, 't': 0.11010558069381599, 'c': 0.0030165912518853697, 'a': 0.0036199095022624436, 'r': 0.24615384615384617, 'e': 0.428657616892911}, 'nl': {'l': 0.5450236966824644, 'y': 0.4549763033175355}, 'ca': {'b': 0.020241593209271956, 'a': 0.08684296441397323, 'm': 0.08749591903362716, 'c': 0.07378387202089455, 'r': 0.267058439438459, 'e': 0.4645772118837741}, 'at': {'e': 0.11459129106187929, 'a': 0.01145912910618793, 't': 0.43544690603514136, 'h': 0.19022154316271964, 'p': 0.008403361344537815, 'n': 0.09702062643239114, 'l': 0.053475935828877004, 's': 0.08938120702826585}, 'tc': {'o': 0.0069381598793363496, 'n': 0.2015082956259427, 't': 0.11010558069381599, 'c': 0.0030165912518853697, 'a': 0.0036199095022624436, 'r': 0.24615384615384617, 'e': 0.428657616892911}, 'ch': {'i': 0.168125, 'c': 0.18875, 'h': 0.215625, 'a': 0.004375, 'e': 0.0825, 'l': 0.115, 'k': 0.0125, 'n': 0.168125, 'p': 0.005, ':': 0.04}, 'bu': {'t': 5.679075446517307e-05, 'p': 0.16994633273703041, 's': 0.16864014538433142, ':': 0.16815742397137745, '/': 0.16210920862083653, '.': 0.16423886191328052, 'c': 0.1634437913507681, 'o': 0.00011358150893034614, '9': 0.001959281029048471, 'f': 0.00014197688616293268, '6': 5.679075446517307e-05, '1': 2.8395377232586536e-05, 'b': 2.8395377232586536e-05, 'u': 2.8395377232586536e-05, 'y': 0.0010222335803731152, 'q': 2.8395377232586536e-05}, 'he': {'e': 0.8972718086897945, 'n': 0.10272819131020546}, 'el': {'e': 0.3236514522821577, 'l': 0.5020746887966805, 't': 0.14937759336099585, 'a': 0.024896265560165973}, 'lp': {'u': 0.023801967629324024, 's': 0.18946366232941922, 't': 0.25261821643922566, 'i': 0.18406854966677244, 'n': 0.06569343065693431, '_': 0.00856870834655665, 'h': 0.0050777530942557915, 'a': 0.008251348778165662, 'l': 0.05553792446842272, 'p': 0.013329101872421454, 'e': 0.008886067914947636, 'r': 0.17613456045699777, ':': 0.00856870834655665}, 'pf': {'a': 0.16795366795366795, 'n': 0.16988416988416988, 'p': 0.013513513513513514, 'f': 0.09652509652509653, 'e': 0.16312741312741313, 'i': 0.008687258687258687, 'r': 0.16312741312741313, ':': 0.2171814671814672}, 'fu': {'o': 0.2235494880546075, 'y': 0.10921501706484642, 'f': 0.025597269624573378, 'u': 0.015358361774744027, 'l': 0.33447098976109213, 'c': 0.027303754266211604, 'a': 0.017064846416382253, 't': 0.24744027303754265}, 'sc': {'o': 0.39185750636132316, 'b': 0.09414758269720101, 'y': 0.08142493638676845, 's': 0.02544529262086514, 'c': 0.022900763358778626, 't': 0.3842239185750636}, 'ho': {'w': 1.0}, 'oo': {'o': 0.8347457627118644, 'f': 0.1652542372881356}, 'da': {'a': 0.16795366795366795, 'n': 0.16988416988416988, 'p': 0.013513513513513514, 'f': 0.09652509652509653, 'e': 0.16312741312741313, 'i': 0.008687258687258687, 'r': 0.16312741312741313, ':': 0.2171814671814672}, 'ay': {'o': 0.06235294117647059, 't': 0.7376470588235294, 'q': 0.004705882352941176, 'u': 0.0058823529411764705, 'a': 0.12705882352941175, 'y': 0.0058823529411764705, ':': 0.05647058823529412}, '#1': {'1': 0.014132762312633832, 'y': 0.014132762312633832, 'e': 0.014132762312633832, 'a': 0.11177730192719486, 'r': 0.332762312633833, 's': 0.1297644539614561, 'o': 0.01670235546038544, 'f': 0.03340471092077088, 'h': 0.02740899357601713, 'n': 0.13147751605995717, 'm': 0.021413276231263382, 't': 0.15289079229122055}, '11': {'1': 0.014132762312633832, 'y': 0.014132762312633832, 'e': 0.014132762312633832, 'a': 0.11177730192719486, 'r': 0.332762312633833, 's': 0.1297644539614561, 'o': 0.01670235546038544, 'f': 0.03340471092077088, 'h': 0.02740899357601713, 'n': 0.13147751605995717, 'm': 0.021413276231263382, 't': 0.15289079229122055}, '1y': {'t': 0.14241713112349663, 'p': 0.1461572308594896, 's': 2.4445096313679475e-05, ':': 0.14461718979172777, '/': 0.13943482937322774, '.': 0.14126821159675368, 'c': 0.14058374889997066, 'o': 0.1401681822626381, 'd': 0.004913464359049575, 'j': 0.00012222548156839739, '3': 7.333528894103843e-05, '9': 4.889019262735895e-05, '7': 2.4445096313679475e-05, '1': 4.889019262735895e-05, 'y': 2.4445096313679475e-05, 'v': 4.889019262735895e-05, 'l': 2.4445096313679475e-05}, 'ye': {'a': 0.26958105646630237, 'r': 0.7304189435336976}, 'ea': {'a': 0.45666666666666667, 'l': 0.29033333333333333, 'i': 0.117, 's': 0.071, 'e': 0.065}, 'ar': {'b': 0.020241593209271956, 'a': 0.08684296441397323, 'm': 0.08749591903362716, 'c': 0.07378387202089455, 'r': 0.267058439438459, 'e': 0.4645772118837741}, 'rs': {'r': 0.6571428571428571, 's': 0.15934065934065933, 'e': 0.1835164835164835}, 'so': {'o': 1.0}, 'of': {'f': 1.0}, 'fh': {'t': 0.1414158855451406, 'p': 0.14514060187469166, 's': 0.1440059200789344, ':': 0.1435865811544154, '/': 0.13848051307350764, '.': 0.14028120374938333, 'c': 0.13963986186482485, 'o': 0.0014553527380365072, 'm': 0.004366058214109522, 'x': 0.000123334977799704, 'd': 4.93339911198816e-05, 'n': 2.46669955599408e-05, 'f': 0.0002220029600394672, 'h': 0.0010113468179575728, 'g': 2.46669955599408e-05, '9': 9.86679822397632e-05, 'v': 7.40009866798224e-05}, 'nt': {'a': 0.20859407592824364, 'n': 0.15310805173133082, 't': 0.2649144764288694, 'e': 0.17271589486858574, 'd': 0.20066750104297038}, 'ta': {'k': 0.25396825396825395, 'e': 0.746031746031746}, 'ht': {'t': 0.16686688404553515, 'p': 2.860248269549797e-05, 's': 0.16995595217664894, ':': 0.16946970997082547, '/': 0.1633773811566844, '.': 0.16552256735884674, 'c': 2.860248269549797e-05, 'o': 0.16426405812024483, '5': 2.860248269549797e-05, '1': 2.860248269549797e-05, 'r': 8.580744808649391e-05, 'b': 2.860248269549797e-05, 'h': 5.720496539099594e-05, 'n': 0.00011440993078199188, 'g': 0.00014301241347748986}, 'tt': {'t': 0.16686688404553515, 'p': 2.860248269549797e-05, 's': 0.16995595217664894, ':': 0.16946970997082547, '/': 0.1633773811566844, '.': 0.16552256735884674, 'c': 2.860248269549797e-05, 'o': 0.16426405812024483, '5': 2.860248269549797e-05, '1': 2.860248269549797e-05, 'r': 8.580744808649391e-05, 'b': 2.860248269549797e-05, 'h': 5.720496539099594e-05, 'n': 0.00011440993078199188, 'g': 0.00014301241347748986}}\n",
            "0\n",
            "defaultdict(<function lm.<locals>.<lambda>.<locals>.<lambda> at 0x7eff4cf58f28>, {})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.lm.<locals>.<lambda>.<locals>.<lambda>>, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3",
        "colab_type": "text"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kkMn328-lJ",
        "colab_type": "code",
        "outputId": "7758f2bf-e436-42cb-fdbe-0caf202c3f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "def compute_entropy_xi(ngram):\n",
        "    ll = 0\n",
        "    for key in ngram.items():\n",
        "        p = key[1]\n",
        "        ll += p*np.log2(p)\n",
        "    return ll\n",
        "\n",
        "def eval(n, model, data_file):\n",
        "    \"\"\"\n",
        "    Compute the perplexity of ngrams from model\n",
        "    \"\"\"\n",
        "    h_x = 0\n",
        "    N = 0\n",
        "    with open(data_file, encoding='utf-8', newline='') as csv_file:\n",
        "        csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "        corpus = []\n",
        "        for tweet_text in csv_reader['tweet_text']:\n",
        "                corpus.append(tweet_text)\n",
        "        data = find_ngrams(\" \".join(corpus),n)\n",
        "        N = len(model)   # Total number of words\n",
        "        for word,chars in data.items():\n",
        "            if word in model:\n",
        "                ngram = model[word]\n",
        "                #for i, ngram in enumerate(model.items()):\n",
        "                h_x += compute_entropy_xi(ngram)\n",
        "            #N += len(h_X)\n",
        "    return pow(2, (-1.0 / N) * h_x)\n",
        "\n",
        "n =3\n",
        "file_name = '/content/drive/My Drive/IDC/NLP/Assignment1/en.csv'\n",
        "model = lm(n, vocabulary,file_name , False)\n",
        "test_file_name = '/content/drive/My Drive/IDC/NLP/Assignment1/fr.csv'\n",
        "preplexity = eval(n,model,test_file_name)\n",
        "print(\"Preplexity evaluation : {:f}\".format(preplexity))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preplexity evaluation : 2.589641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p",
        "colab_type": "text"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAxLE9s_fvn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "94fd9c5f-2027-4a76-8218-f94e17668c0e"
      },
      "source": [
        "import os\n",
        "# def match(n, add_one):\n",
        "#   # n - the n-gram to use for creating n-gram models\n",
        "#   # add_one - use add_one smoothing or not\n",
        "\n",
        "#   #TODO\n",
        "#   file_path = '/content/drive/My Drive/IDC/NLP/Assignment1/{}.csv'\n",
        "#   vocabulary = preprocess()\n",
        "\n",
        "#   lang = ['en' ,'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "#   df = pd.DataFrame(columns=lang, index=lang)\n",
        "#   for l1 in lang:\n",
        "#     l1_model = lm(n, vocabulary, file_path.format(l1), add_one)\n",
        "#     for l2 in lang:\n",
        "#       df.at[l1, l2] = eval(n, l1_model, file_path.format(l2))\n",
        "\n",
        "#   return df\n",
        "def match_ronen(n, add_one):\n",
        " path_name = '/content/drive/My Drive/IDC/NLP/Assignment1/'\n",
        " csvglob = path_name + '*.csv'\n",
        " pathlist = glob(csvglob) #'C:\\\\Users\\\\ronen\\\\Documents\\\\IDC\\Year-3\\\\Spring 2020\\\\NLP\\\\HW\\\\Assignment1\\\\*.csv')\n",
        " num = len(pathlist)\n",
        " matrix = np.empty(shape=(num,num),dtype='float')\n",
        " headers_list = []\n",
        " i = 0\n",
        " for model_path in pathlist:\n",
        "    head, tail = os.path.split(model_path)\n",
        "    headers_list.append(tail)\n",
        "    j = 0\n",
        "    modeli = lm(n, vocabulary,model_path, add_one)\n",
        "    for data_file_path in pathlist:\n",
        "        #if ( i != j ):\n",
        "        pp = eval(n,modeli,data_file_path)\n",
        "        matrix[i][j] = pp\n",
        "        print (\"Preplexity of model {:d} with data file {:d} is {:f}\".format(i,j,pp))\n",
        "        j = j+1\n",
        "    i = i + 1\n",
        " print(matrix)\n",
        " dataframe = pd.DataFrame(matrix,columns=headers_list)\n",
        " print(\"passed dataframe\")\n",
        " return dataframe\n",
        "\n",
        "print (match(2, False))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  3.68839  1.81316  1.69499  1.83623  1.72887  1.85566  1.70515  1.81775\n",
            "es  2.05199  3.46853  1.83994  1.89696  1.92877  1.77403  1.90422  1.95694\n",
            "fr  1.78315  1.68288  3.72805  1.90707  1.64385  1.75699  1.59359  1.87638\n",
            "in  1.77608  1.59407  1.68353  3.46007  1.54961  1.73357  1.57384  1.76976\n",
            "it  2.24157  2.23715  2.03807   2.1087  3.71447  2.02468   1.8361  2.21411\n",
            "nl  2.14532  1.71133  1.91533  2.06477   1.7902  3.91663  1.77082  2.15922\n",
            "pt  2.07748  2.02448   1.7561  1.93376  1.71055  1.88219  3.77545  1.91356\n",
            "tl  1.75608  1.69882  1.75978  1.85108  1.62271  1.77274  1.58028  3.52628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17",
        "colab_type": "text"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk32naXyAMdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "max_n = 4 \n",
        "for n in range(4): \n",
        "  print(\"n = {:d} add_one:{:b}\".format(n+1,False))\n",
        "  data1 = match(n+1,False) \n",
        "  print(data1) \n",
        "  print(\"n = {:d} add_one:{:b}\".format(n+1,True))\n",
        "  data2 = match(n+1,True) \n",
        "  print(data2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW",
        "colab_type": "text"
      },
      "source": [
        "# **Good luck!**\n",
        "max_n = 4\n",
        "for n = 1 in range(max_n):\n",
        "  data = match(n,False)\n",
        "  print(data)\n",
        "  data = match(n,True)\n",
        "  print(data)\n",
        "  "
      ]
    }
  ]
}