{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ronen_Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronenbendavid/IDC_NLP/blob/master/Ronen_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM",
        "colab_type": "text"
      },
      "source": [
        "*As a preparation for this task, place the data files somewhere in your drive so that you can access the files from this notebook. The files are available to download from the Moodle assignment activity*\n",
        "\n",
        "The relevant files are:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGtqPrCPnLUn",
        "colab_type": "code",
        "outputId": "4a302cfb-2392-4607-ecd9-642eea3efeba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQL54PFxXYqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfzsITW8Yaj",
        "colab_type": "code",
        "outputId": "7911ce99-07f9-4735-8cae-72edce492c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "def preprocess():\n",
        "  # TODO\n",
        " vocabulary = set()\n",
        " pathlist = glob('/content/drive/My Drive/IDC/NLP/Assignment1/*.csv')\n",
        " for path in pathlist:\n",
        "    with open(path, encoding='utf-8', newline='') as csv_file:\n",
        "        csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "        for text in csv_reader['tweet_text']:\n",
        "            list_text = list(text)\n",
        "            vocabulary.update(list_text)\n",
        "            line_count += 1\n",
        "        #print(line_count)\n",
        " return sorted(list(vocabulary))\n",
        "\n",
        "vocabulary = preprocess()\n",
        "print(len(vocabulary))\n",
        "print(vocabulary)\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1859\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x97', '\\x9d', 'Â¡', 'Â£', 'Â¤', 'Â¥', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', '\\xad', 'Â®', 'Â¯', 'Â°', 'Â²', 'Â³', 'Â´', 'Â¶', 'Â·', 'Â¸', 'Âº', 'Â»', 'Â½', 'Â¾', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã…', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'ÃŽ', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã™', 'Ãš', 'Ãœ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ä—', 'Äž', 'ÄŸ', 'Ä°', 'Ä±', 'Å„', 'Å', 'Å’', 'Å“', 'Åž', 'ÅŸ', 'Å ', 'Å¸', 'Æ’', 'Ê”', 'Ê•', 'Ê–', 'Ê°', 'Ê³', 'Ê·', 'Ê¸', 'Ë', 'Ë–', 'Ë˜', 'Ëš', 'Ë›', 'Ë¡', 'Ë¢', 'Ì€', 'Ì', 'Ìƒ', 'Ìˆ', 'Ì¥', 'Ì®', 'Ì¯', 'Íœ', 'Í¡', 'Î”', 'Î˜', 'Î©', 'Ï…', 'Ï‰', 'Ð', 'Ð˜', 'Ðœ', 'Ð', 'Ðž', 'ÐŸ', 'Ð ', 'Ð¤', 'Ð¦', 'Ð¯', 'Ð°', 'Ð±', 'Ð²', 'Ð³', 'Ð´', 'Ðµ', 'Ð·', 'Ð¸', 'Ðº', 'Ð»', 'Ð¼', 'Ð½', 'Ð¾', 'Ð¿', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ…', 'Ñ‹', 'Ñ', 'ÑŽ', 'Ñ', 'Ò’', 'Ò¯', 'ØŒ', 'Ø¢', 'Ø¦', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Ø¶', 'Ø·', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'ÛŒ', 'Û¶', 'à¤‚', 'à¤•', 'à¤—', 'à¤ª', 'à¤¬', 'à¤°', 'à¤¸', 'à¤¾', 'à¥‡', 'à¥', 'à³ƒ', 'à·´', 'à¸', 'à¸‚', 'à¸‡', 'à¸ˆ', 'à¸', 'à¸”', 'à¸•', 'à¸–', 'à¸—', 'à¸™', 'à¸š', 'à¸›', 'à¸ž', 'à¸ ', 'à¸¡', 'à¸¢', 'à¸£', 'à¸¥', 'à¸§', 'à¸¨', 'à¸ª', 'à¸­', 'à¸°', 'à¸±', 'à¸²', 'à¸´', 'à¸µ', 'à¸¸', 'à¸¹', 'à¹€', 'à¹', 'à¹ˆ', 'à¹‰', 'à¹', 'à¹‘', 'àº¶', 'à¼Ž', 'à¼º', 'à¼»', 'à¼¼', 'à¼½', 'áƒ¦', 'á™“', 'á´—', 'á´¬', 'á´°', 'áµƒ', 'áµ‡', 'áµˆ', 'áµ‰', 'áµ', 'áµ', 'áµ’', 'áµ–', 'áµ—', 'áµ˜', 'áµ›', 'á¶œ', 'á¶ ', 'á¶¦', 'á¶°', '\\u2009', '\\u200a', '\\u200b', '\\u200d', 'â€“', 'â€”', 'â€•', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€ž', 'â€ ', 'â€¢', 'â€¤', 'â€¦', 'â€°', 'â€²', 'â€¹', 'â€º', 'â€»', 'â€¼', 'â€¿', 'â‰', '\\u2066', '\\u2067', '\\u2069', 'â±', 'â·', 'â¿', 'â‚¬', 'â‚¹', 'âƒ£', 'â„ƒ', 'â„…', 'â„¢', 'â… ', 'â…¡', 'â…¢', 'â…£', 'â…¤', 'â†', 'â†‘', 'â†’', 'â†“', 'â†”', 'â†•', 'â†—', 'â†˜', 'â†š', 'â†›', 'â†©', 'â†ª', 'â†¯', 'â†º', 'â‡˜', 'â‡¨', 'âˆ€', 'âˆ†', 'âˆ‡', 'âˆš', 'âˆž', 'âˆ´', 'âˆµ', 'â‰¤', 'â‰¥', 'â‰¦', 'â‰§', 'âŠ™', 'â‹…', 'â‹ª', 'â‹­', 'âŒš', 'âŒ›', 'âŒ£', 'âŽ‹', 'â©', 'â°', 'â±', 'â³', 'â¸', 'â‘ ', 'â‘¥', 'â’ˆ', 'â’‰', 'â’Š', 'â’‹', 'â’Œ', 'â’', 'â’Ž', 'â’', 'â’', 'â’‘', 'â“‚', 'â“˜', 'â“™', 'â“¢', 'â“¦', 'â”€', 'â”', 'â”ƒ', 'â”„', 'â”†', 'â”Œ', 'â”', 'â”', 'â”“', 'â””', 'â”—', 'â”˜', 'â”›', 'â”³', 'â”»', 'â•‘', 'â•”', 'â•—', 'â•š', 'â•', 'â•¦', 'â•©', 'â•¬', 'â•­', 'â•®', 'â•¯', 'â•°', 'â•±', 'â•²', 'â•´', 'â–ˆ', 'â–Š', 'â–', 'â–’', 'â–”', 'â–•', 'â–™', 'â–', 'â–£', 'â–¦', 'â–ª', 'â–²', 'â–³', 'â–¶', 'â–¸', 'â–º', 'â–¼', 'â–½', 'â–¿', 'â—€', 'â—', 'â—„', 'â—†', 'â—‡', 'â—ˆ', 'â—‹', 'â—Ž', 'â—', 'â—‘', 'â—•', 'â—¡', 'â—»', 'â—¼', 'â—½', 'â—¾', 'â˜€', 'â˜', 'â˜ƒ', 'â˜„', 'â˜…', 'â˜†', 'â˜‰', 'â˜Ž', 'â˜‘', 'â˜“', 'â˜”', 'â˜•', 'â˜˜', 'â˜™', 'â˜š', 'â˜›', 'â˜œ', 'â˜', 'â˜ž', 'â˜ ', 'â˜£', 'â˜ª', 'â˜®', 'â˜¯', 'â˜°', 'â˜¹', 'â˜º', 'â˜¼', 'â˜½', 'â˜¾', 'â™€', 'â™‚', 'â™Š', 'â™‹', 'â™', 'â™Ž', 'â™', 'â™', 'â™“', 'â™›', 'â™¡', 'â™£', 'â™¤', 'â™¥', 'â™¦', 'â™©', 'â™ª', 'â™«', 'â™¬', 'â™¯', 'â™»', 'âš’', 'âš“', 'âš”', 'âš•', 'âš–', 'âš˜', 'âšœ', 'âš', 'âš ', 'âš¡', 'âšª', 'âš«', 'âš°', 'âš½', 'âš¾', 'â›„', 'â›…', 'â›ˆ', 'â›“', 'â›”', 'â›©', 'â›ª', 'â›³', 'â›·', 'â›½', 'âœ', 'âœ‚', 'âœƒ', 'âœ…', 'âœˆ', 'âœ‰', 'âœŠ', 'âœ‹', 'âœŒ', 'âœ', 'âœ', 'âœ“', 'âœ”', 'âœ–', 'âœ', 'âœ¡', 'âœ§', 'âœ¨', 'âœ©', 'âœª', 'âœ­', 'âœ°', 'âœ³', 'âœ´', 'âœµ', 'âœ¶', 'âœ·', 'âœ¿', 'â€', 'â', 'â„', 'â…', 'âˆ', 'â‹', 'âŒ', 'âŽ', 'â“', 'â”', 'â—', 'â', 'âž', 'â£', 'â¤', 'â¥', 'âžŠ', 'âž‹', 'âžŒ', 'âž', 'âžŽ', 'âž', 'âž”', 'âž–', 'âž—', 'âž™', 'âž›', 'âžœ', 'âžž', 'âžŸ', 'âž ', 'âž¡', 'âž¢', 'âž¤', 'âž°', 'âžµ', 'â €', 'â¤µ', 'â¦‘', 'â¦’', 'â¬…', 'â¬‡', 'â­', 'â¸„', 'â¸…', '\\u3000', 'ã€', 'ã€‚', 'ã€†', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€', 'ã€Ž', 'ã€', 'ã€', 'ã€‘', 'ã€œ', 'ã€¡', 'ã€°', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ãŒ', 'ã', 'ã', 'ã—', 'ã›', 'ãœ', 'ãŸ', 'ã£', 'ã¥', 'ã¦', 'ã§', 'ã¨', 'ãª', 'ã«', 'ã­', 'ã®', 'ã¯', 'ã²', 'ã¿', 'ã‚€', 'ã‚‡', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚Œ', 'ã‚', 'ã‚’', 'ã‚œ', 'ã‚¤', 'ã‚§', 'ã‚¨', 'ã‚©', 'ã‚ª', 'ã‚«', 'ã‚­', 'ã‚¯', 'ã‚°', 'ã‚³', 'ã‚´', 'ã‚µ', 'ã‚¸', 'ã‚¹', 'ã‚»', 'ã‚¿', 'ãƒ', 'ãƒƒ', 'ãƒ„', 'ãƒ†', 'ãƒˆ', 'ãƒ‰', 'ãƒŠ', 'ãƒ‹', 'ãƒ', 'ãƒŽ', 'ãƒ‘', 'ãƒ’', 'ãƒ“', 'ãƒ”', 'ãƒ•', 'ãƒ–', 'ãƒ—', 'ãƒ', 'ãƒ ', 'ãƒ¡', 'ãƒ¥', 'ãƒ¦', 'ãƒ§', 'ãƒ©', 'ãƒ¬', 'ãƒ­', 'ãƒ®', 'ãƒ¯', 'ãƒ³', 'ãƒ»', 'ãƒ¼', 'ãƒ½', 'ã……', 'ã…ˆ', 'ã…‹', 'ã…', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£', 'ã…¤', '\\u31ef', 'ä¸–', 'ä¸­', 'ä¸»', 'äº’', 'äºº', 'ä»˜', 'ä¼š', 'åƒ', 'å„¿', 'å…', 'å…ˆ', 'å…¥', 'å†™', 'åˆ†', 'åˆ©', 'åˆ¶', 'åˆ¹', 'åŠ›', 'åŠª', 'å‹•', 'åˆ', 'å’', 'å—', 'åˆ', 'å‘Ÿ', 'å˜‰', 'å¢—', 'å¥½', 'å§¿', 'å«Œ', 'å­¦', 'å°”', 'å¸Œ', 'å½¡', 'å½±', 'å½¼', 'å¾Œ', 'æ‚ª', 'æ‰‹', 'æŠ•', 'æ‹¶', 'æŒ¨', 'æ’ƒ', 'æ’®', 'æ–‡', 'æ˜ ', 'æ™‚', 'æ›²', 'æœˆ', 'æœ‰', 'æœ', 'æœ¬', 'æž—', 'æŸ±', 'æ¥­', 'æ©Ÿ', 'æ­Œ', 'æ­³', 'æ¯…', 'æ°—', 'æ´²', 'æ´¸', 'çŽ‹', 'ç”Ÿ', 'ç”¨', 'ç”»', 'ç•Œ', 'ç›¸', 'çœŸ', 'çž¬', 'çŸ¥', 'ç¤¾', 'ç¨¿', 'ç©º', 'ç³Ÿ', 'çµ‚', 'çµ', 'ç¹‹', 'è€…', 'èŠ±', 'èœ', 'è¡Œ', 'è®¸', 'èµ«', 'è¸Š', 'è¾¼', 'é€š', 'é‚£', 'é–“', 'é™', 'é¢¨', 'é­', 'ê Ž', 'ê°€', 'ê°„', 'ê°“', 'ê°•', 'ê±¸', 'ê²€', 'ê²Œ', 'ê²©', 'ê²°', 'ê²½', 'ê³ ', 'ê³¡', 'ê³¼', 'êµ¬', 'êµ­', 'ê·œ', 'ê·¸', 'ê·¼', 'ê¸ˆ', 'ê¸°', 'ê¹€', 'êº½', 'ê¼¼', 'ë‚˜', 'ë‚ ', 'ë‚¨', 'ë‚´', 'ë„ˆ', 'ë„', 'ë„¤', 'ë„·', 'ë…€', 'ë…„', 'ë…¸', 'ë…¼', 'ëˆ„', 'ëŠ”', 'ëŠ˜', 'ë‹ˆ', 'ë‹¤', 'ë‹¨', 'ë‹¹', 'ë‹¿', 'ëŒ€', 'ë”', 'ë„', 'ë™', 'ë‘', 'ë‘‘', 'ë“€', 'ë“œ', 'ë“±', 'ë””', 'ë¼', 'ë½', 'ëž‘', 'ëž™', 'ëžœ', 'ëž¨', 'ëŸ¬', 'ëŸ°', 'ë ˆ', 'ë ›', 'ë¡œ', 'ë¡±', 'ë£Œ', 'ë£¨', 'ë£°', 'ë£¸', 'ë¥¼', 'ë¦„', 'ë¦‰', 'ë¦¬', 'ë¦¼', 'ë§', 'ë§ˆ', 'ë§ž', 'ë§¤', 'ë§¨', 'ëª¬', 'ë¬´', 'ë¯¸', 'ë¯¼', 'ë°€', 'ë°”', 'ë°•', 'ë°©', 'ë°°', 'ë°±', 'ë±€', 'ë²„', 'ë²…', 'ë²•', 'ë² ', 'ë²¨', 'ë²³', 'ë³´', 'ë³µ', 'ë³¸', 'ë´„', 'ë´‰', 'ë·”', 'ë¸Œ', 'ë¸', 'ë¸”', 'ë¹„', 'ë¹…', 'ë¹¼', 'ì‚¬', 'ì‚´', 'ì‚¼', 'ìƒ', 'ìƒ', 'ìƒ¤', 'ìƒµ', 'ì„œ', 'ì„', 'ì„ ', 'ì„±', 'ì„¸', 'ì„¹', 'ì…”', 'ì…˜', 'ì…©', 'ì†Œ', 'ì†¡', 'ìˆ˜', 'ìŠˆ', 'ìŠ¤', 'ìŠ¨', 'ìŠ¬', 'ìŠ¹', 'ì‹œ', 'ì‹', 'ì‹ ', 'ì‹¤', 'ì‹¸', 'ì•„', 'ì•ˆ', 'ì••', 'ì• ', 'ì•¼', 'ì–‘', 'ì–´', 'ì—', 'ì—‘', 'ì—˜', 'ì— ', 'ì—£', 'ì—¬', 'ì—­', 'ì—°', 'ì˜', 'ì˜ˆ', 'ì˜¤', 'ì˜¨', 'ì™€', 'ì™•', 'ì™¸', 'ìš”', 'ìš©', 'ìš°', 'ìš¸', 'ì›Œ', 'ì›', 'ìœ„', 'ìœ ', 'ìœ¤', 'ì˜', 'ì´', 'ì¸', 'ì¼', 'ìž„', 'ìž˜', 'ìž¥', 'ìž¬', 'ìž­', 'ì „', 'ì •', 'ì œ', 'ì ¤', 'ì¢…', 'ì£¼', 'ì¥”', 'ì¦ˆ', 'ì§€', 'ì§', 'ì§„', 'ì§‘', 'ì©œ', 'ì°Œ', 'ì°°', 'ì±„', 'ì²œ', 'ì² ', 'ì´ˆ', 'ìµœ', 'ì¶”', 'ì¶œ', 'ì¸ ', 'ì¹˜', 'ì¹´', 'ì»¤', 'ì½”', 'ì½˜', 'ì½¤', 'ì¿±', 'í¬', 'í‚¤', 'í‚¹', 'íƒ€', 'íƒ„', 'íƒ‘', 'íƒœ', 'í„°', 'í…', 'í† ', 'í†¡', 'íŠ¸', 'í‹°', 'í‹´', 'íŒ', 'íŒŒ', 'íŒ¨', 'íŽ€', 'í¬', 'í’€', 'í”„', 'í”Œ', 'í”¼', 'í•‘', 'í•˜', 'í•œ', 'í•´', 'í•¸', 'í—Œ', 'í—¤', 'í—¨', 'í˜', 'í˜„', 'í˜•', 'í˜¸', 'í™”', 'í™˜', 'í›ˆ', 'íž', 'ï·»', 'ï¸Ž', 'ï¸', 'ï¸µ', 'ï¹', 'ï¹ª', 'ï¼', 'ï¼‚', 'ï¼ƒ', 'ï¼ˆ', 'ï¼‰', 'ï¼Š', 'ï¼Ž', 'ï¼“', 'ï¼–', 'ï¼—', 'ï¼˜', 'ï¼Ÿ', 'ï¼ ', 'ï¼¡', 'ï¼¢', 'ï¼£', 'ï¼¤', 'ï¼¥', 'ï¼¦', 'ï¼§', 'ï¼¨', 'ï¼©', 'ï¼«', 'ï¼¬', 'ï¼­', 'ï¼®', 'ï¼¯', 'ï¼°', 'ï¼±', 'ï¼²', 'ï¼³', 'ï¼´', 'ï¼µ', 'ï¼¶', 'ï¼·', 'ï¼¹', 'ï¼»', 'ï¼½', 'ï¼¿', 'ï½€', 'ï½', 'ï½‚', 'ï½ƒ', 'ï½„', 'ï½…', 'ï½‡', 'ï½‰', 'ï½Œ', 'ï½', 'ï½Ž', 'ï½', 'ï½', 'ï½‘', 'ï½’', 'ï½“', 'ï½”', 'ï½•', 'ï½–', 'ï½—', 'ï½˜', 'ï½™', 'ï½š', 'ï½œ', 'ï½¡', 'ï½¥', 'ï¾‰', 'ï¾’', 'ï¿£', 'ï¿¼', 'ï¿½', 'ðŸƒ', 'ðŸ…°', 'ðŸ…±', 'ðŸ…¾', 'ðŸ…¿', 'ðŸ†‘', 'ðŸ†’', 'ðŸ†“', 'ðŸ†”', 'ðŸ†•', 'ðŸ†–', 'ðŸ†—', 'ðŸ†˜', 'ðŸ†™', 'ðŸ†š', 'ðŸ‡¦', 'ðŸ‡§', 'ðŸ‡¨', 'ðŸ‡©', 'ðŸ‡ª', 'ðŸ‡«', 'ðŸ‡¬', 'ðŸ‡­', 'ðŸ‡®', 'ðŸ‡¯', 'ðŸ‡°', 'ðŸ‡±', 'ðŸ‡²', 'ðŸ‡³', 'ðŸ‡´', 'ðŸ‡µ', 'ðŸ‡·', 'ðŸ‡¸', 'ðŸ‡¹', 'ðŸ‡º', 'ðŸ‡»', 'ðŸ‡¼', 'ðŸ‡½', 'ðŸ‡¾', 'ðŸ‡¿', 'ðŸˆ´', 'ðŸˆµ', 'ðŸˆ¶', 'ðŸˆ·', 'ðŸŒ€', 'ðŸŒƒ', 'ðŸŒ„', 'ðŸŒ…', 'ðŸŒ†', 'ðŸŒ‡', 'ðŸŒˆ', 'ðŸŒŠ', 'ðŸŒ‹', 'ðŸŒŒ', 'ðŸŒ', 'ðŸŒŽ', 'ðŸŒ', 'ðŸŒ', 'ðŸŒ’', 'ðŸŒ“', 'ðŸŒ—', 'ðŸŒ™', 'ðŸŒš', 'ðŸŒ›', 'ðŸŒœ', 'ðŸŒ', 'ðŸŒž', 'ðŸŒŸ', 'ðŸŒ ', 'ðŸŒ¤', 'ðŸŒ¥', 'ðŸŒ§', 'ðŸŒ¨', 'ðŸŒª', 'ðŸŒ«', 'ðŸŒ¬', 'ðŸŒ­', 'ðŸŒ®', 'ðŸŒ¯', 'ðŸŒ°', 'ðŸŒ±', 'ðŸŒ²', 'ðŸŒ³', 'ðŸŒ´', 'ðŸŒµ', 'ðŸŒ¶', 'ðŸŒ·', 'ðŸŒ¸', 'ðŸŒ¹', 'ðŸŒº', 'ðŸŒ»', 'ðŸŒ¼', 'ðŸŒ½', 'ðŸŒ¾', 'ðŸŒ¿', 'ðŸ€', 'ðŸ', 'ðŸ‚', 'ðŸƒ', 'ðŸ…', 'ðŸ†', 'ðŸ‡', 'ðŸ‰', 'ðŸŠ', 'ðŸ‹', 'ðŸŒ', 'ðŸ', 'ðŸŽ', 'ðŸ', 'ðŸ‘', 'ðŸ’', 'ðŸ“', 'ðŸ”', 'ðŸ•', 'ðŸ–', 'ðŸ—', 'ðŸš', 'ðŸ›', 'ðŸœ', 'ðŸ', 'ðŸž', 'ðŸŸ', 'ðŸ£', 'ðŸ¤', 'ðŸ¥', 'ðŸ¦', 'ðŸ¨', 'ðŸ©', 'ðŸª', 'ðŸ«', 'ðŸ¬', 'ðŸ­', 'ðŸ¯', 'ðŸ°', 'ðŸ±', 'ðŸ³', 'ðŸ´', 'ðŸµ', 'ðŸ¶', 'ðŸ·', 'ðŸ¸', 'ðŸ¹', 'ðŸº', 'ðŸ»', 'ðŸ¼', 'ðŸ½', 'ðŸ¾', 'ðŸ¿', 'ðŸŽ€', 'ðŸŽ', 'ðŸŽ‚', 'ðŸŽ…', 'ðŸŽ†', 'ðŸŽ‡', 'ðŸŽˆ', 'ðŸŽ‰', 'ðŸŽŠ', 'ðŸŽ‹', 'ðŸŽ', 'ðŸŽ’', 'ðŸŽ“', 'ðŸŽ—', 'ðŸŽ™', 'ðŸŽž', 'ðŸŽŸ', 'ðŸŽ ', 'ðŸŽ¡', 'ðŸŽ¢', 'ðŸŽ¤', 'ðŸŽ¥', 'ðŸŽ¦', 'ðŸŽ§', 'ðŸŽ¨', 'ðŸŽ©', 'ðŸŽª', 'ðŸŽ«', 'ðŸŽ¬', 'ðŸŽ­', 'ðŸŽ®', 'ðŸŽ¯', 'ðŸŽ°', 'ðŸŽ±', 'ðŸŽ²', 'ðŸŽµ', 'ðŸŽ¶', 'ðŸŽ·', 'ðŸŽ¸', 'ðŸŽ¹', 'ðŸŽº', 'ðŸŽ»', 'ðŸŽ¼', 'ðŸŽ¾', 'ðŸŽ¿', 'ðŸ€', 'ðŸ', 'ðŸƒ', 'ðŸ„', 'ðŸ…', 'ðŸ†', 'ðŸ‡', 'ðŸˆ', 'ðŸ‰', 'ðŸŠ', 'ðŸ‹', 'ðŸŒ', 'ðŸ’', 'ðŸ“', 'ðŸ”', 'ðŸ–', 'ðŸ˜', 'ðŸ™', 'ðŸš', 'ðŸŸ', 'ðŸ ', 'ðŸ¡', 'ðŸ¢', 'ðŸ©', 'ðŸ«', 'ðŸ°', 'ðŸ³', 'ðŸ´', 'ðŸ¹', 'ðŸ»', 'ðŸ¼', 'ðŸ½', 'ðŸ¾', 'ðŸ¿', 'ðŸ€', 'ðŸ', 'ðŸ‚', 'ðŸ„', 'ðŸ†', 'ðŸ‡', 'ðŸˆ', 'ðŸ‰', 'ðŸŠ', 'ðŸ', 'ðŸŽ', 'ðŸ', 'ðŸ‘', 'ðŸ’', 'ðŸ“', 'ðŸ”', 'ðŸ•', 'ðŸ–', 'ðŸ˜', 'ðŸ™', 'ðŸš', 'ðŸœ', 'ðŸ', 'ðŸž', 'ðŸŸ', 'ðŸ ', 'ðŸ¡', 'ðŸ¢', 'ðŸ£', 'ðŸ¥', 'ðŸ¦', 'ðŸ§', 'ðŸ¨', 'ðŸ©', 'ðŸ«', 'ðŸ¬', 'ðŸ­', 'ðŸ®', 'ðŸ¯', 'ðŸ°', 'ðŸ±', 'ðŸ²', 'ðŸ³', 'ðŸ´', 'ðŸ¶', 'ðŸ·', 'ðŸ¸', 'ðŸ¹', 'ðŸº', 'ðŸ»', 'ðŸ¼', 'ðŸ½', 'ðŸ¾', 'ðŸ¿', 'ðŸ‘€', 'ðŸ‘', 'ðŸ‘‚', 'ðŸ‘„', 'ðŸ‘…', 'ðŸ‘†', 'ðŸ‘‡', 'ðŸ‘ˆ', 'ðŸ‘‰', 'ðŸ‘Š', 'ðŸ‘‹', 'ðŸ‘Œ', 'ðŸ‘', 'ðŸ‘Ž', 'ðŸ‘', 'ðŸ‘', 'ðŸ‘‘', 'ðŸ‘“', 'ðŸ‘•', 'ðŸ‘–', 'ðŸ‘—', 'ðŸ‘™', 'ðŸ‘ž', 'ðŸ‘Ÿ', 'ðŸ‘ ', 'ðŸ‘¡', 'ðŸ‘£', 'ðŸ‘¤', 'ðŸ‘¥', 'ðŸ‘¦', 'ðŸ‘§', 'ðŸ‘¨', 'ðŸ‘©', 'ðŸ‘ª', 'ðŸ‘«', 'ðŸ‘­', 'ðŸ‘®', 'ðŸ‘¯', 'ðŸ‘°', 'ðŸ‘±', 'ðŸ‘²', 'ðŸ‘³', 'ðŸ‘µ', 'ðŸ‘¶', 'ðŸ‘·', 'ðŸ‘¸', 'ðŸ‘¹', 'ðŸ‘º', 'ðŸ‘»', 'ðŸ‘¼', 'ðŸ‘½', 'ðŸ‘¿', 'ðŸ’€', 'ðŸ’', 'ðŸ’‚', 'ðŸ’ƒ', 'ðŸ’„', 'ðŸ’…', 'ðŸ’†', 'ðŸ’ˆ', 'ðŸ’‰', 'ðŸ’Š', 'ðŸ’‹', 'ðŸ’Œ', 'ðŸ’', 'ðŸ’Ž', 'ðŸ’', 'ðŸ’', 'ðŸ’‘', 'ðŸ’’', 'ðŸ’“', 'ðŸ’”', 'ðŸ’•', 'ðŸ’–', 'ðŸ’—', 'ðŸ’˜', 'ðŸ’™', 'ðŸ’š', 'ðŸ’›', 'ðŸ’œ', 'ðŸ’', 'ðŸ’ž', 'ðŸ’Ÿ', 'ðŸ’¡', 'ðŸ’¢', 'ðŸ’£', 'ðŸ’¤', 'ðŸ’¥', 'ðŸ’¦', 'ðŸ’§', 'ðŸ’¨', 'ðŸ’©', 'ðŸ’ª', 'ðŸ’«', 'ðŸ’¬', 'ðŸ’­', 'ðŸ’®', 'ðŸ’¯', 'ðŸ’°', 'ðŸ’²', 'ðŸ’³', 'ðŸ’µ', 'ðŸ’¶', 'ðŸ’¸', 'ðŸ’»', 'ðŸ’¼', 'ðŸ’½', 'ðŸ’¿', 'ðŸ“€', 'ðŸ“‚', 'ðŸ“…', 'ðŸ“†', 'ðŸ“ˆ', 'ðŸ“Š', 'ðŸ“‹', 'ðŸ“Œ', 'ðŸ“', 'ðŸ“', 'ðŸ““', 'ðŸ“–', 'ðŸ“š', 'ðŸ“›', 'ðŸ“', 'ðŸ“ž', 'ðŸ“¡', 'ðŸ“¢', 'ðŸ“£', 'ðŸ“¦', 'ðŸ“§', 'ðŸ“©', 'ðŸ“¬', 'ðŸ“¯', 'ðŸ“°', 'ðŸ“±', 'ðŸ“²', 'ðŸ“´', 'ðŸ“·', 'ðŸ“¸', 'ðŸ“¹', 'ðŸ“º', 'ðŸ“»', 'ðŸ“¼', 'ðŸ“½', 'ðŸ“¿', 'ðŸ”', 'ðŸ”‚', 'ðŸ”ƒ', 'ðŸ”„', 'ðŸ”…', 'ðŸ”‰', 'ðŸ”Š', 'ðŸ”‹', 'ðŸ”Œ', 'ðŸ”', 'ðŸ”‘', 'ðŸ”’', 'ðŸ”“', 'ðŸ””', 'ðŸ”˜', 'ðŸ”™', 'ðŸ”›', 'ðŸ”œ', 'ðŸ”', 'ðŸ”ž', 'ðŸ”¥', 'ðŸ”¨', 'ðŸ”©', 'ðŸ”ª', 'ðŸ”«', 'ðŸ”®', 'ðŸ”°', 'ðŸ”±', 'ðŸ”²', 'ðŸ”´', 'ðŸ”µ', 'ðŸ”¶', 'ðŸ”¸', 'ðŸ”¹', 'ðŸ”º', 'ðŸ”»', 'ðŸ”¼', 'ðŸ”½', 'ðŸ•Š', 'ðŸ•‹', 'ðŸ•Œ', 'ðŸ•Ž', 'ðŸ•', 'ðŸ•’', 'ðŸ•˜', 'ðŸ•›', 'ðŸ•œ', 'ðŸ•Ÿ', 'ðŸ•¤', 'ðŸ•ª', 'ðŸ•¯', 'ðŸ•µ', 'ðŸ•¶', 'ðŸ•·', 'ðŸ•º', 'ðŸ–', 'ðŸ–’', 'ðŸ–•', 'ðŸ––', 'ðŸ–¤', 'ðŸ–¥', 'ðŸ–²', 'ðŸ–¼', 'ðŸ—‚', 'ðŸ—“', 'ðŸ—', 'ðŸ—ž', 'ðŸ—¡', 'ðŸ—£', 'ðŸ—¨', 'ðŸ—³', 'ðŸ—»', 'ðŸ—¼', 'ðŸ—½', 'ðŸ—¾', 'ðŸ˜€', 'ðŸ˜', 'ðŸ˜‚', 'ðŸ˜ƒ', 'ðŸ˜„', 'ðŸ˜…', 'ðŸ˜†', 'ðŸ˜‡', 'ðŸ˜ˆ', 'ðŸ˜‰', 'ðŸ˜Š', 'ðŸ˜‹', 'ðŸ˜Œ', 'ðŸ˜', 'ðŸ˜Ž', 'ðŸ˜', 'ðŸ˜', 'ðŸ˜‘', 'ðŸ˜’', 'ðŸ˜“', 'ðŸ˜”', 'ðŸ˜•', 'ðŸ˜–', 'ðŸ˜—', 'ðŸ˜˜', 'ðŸ˜™', 'ðŸ˜š', 'ðŸ˜›', 'ðŸ˜œ', 'ðŸ˜', 'ðŸ˜ž', 'ðŸ˜Ÿ', 'ðŸ˜ ', 'ðŸ˜¡', 'ðŸ˜¢', 'ðŸ˜£', 'ðŸ˜¤', 'ðŸ˜¥', 'ðŸ˜¦', 'ðŸ˜§', 'ðŸ˜¨', 'ðŸ˜©', 'ðŸ˜ª', 'ðŸ˜«', 'ðŸ˜¬', 'ðŸ˜­', 'ðŸ˜®', 'ðŸ˜¯', 'ðŸ˜°', 'ðŸ˜±', 'ðŸ˜²', 'ðŸ˜³', 'ðŸ˜´', 'ðŸ˜µ', 'ðŸ˜¶', 'ðŸ˜·', 'ðŸ˜¸', 'ðŸ˜¹', 'ðŸ˜º', 'ðŸ˜»', 'ðŸ˜¼', 'ðŸ˜½', 'ðŸ˜¿', 'ðŸ™€', 'ðŸ™', 'ðŸ™‚', 'ðŸ™ƒ', 'ðŸ™„', 'ðŸ™…', 'ðŸ™†', 'ðŸ™‡', 'ðŸ™ˆ', 'ðŸ™‰', 'ðŸ™Š', 'ðŸ™‹', 'ðŸ™Œ', 'ðŸ™', 'ðŸ™Ž', 'ðŸ™', 'ðŸš€', 'ðŸš', 'ðŸš‡', 'ðŸšˆ', 'ðŸšŒ', 'ðŸš‘', 'ðŸš“', 'ðŸš”', 'ðŸš–', 'ðŸš—', 'ðŸš˜', 'ðŸš™', 'ðŸš¢', 'ðŸš£', 'ðŸš¦', 'ðŸš§', 'ðŸš¨', 'ðŸš©', 'ðŸš«', 'ðŸš¬', 'ðŸš®', 'ðŸš²', 'ðŸš´', 'ðŸšµ', 'ðŸš¶', 'ðŸš»', 'ðŸš¼', 'ðŸš¿', 'ðŸ›€', 'ðŸ›', 'ðŸ›ƒ', 'ðŸ›„', 'ðŸ›', 'ðŸ›©', 'ðŸ›«', 'ðŸ›¬', 'ðŸ›°', 'ðŸ›³', 'ðŸ›´', 'ðŸ¤', 'ðŸ¤‘', 'ðŸ¤’', 'ðŸ¤“', 'ðŸ¤”', 'ðŸ¤•', 'ðŸ¤–', 'ðŸ¤—', 'ðŸ¤˜', 'ðŸ¤™', 'ðŸ¤š', 'ðŸ¤›', 'ðŸ¤œ', 'ðŸ¤', 'ðŸ¤ž', 'ðŸ¤ ', 'ðŸ¤¡', 'ðŸ¤¢', 'ðŸ¤£', 'ðŸ¤¤', 'ðŸ¤¥', 'ðŸ¤¦', 'ðŸ¤§', 'ðŸ¤³', 'ðŸ¤´', 'ðŸ¤·', 'ðŸ¥€', 'ðŸ¥', 'ðŸ¥‚', 'ðŸ¥ƒ', 'ðŸ¥„', 'ðŸ¥…', 'ðŸ¥‡', 'ðŸ¥Š', 'ðŸ¥', 'ðŸ¥’', 'ðŸ¥“', 'ðŸ¥”', 'ðŸ¥˜', 'ðŸ¥™', 'ðŸ¥ž', 'ðŸ¦€', 'ðŸ¦', 'ðŸ¦ƒ', 'ðŸ¦„', 'ðŸ¦…', 'ðŸ¦‡', 'ðŸ¦‰', 'ðŸ¦‹', 'ðŸ¦‘', '\\U000fe4e6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMC_u8eQbVvZ",
        "colab_type": "code",
        "outputId": "b81fb63e-def7-41a5-f486-492c6a06df98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "def find_ngrams(text, length):\n",
        "    counter = Counter()\n",
        "    ngram = []\n",
        "    num_tokens, tokens = split_into_tokens(text.lower())\n",
        "    for token in tokens:\n",
        "        for i,_ in enumerate(token):\n",
        "           if(i>=length - 1):\n",
        "                post = token[i]\n",
        "                prior = token[i - (length - 1) : i]\n",
        "                ngram = ''.join(prior) + post\n",
        "                counter[ngram] = counter.get(ngram, 0) + 1\n",
        "    return counter\n",
        "def token_to_unigram(token):\n",
        "    token = token.strip().strip(\",.!|&-_()[]<>{}/\\\"'\").strip()\n",
        "\n",
        "    def has_no_chars(token):\n",
        "        for char in token:\n",
        "            if char.isalpha():\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(token) == 1 or token.isdigit() or has_no_chars(token):\n",
        "        return None\n",
        "    return token\n",
        "def split_into_tokens(text):\n",
        "    tokens = []\n",
        "    for token in nltk.tokenize.WhitespaceTokenizer().tokenize(text):\n",
        "        unigram = token_to_unigram(token)\n",
        "        if unigram:\n",
        "            tokens.append(unigram)\n",
        "    return len(tokens), tokens\n",
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  # TODO\n",
        "  model = {}\n",
        "  with open(data_file_path, encoding='utf-8', newline='') as csv_file:\n",
        "    csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "    corpus = []\n",
        "    for tweet_text in csv_reader['tweet_text']:\n",
        "            corpus.append(tweet_text)\n",
        "    data = find_ngrams(\" \".join(corpus),n)\n",
        "    unigram_total = float(sum(data.values()))\n",
        "    temp_dic = {}\n",
        "\n",
        "    for ngram in data.items():\n",
        "        if (add_one == True):\n",
        "            p = float(ngram[1] + 1)/(unigram_total + len(vocabulary))\n",
        "        else:\n",
        "            p = float(ngram[1])/unigram_total\n",
        "        model.update({ngram[0]:p})\n",
        "  return model\n",
        "\n",
        "model = lm(3, vocabulary, '/content/drive/My Drive/IDC/NLP/Assignment1/en.csv.json', True)\n",
        "print({k: v for i, (k, v) in enumerate(model.items()) if i < 50})\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  # TODO\n",
        "  # model = {}\n",
        "\n",
        "  # corpus_words = sorted(list(set([word for words_list in corpus for word in words_list])))\n",
        "  # num_corpus_words = len(corpus_words)\n",
        "  # data = []\n",
        "  # with open(data_file_path) as csv_file:\n",
        "  #     csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "  #       for corpus in csv_reader['tweet_text']:\n",
        "  #         data = sorted(list(set([word for words_list in corpus.split(\" \")]\n",
        "  # #       for word in list(data)\n",
        "  # #      data =  [[START_TOKEN] + [w.lower() for w in list(data.words(f))] + [END_TOKEN] for f in files]\n",
        "  # #     data = (\" \" * n) + data\n",
        "  # #     for i in range(len(data)-n):\n",
        "  # #       word, char = data[i:i+n], data[i+n]\n",
        "  # #       if not word in model:\n",
        "  # #         model[word] = {}\n",
        "  # #       model[word][char] = model[word].get(char, 0) + 1\n",
        "  # # word_freq = Counter(chain(*data))\n",
        "  # return model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bc1696842c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_unigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_into_unigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_unigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m###########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m from nltk.chunk.util import (ChunkScore, accuracy, tagstr2tree, conllstr2tree,\n\u001b[1;32m    159\u001b[0m                              \u001b[0mconlltags2tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conllstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/chunk/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m##//////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmalt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaltParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDependencyEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitionparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransitionParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbllip\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBllipParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorenlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoreNLPParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCoreNLPDependencyParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/transitionparser.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_svmlight_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m _DEFAULT_TAGS = {\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .validation import (as_float_array,\n\u001b[1;32m     29\u001b[0m                          \u001b[0massert_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Preserves earlier default choice of pinvh cutoff `cond` value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Can be removed once issue #14055 is fully addressed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scipy_linalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpinvh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpinvh\u001b[0m \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3",
        "colab_type": "text"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kkMn328-lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(n, model, data_file):\n",
        "  # n - the n-gram that you used to build your model (must be the same number)\n",
        "  # model - the dictionary (model) to use for calculating perplexity\n",
        "  # data_file - the tweets file that you wish to claculate a perplexity score for\n",
        "\n",
        "  # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p",
        "colab_type": "text"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAxLE9s_fvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match(n, add_one):\n",
        "  # n - the n-gram to use for creating n-gram models\n",
        "  # add_one - use add_one smoothing or not\n",
        "\n",
        "  #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17",
        "colab_type": "text"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk32naXyAMdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW",
        "colab_type": "text"
      },
      "source": [
        "# **Good luck!**"
      ]
    }
  ]
}