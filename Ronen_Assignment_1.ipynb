{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ronen_Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronenbendavid/IDC_NLP/blob/master/Ronen_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM",
        "colab_type": "text"
      },
      "source": [
        "*As a preparation for this task, place the data files somewhere in your drive so that you can access the files from this notebook. The files are available to download from the Moodle assignment activity*\n",
        "\n",
        "The relevant files are:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGtqPrCPnLUn",
        "colab_type": "code",
        "outputId": "a5b826b2-881d-41a6-a406-084630ddc0dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfzsITW8Yaj",
        "colab_type": "code",
        "outputId": "14a016ca-580d-4682-90fd-21afcc97d51c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess():\n",
        "  # TODO\n",
        " vocabulary = set()\n",
        " pathlist = glob('/content/drive/My Drive/IDC/NLP/Assignment1/*.csv')\n",
        " for path in pathlist:\n",
        "    with open(path, encoding='utf-8', newline='') as csv_file:\n",
        "        csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "        for text in csv_reader['tweet_text']:\n",
        "            list_text = list(text)\n",
        "            vocabulary.update(list_text)\n",
        "            line_count += 1\n",
        "        #print(line_count)\n",
        " return sorted(list(vocabulary))\n",
        "\n",
        "vocabulary = preprocess()\n",
        "print(len(vocabulary))\n",
        "print(vocabulary)\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1859\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x97', '\\x9d', '¡', '£', '¤', '¥', '§', '¨', '©', 'ª', '«', '\\xad', '®', '¯', '°', '²', '³', '´', '¶', '·', '¸', 'º', '»', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Å', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ù', 'Ú', 'Ü', 'à', 'á', 'â', 'ã', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ė', 'Ğ', 'ğ', 'İ', 'ı', 'ń', 'ō', 'Œ', 'œ', 'Ş', 'ş', 'Š', 'Ÿ', 'ƒ', 'ʔ', 'ʕ', 'ʖ', 'ʰ', 'ʳ', 'ʷ', 'ʸ', 'ˍ', '˖', '˘', '˚', '˛', 'ˡ', 'ˢ', '̀', '́', '̃', '̈', '̥', '̮', '̯', '͜', '͡', 'Δ', 'Θ', 'Ω', 'υ', 'ω', 'А', 'И', 'М', 'Н', 'О', 'П', 'Р', 'Ф', 'Ц', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'з', 'и', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'х', 'ы', 'э', 'ю', 'я', 'Ғ', 'ү', '،', 'آ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ض', 'ط', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ی', '۶', 'ं', 'क', 'ग', 'प', 'ब', 'र', 'स', 'ा', 'े', '्', 'ೃ', '෴', 'ก', 'ข', 'ง', 'จ', 'ญ', 'ด', 'ต', 'ถ', 'ท', 'น', 'บ', 'ป', 'พ', 'ภ', 'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ส', 'อ', 'ะ', 'ั', 'า', 'ิ', 'ี', 'ุ', 'ู', 'เ', 'แ', '่', '้', '๐', '๑', 'ຶ', '༎', '༺', '༻', '༼', '༽', 'ღ', 'ᙓ', 'ᴗ', 'ᴬ', 'ᴰ', 'ᵃ', 'ᵇ', 'ᵈ', 'ᵉ', 'ᵍ', 'ᵐ', 'ᵒ', 'ᵖ', 'ᵗ', 'ᵘ', 'ᵛ', 'ᶜ', 'ᶠ', 'ᶦ', 'ᶰ', '\\u2009', '\\u200a', '\\u200b', '\\u200d', '–', '—', '―', '‘', '’', '“', '”', '„', '†', '•', '․', '…', '‰', '′', '‹', '›', '※', '‼', '‿', '⁉', '\\u2066', '\\u2067', '\\u2069', 'ⁱ', '⁷', 'ⁿ', '€', '₹', '⃣', '℃', '℅', '™', 'Ⅰ', 'Ⅱ', 'Ⅲ', 'Ⅳ', 'Ⅴ', '←', '↑', '→', '↓', '↔', '↕', '↗', '↘', '↚', '↛', '↩', '↪', '↯', '↺', '⇘', '⇨', '∀', '∆', '∇', '√', '∞', '∴', '∵', '≤', '≥', '≦', '≧', '⊙', '⋅', '⋪', '⋭', '⌚', '⌛', '⌣', '⎋', '⏩', '⏰', '⏱', '⏳', '⏸', '①', '⑥', '⒈', '⒉', '⒊', '⒋', '⒌', '⒍', '⒎', '⒏', '⒐', '⒑', 'Ⓜ', 'ⓘ', 'ⓙ', 'ⓢ', 'ⓦ', '─', '━', '┃', '┄', '┆', '┌', '┏', '┐', '┓', '└', '┗', '┘', '┛', '┳', '┻', '║', '╔', '╗', '╚', '╝', '╦', '╩', '╬', '╭', '╮', '╯', '╰', '╱', '╲', '╴', '█', '▊', '▏', '▒', '▔', '▕', '▙', '▝', '▣', '▦', '▪', '▲', '△', '▶', '▸', '►', '▼', '▽', '▿', '◀', '◁', '◄', '◆', '◇', '◈', '○', '◎', '●', '◑', '◕', '◡', '◻', '◼', '◽', '◾', '☀', '☁', '☃', '☄', '★', '☆', '☉', '☎', '☑', '☓', '☔', '☕', '☘', '☙', '☚', '☛', '☜', '☝', '☞', '☠', '☣', '☪', '☮', '☯', '☰', '☹', '☺', '☼', '☽', '☾', '♀', '♂', '♊', '♋', '♍', '♎', '♏', '♐', '♓', '♛', '♡', '♣', '♤', '♥', '♦', '♩', '♪', '♫', '♬', '♯', '♻', '⚒', '⚓', '⚔', '⚕', '⚖', '⚘', '⚜', '⚝', '⚠', '⚡', '⚪', '⚫', '⚰', '⚽', '⚾', '⛄', '⛅', '⛈', '⛓', '⛔', '⛩', '⛪', '⛳', '⛷', '⛽', '✁', '✂', '✃', '✅', '✈', '✉', '✊', '✋', '✌', '✍', '✏', '✓', '✔', '✖', '✝', '✡', '✧', '✨', '✩', '✪', '✭', '✰', '✳', '✴', '✵', '✶', '✷', '✿', '❀', '❁', '❄', '❅', '❈', '❋', '❌', '❎', '❓', '❔', '❗', '❝', '❞', '❣', '❤', '❥', '➊', '➋', '➌', '➍', '➎', '➏', '➔', '➖', '➗', '➙', '➛', '➜', '➞', '➟', '➠', '➡', '➢', '➤', '➰', '➵', '⠀', '⤵', '⦑', '⦒', '⬅', '⬇', '⭐', '⸄', '⸅', '\\u3000', '、', '。', '〆', '《', '》', '「', '」', '『', '』', '【', '】', '〜', '〡', '〰', 'い', 'う', 'え', 'お', 'か', 'が', 'き', 'く', 'し', 'せ', 'ぜ', 'た', 'っ', 'づ', 'て', 'で', 'と', 'な', 'に', 'ね', 'の', 'は', 'ひ', 'み', 'む', 'ょ', 'ら', 'り', 'る', 'れ', 'わ', 'を', '゜', 'イ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'キ', 'ク', 'グ', 'コ', 'ゴ', 'サ', 'ジ', 'ス', 'セ', 'タ', 'チ', 'ッ', 'ツ', 'テ', 'ト', 'ド', 'ナ', 'ニ', 'ネ', 'ノ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ポ', 'ム', 'メ', 'ュ', 'ユ', 'ョ', 'ラ', 'レ', 'ロ', 'ヮ', 'ワ', 'ン', '・', 'ー', 'ヽ', 'ㅅ', 'ㅈ', 'ㅋ', 'ㅏ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ', 'ㅤ', '\\u31ef', '世', '中', '主', '互', '人', '付', '会', '像', '儿', '允', '先', '入', '写', '分', '利', '制', '刹', '力', '努', '動', '午', '卒', '南', '合', '呟', '嘉', '増', '好', '姿', '嫌', '学', '尔', '希', '彡', '影', '彼', '後', '悪', '手', '投', '拶', '挨', '撃', '撮', '文', '映', '時', '曲', '月', '有', '服', '本', '林', '柱', '業', '機', '歌', '歳', '毅', '気', '洲', '洸', '王', '生', '用', '画', '界', '相', '真', '瞬', '知', '社', '稿', '空', '糟', '終', '結', '繋', '者', '花', '菜', '行', '许', '赫', '踊', '込', '通', '那', '間', '限', '風', '魏', 'ꠎ', '가', '간', '갓', '강', '걸', '검', '게', '격', '결', '경', '고', '곡', '과', '구', '국', '규', '그', '근', '금', '기', '김', '꺽', '꼼', '나', '날', '남', '내', '너', '널', '네', '넷', '녀', '년', '노', '논', '누', '는', '늘', '니', '다', '단', '당', '닿', '대', '더', '도', '동', '두', '둑', '듀', '드', '등', '디', '라', '락', '랑', '랙', '랜', '램', '러', '런', '레', '렛', '로', '롱', '료', '루', '룰', '룸', '를', '름', '릉', '리', '림', '링', '마', '맞', '매', '맨', '몬', '무', '미', '민', '밀', '바', '박', '방', '배', '백', '뱀', '버', '벅', '법', '베', '벨', '벳', '보', '복', '본', '봄', '봉', '뷔', '브', '븐', '블', '비', '빅', '빼', '사', '살', '삼', '상', '생', '샤', '샵', '서', '석', '선', '성', '세', '섹', '셔', '션', '셩', '소', '송', '수', '슈', '스', '슨', '슬', '승', '시', '식', '신', '실', '싸', '아', '안', '압', '애', '야', '양', '어', '에', '엑', '엘', '엠', '엣', '여', '역', '연', '영', '예', '오', '온', '와', '왕', '외', '요', '용', '우', '울', '워', '원', '위', '유', '윤', '의', '이', '인', '일', '임', '잘', '장', '재', '잭', '전', '정', '제', '젤', '종', '주', '쥔', '즈', '지', '직', '진', '집', '쩜', '찌', '찰', '채', '천', '철', '초', '최', '추', '출', '츠', '치', '카', '커', '코', '콘', '콤', '쿱', '크', '키', '킹', '타', '탄', '탑', '태', '터', '텐', '토', '톡', '트', '티', '틴', '팁', '파', '패', '펀', '포', '풀', '프', '플', '피', '핑', '하', '한', '해', '핸', '헌', '헤', '헨', '혁', '현', '형', '호', '화', '환', '훈', '힐', 'ﷻ', '︎', '️', '︵', '﹏', '﹪', '！', '＂', '＃', '（', '）', '＊', '．', '３', '６', '７', '８', '？', '＠', 'Ａ', 'Ｂ', 'Ｃ', 'Ｄ', 'Ｅ', 'Ｆ', 'Ｇ', 'Ｈ', 'Ｉ', 'Ｋ', 'Ｌ', 'Ｍ', 'Ｎ', 'Ｏ', 'Ｐ', 'Ｑ', 'Ｒ', 'Ｓ', 'Ｔ', 'Ｕ', 'Ｖ', 'Ｗ', 'Ｙ', '［', '］', '＿', '｀', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｇ', 'ｉ', 'ｌ', 'ｍ', 'ｎ', 'ｏ', 'ｐ', 'ｑ', 'ｒ', 'ｓ', 'ｔ', 'ｕ', 'ｖ', 'ｗ', 'ｘ', 'ｙ', 'ｚ', '｜', '｡', '･', 'ﾉ', 'ﾒ', '￣', '￼', '�', '🃏', '🅰', '🅱', '🅾', '🅿', '🆑', '🆒', '🆓', '🆔', '🆕', '🆖', '🆗', '🆘', '🆙', '🆚', '🇦', '🇧', '🇨', '🇩', '🇪', '🇫', '🇬', '🇭', '🇮', '🇯', '🇰', '🇱', '🇲', '🇳', '🇴', '🇵', '🇷', '🇸', '🇹', '🇺', '🇻', '🇼', '🇽', '🇾', '🇿', '🈴', '🈵', '🈶', '🈷', '🌀', '🌃', '🌄', '🌅', '🌆', '🌇', '🌈', '🌊', '🌋', '🌌', '🌍', '🌎', '🌏', '🌐', '🌒', '🌓', '🌗', '🌙', '🌚', '🌛', '🌜', '🌝', '🌞', '🌟', '🌠', '🌤', '🌥', '🌧', '🌨', '🌪', '🌫', '🌬', '🌭', '🌮', '🌯', '🌰', '🌱', '🌲', '🌳', '🌴', '🌵', '🌶', '🌷', '🌸', '🌹', '🌺', '🌻', '🌼', '🌽', '🌾', '🌿', '🍀', '🍁', '🍂', '🍃', '🍅', '🍆', '🍇', '🍉', '🍊', '🍋', '🍌', '🍍', '🍎', '🍏', '🍑', '🍒', '🍓', '🍔', '🍕', '🍖', '🍗', '🍚', '🍛', '🍜', '🍝', '🍞', '🍟', '🍣', '🍤', '🍥', '🍦', '🍨', '🍩', '🍪', '🍫', '🍬', '🍭', '🍯', '🍰', '🍱', '🍳', '🍴', '🍵', '🍶', '🍷', '🍸', '🍹', '🍺', '🍻', '🍼', '🍽', '🍾', '🍿', '🎀', '🎁', '🎂', '🎅', '🎆', '🎇', '🎈', '🎉', '🎊', '🎋', '🎍', '🎒', '🎓', '🎗', '🎙', '🎞', '🎟', '🎠', '🎡', '🎢', '🎤', '🎥', '🎦', '🎧', '🎨', '🎩', '🎪', '🎫', '🎬', '🎭', '🎮', '🎯', '🎰', '🎱', '🎲', '🎵', '🎶', '🎷', '🎸', '🎹', '🎺', '🎻', '🎼', '🎾', '🎿', '🏀', '🏁', '🏃', '🏄', '🏅', '🏆', '🏇', '🏈', '🏉', '🏊', '🏋', '🏌', '🏒', '🏓', '🏔', '🏖', '🏘', '🏙', '🏚', '🏟', '🏠', '🏡', '🏢', '🏩', '🏫', '🏰', '🏳', '🏴', '🏹', '🏻', '🏼', '🏽', '🏾', '🏿', '🐀', '🐁', '🐂', '🐄', '🐆', '🐇', '🐈', '🐉', '🐊', '🐍', '🐎', '🐐', '🐑', '🐒', '🐓', '🐔', '🐕', '🐖', '🐘', '🐙', '🐚', '🐜', '🐝', '🐞', '🐟', '🐠', '🐡', '🐢', '🐣', '🐥', '🐦', '🐧', '🐨', '🐩', '🐫', '🐬', '🐭', '🐮', '🐯', '🐰', '🐱', '🐲', '🐳', '🐴', '🐶', '🐷', '🐸', '🐹', '🐺', '🐻', '🐼', '🐽', '🐾', '🐿', '👀', '👁', '👂', '👄', '👅', '👆', '👇', '👈', '👉', '👊', '👋', '👌', '👍', '👎', '👏', '👐', '👑', '👓', '👕', '👖', '👗', '👙', '👞', '👟', '👠', '👡', '👣', '👤', '👥', '👦', '👧', '👨', '👩', '👪', '👫', '👭', '👮', '👯', '👰', '👱', '👲', '👳', '👵', '👶', '👷', '👸', '👹', '👺', '👻', '👼', '👽', '👿', '💀', '💁', '💂', '💃', '💄', '💅', '💆', '💈', '💉', '💊', '💋', '💌', '💍', '💎', '💏', '💐', '💑', '💒', '💓', '💔', '💕', '💖', '💗', '💘', '💙', '💚', '💛', '💜', '💝', '💞', '💟', '💡', '💢', '💣', '💤', '💥', '💦', '💧', '💨', '💩', '💪', '💫', '💬', '💭', '💮', '💯', '💰', '💲', '💳', '💵', '💶', '💸', '💻', '💼', '💽', '💿', '📀', '📂', '📅', '📆', '📈', '📊', '📋', '📌', '📍', '📏', '📓', '📖', '📚', '📛', '📝', '📞', '📡', '📢', '📣', '📦', '📧', '📩', '📬', '📯', '📰', '📱', '📲', '📴', '📷', '📸', '📹', '📺', '📻', '📼', '📽', '📿', '🔁', '🔂', '🔃', '🔄', '🔅', '🔉', '🔊', '🔋', '🔌', '🔐', '🔑', '🔒', '🔓', '🔔', '🔘', '🔙', '🔛', '🔜', '🔝', '🔞', '🔥', '🔨', '🔩', '🔪', '🔫', '🔮', '🔰', '🔱', '🔲', '🔴', '🔵', '🔶', '🔸', '🔹', '🔺', '🔻', '🔼', '🔽', '🕊', '🕋', '🕌', '🕎', '🕐', '🕒', '🕘', '🕛', '🕜', '🕟', '🕤', '🕪', '🕯', '🕵', '🕶', '🕷', '🕺', '🖐', '🖒', '🖕', '🖖', '🖤', '🖥', '🖲', '🖼', '🗂', '🗓', '🗝', '🗞', '🗡', '🗣', '🗨', '🗳', '🗻', '🗼', '🗽', '🗾', '😀', '😁', '😂', '😃', '😄', '😅', '😆', '😇', '😈', '😉', '😊', '😋', '😌', '😍', '😎', '😏', '😐', '😑', '😒', '😓', '😔', '😕', '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😦', '😧', '😨', '😩', '😪', '😫', '😬', '😭', '😮', '😯', '😰', '😱', '😲', '😳', '😴', '😵', '😶', '😷', '😸', '😹', '😺', '😻', '😼', '😽', '😿', '🙀', '🙁', '🙂', '🙃', '🙄', '🙅', '🙆', '🙇', '🙈', '🙉', '🙊', '🙋', '🙌', '🙍', '🙎', '🙏', '🚀', '🚁', '🚇', '🚈', '🚌', '🚑', '🚓', '🚔', '🚖', '🚗', '🚘', '🚙', '🚢', '🚣', '🚦', '🚧', '🚨', '🚩', '🚫', '🚬', '🚮', '🚲', '🚴', '🚵', '🚶', '🚻', '🚼', '🚿', '🛀', '🛁', '🛃', '🛄', '🛐', '🛩', '🛫', '🛬', '🛰', '🛳', '🛴', '🤐', '🤑', '🤒', '🤓', '🤔', '🤕', '🤖', '🤗', '🤘', '🤙', '🤚', '🤛', '🤜', '🤝', '🤞', '🤠', '🤡', '🤢', '🤣', '🤤', '🤥', '🤦', '🤧', '🤳', '🤴', '🤷', '🥀', '🥁', '🥂', '🥃', '🥄', '🥅', '🥇', '🥊', '🥐', '🥒', '🥓', '🥔', '🥘', '🥙', '🥞', '🦀', '🦁', '🦃', '🦄', '🦅', '🦇', '🦉', '🦋', '🦑', '\\U000fe4e6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMC_u8eQbVvZ",
        "colab_type": "code",
        "outputId": "1fcc9fc2-32e5-411a-a845-c466c0d76b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        }
      },
      "source": [
        "import nltk.data\n",
        "import nltk.tokenize \n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "def find_ngrams(text, n):\n",
        "    counter = Counter()\n",
        "    ngram = {}\n",
        "\n",
        "    num_tokens, tokens = split_into_tokens(text.lower())\n",
        "    for token in tokens:\n",
        "        sequence  = {}\n",
        "        for i,_ in enumerate(token):\n",
        "            if(i>=n - 1):\n",
        "                word, char = token[i - (n - 1)], token[i]\n",
        "                nchar_word = ''.join(word) + char\n",
        "                counter[nchar_word] = counter.get(nchar_word, 0) + 1\n",
        "                sequence[char] = counter[nchar_word]\n",
        "                ngram[word] = sequence\n",
        "    return ngram\n",
        "def token_to_unigram(token):\n",
        "    token = token.strip().strip(\",.!|&-_()[]<>{}/\\\"'\").strip()\n",
        "\n",
        "    def has_no_chars(token):\n",
        "        for char in token:\n",
        "            if char.isalpha():\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(token) == 1 or token.isdigit() or has_no_chars(token):\n",
        "        return None\n",
        "    return token\n",
        "def split_into_tokens(text):\n",
        "    tokens = []\n",
        "    for token in nltk.tokenize.WhitespaceTokenizer().tokenize(text):\n",
        "    #for token in tokenize(text):\n",
        "        unigram = token_to_unigram(token)\n",
        "        if unigram:\n",
        "            tokens.append(unigram)\n",
        "    return len(tokens), tokens\n",
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  # TODO\n",
        "  voc_size = len(vocabulary)\n",
        "  if add_one:\n",
        "      model = defaultdict(lambda: defaultdict(lambda: 1/voc_size))\n",
        "  else:\n",
        "      model = defaultdict(lambda: defaultdict(lambda: 1e-08))\n",
        "  with open(data_file_path, encoding='utf-8', newline='') as csv_file:\n",
        "    csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "    corpus = []\n",
        "    for tweet_text in csv_reader['tweet_text']:\n",
        "            corpus.append(tweet_text)\n",
        "    data = find_ngrams(\" \".join(corpus),n)\n",
        "\n",
        "    for word,chars in data.items():\n",
        "        temp_dict = {}\n",
        "        words_total = float(sum(chars.values()))\n",
        "        for char, count in chars.items():\n",
        "            if (add_one == True):\n",
        "                p = float(count + 1)/(words_total + len(vocabulary))\n",
        "            else:\n",
        "                p = float(count)/words_total\n",
        "            temp_dict[char] = p\n",
        "        model[word] = temp_dict\n",
        "  return model\n",
        "n =3\n",
        "file_name = '/content/drive/My Drive/IDC/NLP/Assignment1/en.csv'\n",
        "model = lm(n, vocabulary,file_name , False)\n",
        "# print({k: v for i, (k, v) in enumerate(model.items()) if i < 50})\n",
        "# print(sum(v for v in model['zd'].values()))\n",
        "#model['rt']\n",
        "print({k: v for i, (k, v) in enumerate(model.items()) if i < 50})\n",
        "for j, key in enumerate(model.keys()):\n",
        "    if j < 50:\n",
        "        print(\"ngram:{:s} {:f}\".format(key,sum(model[key].values())))  "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'@': {'v': 0.007225130890052356, 'e': 0.3006282722513089, 'b': 0.009424083769633508, 'n': 0.16073298429319371, 't': 0.4637696335078534, 'x': 0.019895287958115182, ':': 0.038324607329842934}, 'h': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, 'a': {'a': 0.18001368925393565, 'l': 0.14449007529089664, 'i': 0.22306639288158794, 's': 0.11225188227241616, 'e': 0.3401779603011636}, 'n': {'n': 0.1119746233148295, 'i': 0.24885011895321174, 's': 0.2696272799365583, 'h': 0.3695479777954005}, 'm': {'m': 0.12084468664850136, 'e': 0.6769754768392371, 's': 0.20217983651226157}, 'e': {'a': 0.18001368925393565, 'l': 0.14449007529089664, 'i': 0.22306639288158794, 's': 0.11225188227241616, 'e': 0.3401779603011636}, 'o': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, 'r': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, 'i': {\"'\": 0.04661296712003169, 's': 0.9533870328799683}, 's': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, 't': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, 'w': {'e': 0.3261961406291303, 'n': 0.6738038593708697}, 'c': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, 'b': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, 'l': {'a': 0.18001368925393565, 'l': 0.14449007529089664, 'i': 0.22306639288158794, 's': 0.11225188227241616, 'e': 0.3401779603011636}, 'p': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, 'f': {'n': 0.1119746233148295, 'i': 0.24885011895321174, 's': 0.2696272799365583, 'h': 0.3695479777954005}, 'd': {'t': 0.02232750128992917, 'p': 0.15202401613584127, 's': 0.16928561377175289, ':': 0.14027393404943947, '/': 0.13405882077020498, '.': 0.13567709554857169, 'c': 0.15387682349078288, 'o': 0.0507293963131479, 'v': 0.008349359726065951, 'd': 0.0040808668324030205, 'w': 0.0012899291711618744, 'i': 0.015056991416107697, 'm': 0.0041746798630329755, 'b': 0.005511515549509827, 'j': 0.0025798583423237488, '0': 0.0007035977297246588}, '#': {'u': 0.020380311163679374, 'b': 0.0205622782276408, 'l': 0.08397780001819671, 'i': 0.07096715494495497, 'c': 0.07924665635519972, 'o': 0.19670639614229823, 'p': 0.026112273678464196, 't': 0.2750432171776908, 'n': 0.22700391229187517}, '1': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, 'y': {'u': 0.5882352941176471, 'r': 0.4117647058823529}, ':': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, '/': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, '.': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, '9': {'t': 0.005637791221521431, 'p': 0.16679109280473678, 's': 0.18578967692109666, ':': 0.15394516668811944, '/': 0.1471231818766894, '.': 0.1488994722615523, 'c': 0.16887630325653238, 'o': 0.002291157163084052, '9': 0.0025743338911056765, 'f': 0.004865491054189729, '6': 0.000540610117132192, '1': 0.0006950701505985327, 'b': 0.0005920967949543056, 'u': 0.0006435834727764191, 'y': 0.009370575363624662, 'q': 0.0013643969622860084}, 'u': {'u': 0.03739144419427469, 's': 0.06183660340945642, 't': 0.12881955612737214, 'i': 0.16741717594081698, 'n': 0.12335155998713412, '_': 0.009890640077195239, 'h': 0.04020585397233837, 'a': 0.014634930845931168, 'l': 0.08394982309424252, 'p': 0.06987777420392409, 'e': 0.1034094564168543, 'r': 0.12632679318108717, ':': 0.03288838854937279}, '_': {'u': 0.03739144419427469, 's': 0.06183660340945642, 't': 0.12881955612737214, 'i': 0.16741717594081698, 'n': 0.12335155998713412, '_': 0.009890640077195239, 'h': 0.04020585397233837, 'a': 0.014634930845931168, 'l': 0.08394982309424252, 'p': 0.06987777420392409, 'e': 0.1034094564168543, 'r': 0.12632679318108717, ':': 0.03288838854937279}, 'g': {'t': 1.0}, 'z': {'a': 0.22068578903111277, 'z': 0.016817714659441277, 'i': 0.3038400448472391, 'n': 0.021489302064841634, 'g': 0.43716714939736523}, 'k': {'t': 0.13279545003637325, 'p': 0.1429138284504993, 's': 0.15913850494896722, ':': 0.1318695853448846, '/': 0.1260278200295395, '.': 0.12754888345127086, 'c': 0.14465533584639464, 'o': 0.0050481670083548265, 'y': 0.01664352004937945, '6': 0.0009038202940722615, 'w': 0.007318739942243678, 'q': 0.000705420717324692, '3': 0.0008817758966558649, 'k': 0.0009699534863214514, '5': 0.0007715539095738818, 'v': 0.001807640588144523}, 'j': {'u': 0.03739144419427469, 's': 0.06183660340945642, 't': 0.12881955612737214, 'i': 0.16741717594081698, 'n': 0.12335155998713412, '_': 0.009890640077195239, 'h': 0.04020585397233837, 'a': 0.014634930845931168, 'l': 0.08394982309424252, 'p': 0.06987777420392409, 'e': 0.1034094564168543, 'r': 0.12632679318108717, ':': 0.03288838854937279}, '3': {'t': 0.13279545003637325, 'p': 0.1429138284504993, 's': 0.15913850494896722, ':': 0.1318695853448846, '/': 0.1260278200295395, '.': 0.12754888345127086, 'c': 0.14465533584639464, 'o': 0.0050481670083548265, 'y': 0.01664352004937945, '6': 0.0009038202940722615, 'w': 0.007318739942243678, 'q': 0.000705420717324692, '3': 0.0008817758966558649, 'k': 0.0009699534863214514, '5': 0.0007715539095738818, 'v': 0.001807640588144523}, '7': {'t': 0.04340550784242571, 'p': 0.1305390911656788, 's': 0.14535704205475586, ':': 0.12047901294302649, '/': 0.11515664690939881, '.': 0.11654771985000605, 'c': 0.13213176888028708, 'o': 0.11662836175960646, '2': 0.0007056167090036692, '7': 0.0019152453530099593, 'l': 0.0004636909802024112, 'i': 0.0007862586186040885, 'e': 0.0258457320269344, 'n': 0.0308656908995605, 'y': 0.012318051691464054, 'v': 0.006854562316035644}, '8': {'t': 0.13439655944807813, 'p': 0.14454349968640803, 's': 0.16096227936564825, ':': 0.1334333841053669, '/': 0.12756473434280083, '.': 0.12906549592330435, 'c': 0.14638025266553176, 'o': 0.0027775288952602813, 'b': 0.011692500671982797, 'v': 0.0037855030911208673, 'x': 0.0021055460980198906, 'j': 0.0010975719021593048, 'n': 0.0005599856643669922, '3': 0.0005599856643669922, '8': 0.0006047845175163516, 'e': 0.00047038795806827343}, '😢': {'l': 0.1, 'i': 0.1, 'v': 0.1, 'a': 0.1, 'n': 0.1, 'd': 0.2, 'm': 0.1, 'e': 0.2}, 'v': {'v': 0.007225130890052356, 'e': 0.3006282722513089, 'b': 0.009424083769633508, 'n': 0.16073298429319371, 't': 0.4637696335078534, 'x': 0.019895287958115182, ':': 0.038324607329842934}, 'q': {'t': 0.13279545003637325, 'p': 0.1429138284504993, 's': 0.15913850494896722, ':': 0.1318695853448846, '/': 0.1260278200295395, '.': 0.12754888345127086, 'c': 0.14465533584639464, 'o': 0.0050481670083548265, 'y': 0.01664352004937945, '6': 0.0009038202940722615, 'w': 0.007318739942243678, 'q': 0.000705420717324692, '3': 0.0008817758966558649, 'k': 0.0009699534863214514, '5': 0.0007715539095738818, 'v': 0.001807640588144523}, 'x': {'i': 0.1889505779557659, 's': 0.012105215254391554, 't': 0.19659597706380266, 'n': 0.17557112951670156, 'g': 0.4267771002093383}, '—': {'r': 0.13068314292375846, 'k': 0.1416414082536722, 's': 0.22930753089298206, '—': 0.00023315458148752622, 't': 0.054907903940312426, 'o': 0.00023315458148752622, 'd': 0.1039869433434367, 'a': 0.20366052692935416, 'y': 0.13534623455350897}, '2': {'t': 0.30203020302030203, 'o': 0.19441944194419442, 'p': 0.0855085508550855, 'r': 0.157015701570157, 'u': 0.09850985098509851, 'm': 0.1544154415441544, '2': 0.0036003600360036, '0': 0.004500450045004501}, '5': {'t': 0.15217720751667002, 'p': 0.0007324712063043039, 's': 0.18238533036977167, ':': 0.15111638714891898, '/': 0.14442311578096584, '.': 0.14616589209941402, 'c': 0.00442008486562942, 'o': 0.14629218023843202, '5': 0.0007829864619114973, '1': 0.001035562739947464, 'r': 0.03970499090725399, 'b': 0.011315417256011316, 'h': 0.0007324712063043039, 'n': 0.012224691856940795, 'g': 0.006491210345524349}, '4': {'t': 0.11683648714216399, 'p': 0.1256865599223678, 's': 0.13997088791848616, ':': 0.11600194080543426, '/': 0.11087821445900048, '.': 0.11221737020863659, 'c': 0.1272197962154294, 'o': 0.11229500242600679, 'v': 0.006889859291606016, 'k': 0.0036293061620572536, '4': 0.00034934497816593884, 'x': 0.0010480349344978166, 'a': 0.0006986899563318777, 'u': 0.0016302765647743813, 'h': 0.019097525473071326, 'f': 0.004560892770499757, '9': 0.0006210577389616691, '8': 0.000368753032508491}, '6': {'t': 0.13279545003637325, 'p': 0.1429138284504993, 's': 0.15913850494896722, ':': 0.1318695853448846, '/': 0.1260278200295395, '.': 0.12754888345127086, 'c': 0.14465533584639464, 'o': 0.0050481670083548265, 'y': 0.01664352004937945, '6': 0.0009038202940722615, 'w': 0.007318739942243678, 'q': 0.000705420717324692, '3': 0.0008817758966558649, 'k': 0.0009699534863214514, '5': 0.0007715539095738818, 'v': 0.001807640588144523}, '0': {'t': 0.30203020302030203, 'o': 0.19441944194419442, 'p': 0.0855085508550855, 'r': 0.157015701570157, 'u': 0.09850985098509851, 'm': 0.1544154415441544, '2': 0.0036003600360036, '0': 0.004500450045004501}, '“': {'a': 0.054945054945054944, 'd': 0.945054945054945}, '*': {'o': 0.0029835029835029833, 'r': 0.2084942084942085, 'l': 0.42488592488592486, 'd': 0.2234117234117234, \"'\": 0.007546507546507547, 's': 0.13267813267813267}, '-': {'m': 0.07913834811657643, 'a': 0.2674807049879046, 'r': 0.034212648312406405, '-': 0.004722958184540951, 'b': 0.00023038820412394885, 'i': 0.1522866029259302, '/': 0.00011519410206197443, 's': 0.22151825826517682, 'f': 0.021426102983527245, 'o': 0.2188687939177514}, \"'\": {'u': 0.6716315307057745, \"'\": 0.13863428047662696, 'r': 0.10197066911090742, 'e': 0.0877635197066911}, ',': {'g': 0.05365853658536585, 'h': 0.0004601932811780948, 't': 0.046663598711458816, ',': 0.00018407731247123792, 'e': 0.37984353428439943, 'r': 0.1869305108145421, \"'\": 0.011136677404509894, 's': 0.32112287160607456}, '【': {'b': 0.01818181818181818, 'c': 0.9818181818181818}}\n",
            "ngram:@ 1.000000\n",
            "ngram:h 1.000000\n",
            "ngram:a 1.000000\n",
            "ngram:n 1.000000\n",
            "ngram:m 1.000000\n",
            "ngram:e 1.000000\n",
            "ngram:o 1.000000\n",
            "ngram:r 1.000000\n",
            "ngram:i 1.000000\n",
            "ngram:s 1.000000\n",
            "ngram:t 1.000000\n",
            "ngram:w 1.000000\n",
            "ngram:c 1.000000\n",
            "ngram:b 1.000000\n",
            "ngram:l 1.000000\n",
            "ngram:p 1.000000\n",
            "ngram:f 1.000000\n",
            "ngram:d 1.000000\n",
            "ngram:# 1.000000\n",
            "ngram:1 1.000000\n",
            "ngram:y 1.000000\n",
            "ngram:: 1.000000\n",
            "ngram:/ 1.000000\n",
            "ngram:. 1.000000\n",
            "ngram:9 1.000000\n",
            "ngram:u 1.000000\n",
            "ngram:_ 1.000000\n",
            "ngram:g 1.000000\n",
            "ngram:z 1.000000\n",
            "ngram:k 1.000000\n",
            "ngram:j 1.000000\n",
            "ngram:3 1.000000\n",
            "ngram:7 1.000000\n",
            "ngram:8 1.000000\n",
            "ngram:😢 1.000000\n",
            "ngram:v 1.000000\n",
            "ngram:q 1.000000\n",
            "ngram:x 1.000000\n",
            "ngram:— 1.000000\n",
            "ngram:2 1.000000\n",
            "ngram:5 1.000000\n",
            "ngram:4 1.000000\n",
            "ngram:6 1.000000\n",
            "ngram:0 1.000000\n",
            "ngram:“ 1.000000\n",
            "ngram:* 1.000000\n",
            "ngram:- 1.000000\n",
            "ngram:' 1.000000\n",
            "ngram:, 1.000000\n",
            "ngram:【 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3",
        "colab_type": "text"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kkMn328-lJ",
        "colab_type": "code",
        "outputId": "ba498cd2-0508-4958-f6b6-e47553fcbabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "def compute_entropy_xi(ngram):\n",
        "    ll = 0\n",
        "    for key in ngram.items():\n",
        "        p = key[1]\n",
        "        ll += p*np.log2(p)\n",
        "    return ll\n",
        "\n",
        "def eval(n, model, data_file):\n",
        "    \"\"\"\n",
        "    Compute the perplexity of ngrams from model\n",
        "    \"\"\"\n",
        "    h_x = 0\n",
        "    N = 0\n",
        "    with open(data_file, encoding='utf-8', newline='') as csv_file:\n",
        "        csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "        corpus = []\n",
        "        for tweet_text in csv_reader['tweet_text']:\n",
        "                corpus.append(tweet_text)\n",
        "        data = find_ngrams(\" \".join(corpus),n)\n",
        "        N = len(model)   # Total number of words\n",
        "        for word,chars in data.items():\n",
        "            if word in model:\n",
        "                ngram = model[word]\n",
        "                #for i, ngram in enumerate(model.items()):\n",
        "                h_x += compute_entropy_xi(ngram)\n",
        "            #N += len(h_X)\n",
        "    return pow(2, (-1.0 / N) * h_x)\n",
        "\n",
        "test_file_name = '/content/drive/My Drive/IDC/NLP/Assignment1/es.csv'\n",
        "preplexity = eval(n,model,test_file_name)\n",
        "print(\"Preplexity evaluation : {:f}\".format(preplexity))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preplexity evaluation : 5.128441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p",
        "colab_type": "text"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAxLE9s_fvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "def match(n, add_one):\n",
        "  # n - the n-gram to use for creating n-gram models\n",
        "  # add_one - use add_one smoothing or not\n",
        "\n",
        "  #TODO\n",
        " path_name = '/content/drive/My Drive/IDC/NLP/Assignment1/'\n",
        " csvglob = path_name + '*.csv'\n",
        " pathlist = glob(csvglob) #'C:\\\\Users\\\\ronen\\\\Documents\\\\IDC\\Year-3\\\\Spring 2020\\\\NLP\\\\HW\\\\Assignment1\\\\*.csv')\n",
        " num = len(pathlist)\n",
        " matrix = np.empty(shape=(num,num),dtype='float')\n",
        " headers_list = []\n",
        " i = 0\n",
        " for model_path in pathlist:\n",
        "    head, tail = os.path.split(model_path)\n",
        "    headers_list.append(tail)\n",
        "    j = 0\n",
        "    modeli = lm(n, vocabulary,model_path, add_one)\n",
        "    for data_file_path in pathlist:\n",
        "        #if ( i != j ):\n",
        "        pp = eval(n,modeli,data_file_path)\n",
        "        matrix[i][j] = pp\n",
        "        print (\"Preplexity of model {:d} with data file {:d} is {:f}\".format(i,j,pp))\n",
        "        j = j+1\n",
        "    i = i + 1\n",
        " print(matrix)\n",
        " dataframe = pd.DataFrame(matrix,columns=headers_list)\n",
        " print(\"passed dataframe\")\n",
        " return dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17",
        "colab_type": "text"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk32naXyAMdl",
        "colab_type": "code",
        "outputId": "ce8ae1fd-d23d-482b-9820-bff9becfe677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "max_n = 4 \n",
        "for n in range(4): \n",
        "  print(\"n = {:d} add_one:{:b}\".format(n+1,False))\n",
        "  data1 = match(n+1,False) \n",
        "  print(data1) \n",
        "  print(\"n = {:d} add_one:{:b}\".format(n+1,True))\n",
        "  data2 = match(n+1,True) \n",
        "  print(data2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 1 add_one:0\n",
            "Preplexity of model 0 with data file 0 is 26.496436\n",
            "Preplexity of model 0 with data file 1 is 26.496436\n",
            "Preplexity of model 0 with data file 2 is 26.496436\n",
            "Preplexity of model 0 with data file 3 is 26.496436\n",
            "Preplexity of model 0 with data file 4 is 26.496436\n",
            "Preplexity of model 0 with data file 5 is 26.496436\n",
            "Preplexity of model 0 with data file 6 is 26.496436\n",
            "Preplexity of model 0 with data file 7 is 26.496436\n",
            "Preplexity of model 1 with data file 0 is 25.576839\n",
            "Preplexity of model 1 with data file 1 is 25.576839\n",
            "Preplexity of model 1 with data file 2 is 25.576839\n",
            "Preplexity of model 1 with data file 3 is 25.576839\n",
            "Preplexity of model 1 with data file 4 is 25.576839\n",
            "Preplexity of model 1 with data file 5 is 25.576839\n",
            "Preplexity of model 1 with data file 6 is 25.576839\n",
            "Preplexity of model 1 with data file 7 is 25.576839\n",
            "Preplexity of model 2 with data file 0 is 25.984383\n",
            "Preplexity of model 2 with data file 1 is 25.984383\n",
            "Preplexity of model 2 with data file 2 is 25.984383\n",
            "Preplexity of model 2 with data file 3 is 25.984383\n",
            "Preplexity of model 2 with data file 4 is 25.984383\n",
            "Preplexity of model 2 with data file 5 is 25.984383\n",
            "Preplexity of model 2 with data file 6 is 25.984383\n",
            "Preplexity of model 2 with data file 7 is 25.984383\n",
            "Preplexity of model 3 with data file 0 is 25.191946\n",
            "Preplexity of model 3 with data file 1 is 25.191946\n",
            "Preplexity of model 3 with data file 2 is 25.191946\n",
            "Preplexity of model 3 with data file 3 is 25.191946\n",
            "Preplexity of model 3 with data file 4 is 25.191946\n",
            "Preplexity of model 3 with data file 5 is 25.191946\n",
            "Preplexity of model 3 with data file 6 is 25.191946\n",
            "Preplexity of model 3 with data file 7 is 25.191946\n",
            "Preplexity of model 4 with data file 0 is 26.492316\n",
            "Preplexity of model 4 with data file 1 is 26.492316\n",
            "Preplexity of model 4 with data file 2 is 26.492316\n",
            "Preplexity of model 4 with data file 3 is 26.492316\n",
            "Preplexity of model 4 with data file 4 is 26.492316\n",
            "Preplexity of model 4 with data file 5 is 26.492316\n",
            "Preplexity of model 4 with data file 6 is 26.492316\n",
            "Preplexity of model 4 with data file 7 is 26.492316\n",
            "Preplexity of model 5 with data file 0 is 25.285959\n",
            "Preplexity of model 5 with data file 1 is 25.285959\n",
            "Preplexity of model 5 with data file 2 is 25.285959\n",
            "Preplexity of model 5 with data file 3 is 25.285959\n",
            "Preplexity of model 5 with data file 4 is 25.285959\n",
            "Preplexity of model 5 with data file 5 is 25.285959\n",
            "Preplexity of model 5 with data file 6 is 25.285959\n",
            "Preplexity of model 5 with data file 7 is 25.285959\n",
            "Preplexity of model 6 with data file 0 is 25.146164\n",
            "Preplexity of model 6 with data file 1 is 25.146164\n",
            "Preplexity of model 6 with data file 2 is 25.146164\n",
            "Preplexity of model 6 with data file 3 is 25.146164\n",
            "Preplexity of model 6 with data file 4 is 25.146164\n",
            "Preplexity of model 6 with data file 5 is 25.146164\n",
            "Preplexity of model 6 with data file 6 is 25.146164\n",
            "Preplexity of model 6 with data file 7 is 25.146164\n",
            "Preplexity of model 7 with data file 0 is 26.192283\n",
            "Preplexity of model 7 with data file 1 is 26.192283\n",
            "Preplexity of model 7 with data file 2 is 26.192283\n",
            "Preplexity of model 7 with data file 3 is 26.192283\n",
            "Preplexity of model 7 with data file 4 is 26.192283\n",
            "Preplexity of model 7 with data file 5 is 26.192283\n",
            "Preplexity of model 7 with data file 6 is 26.192283\n",
            "Preplexity of model 7 with data file 7 is 26.192283\n",
            "[[26.49643579 26.49643579 26.49643579 26.49643579 26.49643579 26.49643579\n",
            "  26.49643579 26.49643579]\n",
            " [25.57683885 25.57683885 25.57683885 25.57683885 25.57683885 25.57683885\n",
            "  25.57683885 25.57683885]\n",
            " [25.98438311 25.98438311 25.98438311 25.98438311 25.98438311 25.98438311\n",
            "  25.98438311 25.98438311]\n",
            " [25.19194604 25.19194604 25.19194604 25.19194604 25.19194604 25.19194604\n",
            "  25.19194604 25.19194604]\n",
            " [26.49231601 26.49231601 26.49231601 26.49231601 26.49231601 26.49231601\n",
            "  26.49231601 26.49231601]\n",
            " [25.28595878 25.28595878 25.28595878 25.28595878 25.28595878 25.28595878\n",
            "  25.28595878 25.28595878]\n",
            " [25.14616372 25.14616372 25.14616372 25.14616372 25.14616372 25.14616372\n",
            "  25.14616372 25.14616372]\n",
            " [26.19228272 26.19228272 26.19228272 26.19228272 26.19228272 26.19228272\n",
            "  26.19228272 26.19228272]]\n",
            "passed dataframe\n",
            "      en.csv     tl.csv     pt.csv  ...     in.csv     it.csv     nl.csv\n",
            "0  26.496436  26.496436  26.496436  ...  26.496436  26.496436  26.496436\n",
            "1  25.576839  25.576839  25.576839  ...  25.576839  25.576839  25.576839\n",
            "2  25.984383  25.984383  25.984383  ...  25.984383  25.984383  25.984383\n",
            "3  25.191946  25.191946  25.191946  ...  25.191946  25.191946  25.191946\n",
            "4  26.492316  26.492316  26.492316  ...  26.492316  26.492316  26.492316\n",
            "5  25.285959  25.285959  25.285959  ...  25.285959  25.285959  25.285959\n",
            "6  25.146164  25.146164  25.146164  ...  25.146164  25.146164  25.146164\n",
            "7  26.192283  26.192283  26.192283  ...  26.192283  26.192283  26.192283\n",
            "\n",
            "[8 rows x 8 columns]\n",
            "n = 1 add_one:1\n",
            "Preplexity of model 0 with data file 0 is 26.542122\n",
            "Preplexity of model 0 with data file 1 is 26.542122\n",
            "Preplexity of model 0 with data file 2 is 26.542122\n",
            "Preplexity of model 0 with data file 3 is 26.542122\n",
            "Preplexity of model 0 with data file 4 is 26.542122\n",
            "Preplexity of model 0 with data file 5 is 26.542122\n",
            "Preplexity of model 0 with data file 6 is 26.542122\n",
            "Preplexity of model 0 with data file 7 is 26.542122\n",
            "Preplexity of model 1 with data file 0 is 25.637519\n",
            "Preplexity of model 1 with data file 1 is 25.637519\n",
            "Preplexity of model 1 with data file 2 is 25.637519\n",
            "Preplexity of model 1 with data file 3 is 25.637519\n",
            "Preplexity of model 1 with data file 4 is 25.637519\n",
            "Preplexity of model 1 with data file 5 is 25.637519\n",
            "Preplexity of model 1 with data file 6 is 25.637519\n",
            "Preplexity of model 1 with data file 7 is 25.637519\n",
            "Preplexity of model 2 with data file 0 is 25.971220\n",
            "Preplexity of model 2 with data file 1 is 25.971220\n",
            "Preplexity of model 2 with data file 2 is 25.971220\n",
            "Preplexity of model 2 with data file 3 is 25.971220\n",
            "Preplexity of model 2 with data file 4 is 25.971220\n",
            "Preplexity of model 2 with data file 5 is 25.971220\n",
            "Preplexity of model 2 with data file 6 is 25.971220\n",
            "Preplexity of model 2 with data file 7 is 25.971220\n",
            "Preplexity of model 3 with data file 0 is 25.211794\n",
            "Preplexity of model 3 with data file 1 is 25.211794\n",
            "Preplexity of model 3 with data file 2 is 25.211794\n",
            "Preplexity of model 3 with data file 3 is 25.211794\n",
            "Preplexity of model 3 with data file 4 is 25.211794\n",
            "Preplexity of model 3 with data file 5 is 25.211794\n",
            "Preplexity of model 3 with data file 6 is 25.211794\n",
            "Preplexity of model 3 with data file 7 is 25.211794\n",
            "Preplexity of model 4 with data file 0 is 26.521928\n",
            "Preplexity of model 4 with data file 1 is 26.521928\n",
            "Preplexity of model 4 with data file 2 is 26.521928\n",
            "Preplexity of model 4 with data file 3 is 26.521928\n",
            "Preplexity of model 4 with data file 4 is 26.521928\n",
            "Preplexity of model 4 with data file 5 is 26.521928\n",
            "Preplexity of model 4 with data file 6 is 26.521928\n",
            "Preplexity of model 4 with data file 7 is 26.521928\n",
            "Preplexity of model 5 with data file 0 is 25.351467\n",
            "Preplexity of model 5 with data file 1 is 25.351467\n",
            "Preplexity of model 5 with data file 2 is 25.351467\n",
            "Preplexity of model 5 with data file 3 is 25.351467\n",
            "Preplexity of model 5 with data file 4 is 25.351467\n",
            "Preplexity of model 5 with data file 5 is 25.351467\n",
            "Preplexity of model 5 with data file 6 is 25.351467\n",
            "Preplexity of model 5 with data file 7 is 25.351467\n",
            "Preplexity of model 6 with data file 0 is 25.141867\n",
            "Preplexity of model 6 with data file 1 is 25.141867\n",
            "Preplexity of model 6 with data file 2 is 25.141867\n",
            "Preplexity of model 6 with data file 3 is 25.141867\n",
            "Preplexity of model 6 with data file 4 is 25.141867\n",
            "Preplexity of model 6 with data file 5 is 25.141867\n",
            "Preplexity of model 6 with data file 6 is 25.141867\n",
            "Preplexity of model 6 with data file 7 is 25.141867\n",
            "Preplexity of model 7 with data file 0 is 26.198441\n",
            "Preplexity of model 7 with data file 1 is 26.198441\n",
            "Preplexity of model 7 with data file 2 is 26.198441\n",
            "Preplexity of model 7 with data file 3 is 26.198441\n",
            "Preplexity of model 7 with data file 4 is 26.198441\n",
            "Preplexity of model 7 with data file 5 is 26.198441\n",
            "Preplexity of model 7 with data file 6 is 26.198441\n",
            "Preplexity of model 7 with data file 7 is 26.198441\n",
            "[[26.54212162 26.54212162 26.54212162 26.54212162 26.54212162 26.54212162\n",
            "  26.54212162 26.54212162]\n",
            " [25.63751929 25.63751929 25.63751929 25.63751929 25.63751929 25.63751929\n",
            "  25.63751929 25.63751929]\n",
            " [25.97121995 25.97121995 25.97121995 25.97121995 25.97121995 25.97121995\n",
            "  25.97121995 25.97121995]\n",
            " [25.21179396 25.21179396 25.21179396 25.21179396 25.21179396 25.21179396\n",
            "  25.21179396 25.21179396]\n",
            " [26.52192793 26.52192793 26.52192793 26.52192793 26.52192793 26.52192793\n",
            "  26.52192793 26.52192793]\n",
            " [25.35146716 25.35146716 25.35146716 25.35146716 25.35146716 25.35146716\n",
            "  25.35146716 25.35146716]\n",
            " [25.14186671 25.14186671 25.14186671 25.14186671 25.14186671 25.14186671\n",
            "  25.14186671 25.14186671]\n",
            " [26.1984412  26.1984412  26.1984412  26.1984412  26.1984412  26.1984412\n",
            "  26.1984412  26.1984412 ]]\n",
            "passed dataframe\n",
            "      en.csv     tl.csv     pt.csv  ...     in.csv     it.csv     nl.csv\n",
            "0  26.542122  26.542122  26.542122  ...  26.542122  26.542122  26.542122\n",
            "1  25.637519  25.637519  25.637519  ...  25.637519  25.637519  25.637519\n",
            "2  25.971220  25.971220  25.971220  ...  25.971220  25.971220  25.971220\n",
            "3  25.211794  25.211794  25.211794  ...  25.211794  25.211794  25.211794\n",
            "4  26.521928  26.521928  26.521928  ...  26.521928  26.521928  26.521928\n",
            "5  25.351467  25.351467  25.351467  ...  25.351467  25.351467  25.351467\n",
            "6  25.141867  25.141867  25.141867  ...  25.141867  25.141867  25.141867\n",
            "7  26.198441  26.198441  26.198441  ...  26.198441  26.198441  26.198441\n",
            "\n",
            "[8 rows x 8 columns]\n",
            "n = 2 add_one:0\n",
            "Preplexity of model 0 with data file 0 is 20.317579\n",
            "Preplexity of model 0 with data file 1 is 3.969307\n",
            "Preplexity of model 0 with data file 2 is 3.425897\n",
            "Preplexity of model 0 with data file 3 is 4.104371\n",
            "Preplexity of model 0 with data file 4 is 3.542470\n",
            "Preplexity of model 0 with data file 5 is 4.244031\n",
            "Preplexity of model 0 with data file 6 is 3.542470\n",
            "Preplexity of model 0 with data file 7 is 3.916534\n",
            "Preplexity of model 1 with data file 0 is 4.162896\n",
            "Preplexity of model 1 with data file 1 is 21.929138\n",
            "Preplexity of model 1 with data file 2 is 3.155910\n",
            "Preplexity of model 1 with data file 3 is 3.884435\n",
            "Preplexity of model 1 with data file 4 is 3.965959\n",
            "Preplexity of model 1 with data file 5 is 4.554951\n",
            "Preplexity of model 1 with data file 6 is 3.550093\n",
            "Preplexity of model 1 with data file 7 is 3.993512\n",
            "Preplexity of model 2 with data file 0 is 5.798349\n",
            "Preplexity of model 2 with data file 1 is 4.882399\n",
            "Preplexity of model 2 with data file 2 is 23.386745\n",
            "Preplexity of model 2 with data file 3 is 5.371768\n",
            "Preplexity of model 2 with data file 4 is 4.033345\n",
            "Preplexity of model 2 with data file 5 is 4.699371\n",
            "Preplexity of model 2 with data file 6 is 3.919406\n",
            "Preplexity of model 2 with data file 7 is 4.654696\n",
            "Preplexity of model 3 with data file 0 is 4.987072\n",
            "Preplexity of model 3 with data file 1 is 4.448727\n",
            "Preplexity of model 3 with data file 2 is 3.820228\n",
            "Preplexity of model 3 with data file 3 is 17.789714\n",
            "Preplexity of model 3 with data file 4 is 3.878858\n",
            "Preplexity of model 3 with data file 5 is 4.091242\n",
            "Preplexity of model 3 with data file 6 is 4.315255\n",
            "Preplexity of model 3 with data file 7 is 3.513247\n",
            "Preplexity of model 4 with data file 0 is 3.745249\n",
            "Preplexity of model 4 with data file 1 is 4.016276\n",
            "Preplexity of model 4 with data file 2 is 2.773360\n",
            "Preplexity of model 4 with data file 3 is 3.468195\n",
            "Preplexity of model 4 with data file 4 is 18.810550\n",
            "Preplexity of model 4 with data file 5 is 3.932971\n",
            "Preplexity of model 4 with data file 6 is 3.167070\n",
            "Preplexity of model 4 with data file 7 is 3.566487\n",
            "Preplexity of model 5 with data file 0 is 4.059866\n",
            "Preplexity of model 5 with data file 1 is 4.139647\n",
            "Preplexity of model 5 with data file 2 is 2.860117\n",
            "Preplexity of model 5 with data file 3 is 3.320313\n",
            "Preplexity of model 5 with data file 4 is 3.565891\n",
            "Preplexity of model 5 with data file 5 is 22.213344\n",
            "Preplexity of model 5 with data file 6 is 3.071658\n",
            "Preplexity of model 5 with data file 7 is 3.452090\n",
            "Preplexity of model 6 with data file 0 is 5.655098\n",
            "Preplexity of model 6 with data file 1 is 5.352456\n",
            "Preplexity of model 6 with data file 2 is 3.709424\n",
            "Preplexity of model 6 with data file 3 is 5.812776\n",
            "Preplexity of model 6 with data file 4 is 4.538289\n",
            "Preplexity of model 6 with data file 5 is 4.883616\n",
            "Preplexity of model 6 with data file 6 is 18.791985\n",
            "Preplexity of model 6 with data file 7 is 4.455842\n",
            "Preplexity of model 7 with data file 0 is 4.807779\n",
            "Preplexity of model 7 with data file 1 is 4.662008\n",
            "Preplexity of model 7 with data file 2 is 3.453037\n",
            "Preplexity of model 7 with data file 3 is 3.561006\n",
            "Preplexity of model 7 with data file 4 is 4.058835\n",
            "Preplexity of model 7 with data file 5 is 4.349980\n",
            "Preplexity of model 7 with data file 6 is 3.506606\n",
            "Preplexity of model 7 with data file 7 is 14.677763\n",
            "[[20.31757913  3.96930727  3.42589709  4.10437119  3.54247034  4.24403093\n",
            "   3.54247034  3.91653449]\n",
            " [ 4.16289649 21.92913836  3.15591007  3.88443505  3.96595871  4.55495074\n",
            "   3.5500935   3.99351172]\n",
            " [ 5.79834918  4.8823992  23.38674528  5.37176764  4.03334545  4.69937079\n",
            "   3.91940588  4.65469597]\n",
            " [ 4.98707205  4.44872708  3.82022777 17.78971423  3.87885824  4.09124215\n",
            "   4.31525497  3.51324695]\n",
            " [ 3.74524877  4.01627611  2.7733599   3.46819484 18.81054974  3.93297055\n",
            "   3.16707047  3.566487  ]\n",
            " [ 4.05986641  4.13964687  2.86011706  3.32031311  3.565891   22.21334449\n",
            "   3.0716578   3.45209027]\n",
            " [ 5.65509758  5.35245612  3.70942447  5.81277612  4.53828905  4.88361557\n",
            "  18.79198517  4.45584243]\n",
            " [ 4.80777855  4.6620078   3.45303724  3.56100613  4.05883527  4.34997951\n",
            "   3.50660616 14.67776322]]\n",
            "passed dataframe\n",
            "      en.csv     tl.csv     pt.csv  ...     in.csv     it.csv     nl.csv\n",
            "0  20.317579   3.969307   3.425897  ...   4.244031   3.542470   3.916534\n",
            "1   4.162896  21.929138   3.155910  ...   4.554951   3.550093   3.993512\n",
            "2   5.798349   4.882399  23.386745  ...   4.699371   3.919406   4.654696\n",
            "3   4.987072   4.448727   3.820228  ...   4.091242   4.315255   3.513247\n",
            "4   3.745249   4.016276   2.773360  ...   3.932971   3.167070   3.566487\n",
            "5   4.059866   4.139647   2.860117  ...  22.213344   3.071658   3.452090\n",
            "6   5.655098   5.352456   3.709424  ...   4.883616  18.791985   4.455842\n",
            "7   4.807779   4.662008   3.453037  ...   4.349980   3.506606  14.677763\n",
            "\n",
            "[8 rows x 8 columns]\n",
            "n = 2 add_one:1\n",
            "Preplexity of model 0 with data file 0 is 20.445154\n",
            "Preplexity of model 0 with data file 1 is 3.980697\n",
            "Preplexity of model 0 with data file 2 is 3.434677\n",
            "Preplexity of model 0 with data file 3 is 4.116435\n",
            "Preplexity of model 0 with data file 4 is 3.551796\n",
            "Preplexity of model 0 with data file 5 is 4.256801\n",
            "Preplexity of model 0 with data file 6 is 3.551796\n",
            "Preplexity of model 0 with data file 7 is 3.927664\n",
            "Preplexity of model 1 with data file 0 is 4.182805\n",
            "Preplexity of model 1 with data file 1 is 22.156827\n",
            "Preplexity of model 1 with data file 2 is 3.168067\n",
            "Preplexity of model 1 with data file 3 is 3.902108\n",
            "Preplexity of model 1 with data file 4 is 3.984279\n",
            "Preplexity of model 1 with data file 5 is 4.578112\n",
            "Preplexity of model 1 with data file 6 is 3.565172\n",
            "Preplexity of model 1 with data file 7 is 4.012053\n",
            "Preplexity of model 2 with data file 0 is 5.702043\n",
            "Preplexity of model 2 with data file 1 is 4.809179\n",
            "Preplexity of model 2 with data file 2 is 22.694687\n",
            "Preplexity of model 2 with data file 3 is 5.286395\n",
            "Preplexity of model 2 with data file 4 is 3.980098\n",
            "Preplexity of model 2 with data file 5 is 4.630581\n",
            "Preplexity of model 2 with data file 6 is 3.868719\n",
            "Preplexity of model 2 with data file 7 is 4.586978\n",
            "Preplexity of model 3 with data file 0 is 4.991599\n",
            "Preplexity of model 3 with data file 1 is 4.452478\n",
            "Preplexity of model 3 with data file 2 is 3.823120\n",
            "Preplexity of model 3 with data file 3 is 17.818655\n",
            "Preplexity of model 3 with data file 4 is 3.881828\n",
            "Preplexity of model 3 with data file 5 is 4.094498\n",
            "Preplexity of model 3 with data file 6 is 4.318819\n",
            "Preplexity of model 3 with data file 7 is 3.515741\n",
            "Preplexity of model 4 with data file 0 is 3.746698\n",
            "Preplexity of model 4 with data file 1 is 4.017913\n",
            "Preplexity of model 4 with data file 2 is 2.774189\n",
            "Preplexity of model 4 with data file 3 is 3.469459\n",
            "Preplexity of model 4 with data file 4 is 18.826733\n",
            "Preplexity of model 4 with data file 5 is 3.934549\n",
            "Preplexity of model 4 with data file 6 is 3.168141\n",
            "Preplexity of model 4 with data file 7 is 3.567816\n",
            "Preplexity of model 5 with data file 0 is 4.079665\n",
            "Preplexity of model 5 with data file 1 is 4.160115\n",
            "Preplexity of model 5 with data file 2 is 2.870571\n",
            "Preplexity of model 5 with data file 3 is 3.334176\n",
            "Preplexity of model 5 with data file 4 is 3.581667\n",
            "Preplexity of model 5 with data file 5 is 22.453772\n",
            "Preplexity of model 5 with data file 6 is 3.083649\n",
            "Preplexity of model 5 with data file 7 is 3.466972\n",
            "Preplexity of model 6 with data file 0 is 5.630193\n",
            "Preplexity of model 6 with data file 1 is 5.329631\n",
            "Preplexity of model 6 with data file 2 is 3.697058\n",
            "Preplexity of model 6 with data file 3 is 5.786772\n",
            "Preplexity of model 6 with data file 4 is 4.520836\n",
            "Preplexity of model 6 with data file 5 is 4.863926\n",
            "Preplexity of model 6 with data file 6 is 18.652079\n",
            "Preplexity of model 6 with data file 7 is 4.438914\n",
            "Preplexity of model 7 with data file 0 is 4.822273\n",
            "Preplexity of model 7 with data file 1 is 4.675787\n",
            "Preplexity of model 7 with data file 2 is 3.461250\n",
            "Preplexity of model 7 with data file 3 is 3.569687\n",
            "Preplexity of model 7 with data file 4 is 4.069750\n",
            "Preplexity of model 7 with data file 5 is 4.362257\n",
            "Preplexity of model 7 with data file 6 is 3.515051\n",
            "Preplexity of model 7 with data file 7 is 14.753546\n",
            "[[20.44515381  3.98069729  3.43467655  4.11643506  3.55179556  4.25680135\n",
            "   3.55179556  3.92766381]\n",
            " [ 4.182805   22.1568268   3.16806654  3.90210802  3.98427941  4.57811238\n",
            "   3.56517176  4.01205262]\n",
            " [ 5.70204268  4.80917922 22.69468719  5.28639455  3.98009774  4.63058133\n",
            "   3.86871871  4.58697798]\n",
            " [ 4.9915991   4.45247823  3.82312015 17.81865454  3.88182839  4.09449818\n",
            "   4.31881929  3.51574061]\n",
            " [ 3.74669835  4.01791287  2.77418907  3.46945905 18.82673261  3.93454919\n",
            "   3.1681406   3.56781626]\n",
            " [ 4.07966457  4.16011516  2.87057133  3.33417614  3.58166659 22.45377199\n",
            "   3.08364915  3.46697197]\n",
            " [ 5.63019299  5.32963105  3.69705779  5.78677171  4.52083585  4.86392555\n",
            "  18.65207912  4.43891361]\n",
            " [ 4.82227286  4.67578665  3.46125043  3.56968683  4.06975031  4.36225677\n",
            "   3.51505051 14.75354645]]\n",
            "passed dataframe\n",
            "      en.csv     tl.csv     pt.csv  ...     in.csv     it.csv     nl.csv\n",
            "0  20.445154   3.980697   3.434677  ...   4.256801   3.551796   3.927664\n",
            "1   4.182805  22.156827   3.168067  ...   4.578112   3.565172   4.012053\n",
            "2   5.702043   4.809179  22.694687  ...   4.630581   3.868719   4.586978\n",
            "3   4.991599   4.452478   3.823120  ...   4.094498   4.318819   3.515741\n",
            "4   3.746698   4.017913   2.774189  ...   3.934549   3.168141   3.567816\n",
            "5   4.079665   4.160115   2.870571  ...  22.453772   3.083649   3.466972\n",
            "6   5.630193   5.329631   3.697058  ...   4.863926  18.652079   4.438914\n",
            "7   4.822273   4.675787   3.461250  ...   4.362257   3.515051  14.753546\n",
            "\n",
            "[8 rows x 8 columns]\n",
            "n = 3 add_one:0\n",
            "Preplexity of model 0 with data file 0 is 12.441036\n",
            "Preplexity of model 0 with data file 1 is 5.297803\n",
            "Preplexity of model 0 with data file 2 is 4.570400\n",
            "Preplexity of model 0 with data file 3 is 5.063438\n",
            "Preplexity of model 0 with data file 4 is 4.981969\n",
            "Preplexity of model 0 with data file 5 is 5.270736\n",
            "Preplexity of model 0 with data file 6 is 5.033269\n",
            "Preplexity of model 0 with data file 7 is 5.453831\n",
            "Preplexity of model 1 with data file 0 is 4.228444\n",
            "Preplexity of model 1 with data file 1 is 9.092193\n",
            "Preplexity of model 1 with data file 2 is 3.774021\n",
            "Preplexity of model 1 with data file 3 is 4.057192\n",
            "Preplexity of model 1 with data file 4 is 4.024377\n",
            "Preplexity of model 1 with data file 5 is 4.209755\n",
            "Preplexity of model 1 with data file 6 is 4.057192\n",
            "Preplexity of model 1 with data file 7 is 4.256633\n",
            "Preplexity of model 2 with data file 0 is 6.233994\n",
            "Preplexity of model 2 with data file 1 is 6.356965\n",
            "Preplexity of model 2 with data file 2 is 14.454455\n",
            "Preplexity of model 2 with data file 3 is 7.348625\n",
            "Preplexity of model 2 with data file 4 is 6.509075\n",
            "Preplexity of model 2 with data file 5 is 6.214796\n",
            "Preplexity of model 2 with data file 6 is 6.383162\n",
            "Preplexity of model 2 with data file 7 is 6.569583\n",
            "Preplexity of model 3 with data file 0 is 4.298673\n",
            "Preplexity of model 3 with data file 1 is 4.288787\n",
            "Preplexity of model 3 with data file 2 is 4.432695\n",
            "Preplexity of model 3 with data file 3 is 9.840151\n",
            "Preplexity of model 3 with data file 4 is 4.246206\n",
            "Preplexity of model 3 with data file 5 is 4.136831\n",
            "Preplexity of model 3 with data file 6 is 4.321831\n",
            "Preplexity of model 3 with data file 7 is 4.301974\n",
            "Preplexity of model 4 with data file 0 is 5.022262\n",
            "Preplexity of model 4 with data file 1 is 5.043853\n",
            "Preplexity of model 4 with data file 2 is 4.774355\n",
            "Preplexity of model 4 with data file 3 is 5.035206\n",
            "Preplexity of model 4 with data file 4 is 12.513179\n",
            "Preplexity of model 4 with data file 5 is 5.000763\n",
            "Preplexity of model 4 with data file 6 is 5.139964\n",
            "Preplexity of model 4 with data file 7 is 5.314866\n",
            "Preplexity of model 5 with data file 0 is 4.858927\n",
            "Preplexity of model 5 with data file 1 is 4.858927\n",
            "Preplexity of model 5 with data file 2 is 4.232489\n",
            "Preplexity of model 5 with data file 3 is 4.490935\n",
            "Preplexity of model 5 with data file 4 is 4.586746\n",
            "Preplexity of model 5 with data file 5 is 10.979062\n",
            "Preplexity of model 5 with data file 6 is 4.575587\n",
            "Preplexity of model 5 with data file 7 is 4.862873\n",
            "Preplexity of model 6 with data file 0 is 6.154869\n",
            "Preplexity of model 6 with data file 1 is 6.178548\n",
            "Preplexity of model 6 with data file 2 is 5.645423\n",
            "Preplexity of model 6 with data file 3 is 6.238147\n",
            "Preplexity of model 6 with data file 4 is 6.244138\n",
            "Preplexity of model 6 with data file 5 is 6.037825\n",
            "Preplexity of model 6 with data file 6 is 13.719625\n",
            "Preplexity of model 6 with data file 7 is 6.148963\n",
            "Preplexity of model 7 with data file 0 is 4.665852\n",
            "Preplexity of model 7 with data file 1 is 4.576301\n",
            "Preplexity of model 7 with data file 2 is 4.134404\n",
            "Preplexity of model 7 with data file 3 is 4.364944\n",
            "Preplexity of model 7 with data file 4 is 4.523398\n",
            "Preplexity of model 7 with data file 5 is 4.526906\n",
            "Preplexity of model 7 with data file 6 is 4.334598\n",
            "Preplexity of model 7 with data file 7 is 10.113730\n",
            "[[12.44103637  5.29780279  4.57039961  5.06343765  4.9819689   5.2707355\n",
            "   5.033269    5.45383072]\n",
            " [ 4.22844378  9.09219311  3.77402103  4.05719202  4.02437727  4.20975489\n",
            "   4.05719202  4.25663278]\n",
            " [ 6.23399412  6.35696543 14.45445523  7.34862462  6.50907541  6.21479624\n",
            "   6.38316163  6.56958271]\n",
            " [ 4.29867343  4.28878674  4.4326946   9.84015085  4.2462065   4.13683079\n",
            "   4.3218311   4.30197406]\n",
            " [ 5.02226184  5.04385339  4.7743553   5.03520565 12.5131794   5.00076272\n",
            "   5.13996436  5.31486589]\n",
            " [ 4.85892653  4.85892653  4.23248937  4.4909346   4.58674616 10.97906203\n",
            "   4.57558748  4.86287321]\n",
            " [ 6.15486867  6.17854833  5.64542319  6.23814681  6.24413818  6.03782477\n",
            "  13.71962499  6.14896295]\n",
            " [ 4.66585181  4.57630117  4.13440446  4.36494445  4.5233979   4.52690568\n",
            "   4.33459811 10.11372994]]\n",
            "passed dataframe\n",
            "      en.csv    tl.csv     pt.csv  ...     in.csv     it.csv     nl.csv\n",
            "0  12.441036  5.297803   4.570400  ...   5.270736   5.033269   5.453831\n",
            "1   4.228444  9.092193   3.774021  ...   4.209755   4.057192   4.256633\n",
            "2   6.233994  6.356965  14.454455  ...   6.214796   6.383162   6.569583\n",
            "3   4.298673  4.288787   4.432695  ...   4.136831   4.321831   4.301974\n",
            "4   5.022262  5.043853   4.774355  ...   5.000763   5.139964   5.314866\n",
            "5   4.858927  4.858927   4.232489  ...  10.979062   4.575587   4.862873\n",
            "6   6.154869  6.178548   5.645423  ...   6.037825  13.719625   6.148963\n",
            "7   4.665852  4.576301   4.134404  ...   4.526906   4.334598  10.113730\n",
            "\n",
            "[8 rows x 8 columns]\n",
            "n = 3 add_one:1\n",
            "Preplexity of model 0 with data file 0 is 12.690150\n",
            "Preplexity of model 0 with data file 1 is 5.367725\n",
            "Preplexity of model 0 with data file 2 is 4.625346\n",
            "Preplexity of model 0 with data file 3 is 5.128441\n",
            "Preplexity of model 0 with data file 4 is 5.045283\n",
            "Preplexity of model 0 with data file 5 is 5.340085\n",
            "Preplexity of model 0 with data file 6 is 5.097646\n",
            "Preplexity of model 0 with data file 7 is 5.527074\n",
            "Preplexity of model 1 with data file 0 is 4.410655\n",
            "Preplexity of model 1 with data file 1 is 9.698846\n",
            "Preplexity of model 1 with data file 2 is 3.923576\n",
            "Preplexity of model 1 with data file 3 is 4.226907\n",
            "Preplexity of model 1 with data file 4 is 4.191724\n",
            "Preplexity of model 1 with data file 5 is 4.390592\n",
            "Preplexity of model 1 with data file 6 is 4.226907\n",
            "Preplexity of model 1 with data file 7 is 4.440922\n",
            "Preplexity of model 2 with data file 0 is 5.989781\n",
            "Preplexity of model 2 with data file 1 is 6.105330\n",
            "Preplexity of model 2 with data file 2 is 13.635485\n",
            "Preplexity of model 2 with data file 3 is 7.035429\n",
            "Preplexity of model 2 with data file 4 is 6.248192\n",
            "Preplexity of model 2 with data file 5 is 5.971738\n",
            "Preplexity of model 2 with data file 6 is 6.129939\n",
            "Preplexity of model 2 with data file 7 is 6.305000\n",
            "Preplexity of model 3 with data file 0 is 4.376986\n",
            "Preplexity of model 3 with data file 1 is 4.366794\n",
            "Preplexity of model 3 with data file 2 is 4.515164\n",
            "Preplexity of model 3 with data file 3 is 10.122670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW",
        "colab_type": "text"
      },
      "source": [
        "# **Good luck!**\n",
        "max_n = 4\n",
        "for n = 1 in range(max_n):\n",
        "  data = match(n,False)\n",
        "  print(data)\n",
        "  data = match(n,True)\n",
        "  print(data)\n",
        "  "
      ]
    }
  ]
}