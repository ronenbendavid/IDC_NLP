{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ronen_Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronenbendavid/IDC_NLP/blob/master/Ronen_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM",
        "colab_type": "text"
      },
      "source": [
        "*As a preparation for this task, place the data files somewhere in your drive so that you can access the files from this notebook. The files are available to download from the Moodle assignment activity*\n",
        "\n",
        "The relevant files are:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGtqPrCPnLUn",
        "colab_type": "code",
        "outputId": "4410dec4-c311-4c17-c262-2df88da506cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfzsITW8Yaj",
        "colab_type": "code",
        "outputId": "f96ca62b-935f-4c04-e793-27f66eb35ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/IDC/NLP/Assignment1/')\n",
        "def preprocess():\n",
        "  # TODO\n",
        " vocabulary = set()\n",
        " pathlist = glob('*.csv')\n",
        " for path in pathlist:\n",
        "    with open(path, encoding='utf-8', newline='') as csv_file:\n",
        "        csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "        for text in csv_reader['tweet_text']:\n",
        "            list_text = list(text)\n",
        "            vocabulary.update(list_text)\n",
        "            line_count += 1\n",
        "        #print(line_count)\n",
        " return sorted(list(vocabulary))\n",
        "\n",
        "vocabulary = preprocess()\n",
        "print(len(vocabulary))\n",
        "print(vocabulary)\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1859\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x97', '\\x9d', 'Â¡', 'Â£', 'Â¤', 'Â¥', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', '\\xad', 'Â®', 'Â¯', 'Â°', 'Â²', 'Â³', 'Â´', 'Â¶', 'Â·', 'Â¸', 'Âº', 'Â»', 'Â½', 'Â¾', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã…', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'ÃŽ', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã™', 'Ãš', 'Ãœ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ä—', 'Äž', 'ÄŸ', 'Ä°', 'Ä±', 'Å„', 'Å', 'Å’', 'Å“', 'Åž', 'ÅŸ', 'Å ', 'Å¸', 'Æ’', 'Ê”', 'Ê•', 'Ê–', 'Ê°', 'Ê³', 'Ê·', 'Ê¸', 'Ë', 'Ë–', 'Ë˜', 'Ëš', 'Ë›', 'Ë¡', 'Ë¢', 'Ì€', 'Ì', 'Ìƒ', 'Ìˆ', 'Ì¥', 'Ì®', 'Ì¯', 'Íœ', 'Í¡', 'Î”', 'Î˜', 'Î©', 'Ï…', 'Ï‰', 'Ð', 'Ð˜', 'Ðœ', 'Ð', 'Ðž', 'ÐŸ', 'Ð ', 'Ð¤', 'Ð¦', 'Ð¯', 'Ð°', 'Ð±', 'Ð²', 'Ð³', 'Ð´', 'Ðµ', 'Ð·', 'Ð¸', 'Ðº', 'Ð»', 'Ð¼', 'Ð½', 'Ð¾', 'Ð¿', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ…', 'Ñ‹', 'Ñ', 'ÑŽ', 'Ñ', 'Ò’', 'Ò¯', 'ØŒ', 'Ø¢', 'Ø¦', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Ø¶', 'Ø·', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'ÛŒ', 'Û¶', 'à¤‚', 'à¤•', 'à¤—', 'à¤ª', 'à¤¬', 'à¤°', 'à¤¸', 'à¤¾', 'à¥‡', 'à¥', 'à³ƒ', 'à·´', 'à¸', 'à¸‚', 'à¸‡', 'à¸ˆ', 'à¸', 'à¸”', 'à¸•', 'à¸–', 'à¸—', 'à¸™', 'à¸š', 'à¸›', 'à¸ž', 'à¸ ', 'à¸¡', 'à¸¢', 'à¸£', 'à¸¥', 'à¸§', 'à¸¨', 'à¸ª', 'à¸­', 'à¸°', 'à¸±', 'à¸²', 'à¸´', 'à¸µ', 'à¸¸', 'à¸¹', 'à¹€', 'à¹', 'à¹ˆ', 'à¹‰', 'à¹', 'à¹‘', 'àº¶', 'à¼Ž', 'à¼º', 'à¼»', 'à¼¼', 'à¼½', 'áƒ¦', 'á™“', 'á´—', 'á´¬', 'á´°', 'áµƒ', 'áµ‡', 'áµˆ', 'áµ‰', 'áµ', 'áµ', 'áµ’', 'áµ–', 'áµ—', 'áµ˜', 'áµ›', 'á¶œ', 'á¶ ', 'á¶¦', 'á¶°', '\\u2009', '\\u200a', '\\u200b', '\\u200d', 'â€“', 'â€”', 'â€•', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€ž', 'â€ ', 'â€¢', 'â€¤', 'â€¦', 'â€°', 'â€²', 'â€¹', 'â€º', 'â€»', 'â€¼', 'â€¿', 'â‰', '\\u2066', '\\u2067', '\\u2069', 'â±', 'â·', 'â¿', 'â‚¬', 'â‚¹', 'âƒ£', 'â„ƒ', 'â„…', 'â„¢', 'â… ', 'â…¡', 'â…¢', 'â…£', 'â…¤', 'â†', 'â†‘', 'â†’', 'â†“', 'â†”', 'â†•', 'â†—', 'â†˜', 'â†š', 'â†›', 'â†©', 'â†ª', 'â†¯', 'â†º', 'â‡˜', 'â‡¨', 'âˆ€', 'âˆ†', 'âˆ‡', 'âˆš', 'âˆž', 'âˆ´', 'âˆµ', 'â‰¤', 'â‰¥', 'â‰¦', 'â‰§', 'âŠ™', 'â‹…', 'â‹ª', 'â‹­', 'âŒš', 'âŒ›', 'âŒ£', 'âŽ‹', 'â©', 'â°', 'â±', 'â³', 'â¸', 'â‘ ', 'â‘¥', 'â’ˆ', 'â’‰', 'â’Š', 'â’‹', 'â’Œ', 'â’', 'â’Ž', 'â’', 'â’', 'â’‘', 'â“‚', 'â“˜', 'â“™', 'â“¢', 'â“¦', 'â”€', 'â”', 'â”ƒ', 'â”„', 'â”†', 'â”Œ', 'â”', 'â”', 'â”“', 'â””', 'â”—', 'â”˜', 'â”›', 'â”³', 'â”»', 'â•‘', 'â•”', 'â•—', 'â•š', 'â•', 'â•¦', 'â•©', 'â•¬', 'â•­', 'â•®', 'â•¯', 'â•°', 'â•±', 'â•²', 'â•´', 'â–ˆ', 'â–Š', 'â–', 'â–’', 'â–”', 'â–•', 'â–™', 'â–', 'â–£', 'â–¦', 'â–ª', 'â–²', 'â–³', 'â–¶', 'â–¸', 'â–º', 'â–¼', 'â–½', 'â–¿', 'â—€', 'â—', 'â—„', 'â—†', 'â—‡', 'â—ˆ', 'â—‹', 'â—Ž', 'â—', 'â—‘', 'â—•', 'â—¡', 'â—»', 'â—¼', 'â—½', 'â—¾', 'â˜€', 'â˜', 'â˜ƒ', 'â˜„', 'â˜…', 'â˜†', 'â˜‰', 'â˜Ž', 'â˜‘', 'â˜“', 'â˜”', 'â˜•', 'â˜˜', 'â˜™', 'â˜š', 'â˜›', 'â˜œ', 'â˜', 'â˜ž', 'â˜ ', 'â˜£', 'â˜ª', 'â˜®', 'â˜¯', 'â˜°', 'â˜¹', 'â˜º', 'â˜¼', 'â˜½', 'â˜¾', 'â™€', 'â™‚', 'â™Š', 'â™‹', 'â™', 'â™Ž', 'â™', 'â™', 'â™“', 'â™›', 'â™¡', 'â™£', 'â™¤', 'â™¥', 'â™¦', 'â™©', 'â™ª', 'â™«', 'â™¬', 'â™¯', 'â™»', 'âš’', 'âš“', 'âš”', 'âš•', 'âš–', 'âš˜', 'âšœ', 'âš', 'âš ', 'âš¡', 'âšª', 'âš«', 'âš°', 'âš½', 'âš¾', 'â›„', 'â›…', 'â›ˆ', 'â›“', 'â›”', 'â›©', 'â›ª', 'â›³', 'â›·', 'â›½', 'âœ', 'âœ‚', 'âœƒ', 'âœ…', 'âœˆ', 'âœ‰', 'âœŠ', 'âœ‹', 'âœŒ', 'âœ', 'âœ', 'âœ“', 'âœ”', 'âœ–', 'âœ', 'âœ¡', 'âœ§', 'âœ¨', 'âœ©', 'âœª', 'âœ­', 'âœ°', 'âœ³', 'âœ´', 'âœµ', 'âœ¶', 'âœ·', 'âœ¿', 'â€', 'â', 'â„', 'â…', 'âˆ', 'â‹', 'âŒ', 'âŽ', 'â“', 'â”', 'â—', 'â', 'âž', 'â£', 'â¤', 'â¥', 'âžŠ', 'âž‹', 'âžŒ', 'âž', 'âžŽ', 'âž', 'âž”', 'âž–', 'âž—', 'âž™', 'âž›', 'âžœ', 'âžž', 'âžŸ', 'âž ', 'âž¡', 'âž¢', 'âž¤', 'âž°', 'âžµ', 'â €', 'â¤µ', 'â¦‘', 'â¦’', 'â¬…', 'â¬‡', 'â­', 'â¸„', 'â¸…', '\\u3000', 'ã€', 'ã€‚', 'ã€†', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€', 'ã€Ž', 'ã€', 'ã€', 'ã€‘', 'ã€œ', 'ã€¡', 'ã€°', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ãŒ', 'ã', 'ã', 'ã—', 'ã›', 'ãœ', 'ãŸ', 'ã£', 'ã¥', 'ã¦', 'ã§', 'ã¨', 'ãª', 'ã«', 'ã­', 'ã®', 'ã¯', 'ã²', 'ã¿', 'ã‚€', 'ã‚‡', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚Œ', 'ã‚', 'ã‚’', 'ã‚œ', 'ã‚¤', 'ã‚§', 'ã‚¨', 'ã‚©', 'ã‚ª', 'ã‚«', 'ã‚­', 'ã‚¯', 'ã‚°', 'ã‚³', 'ã‚´', 'ã‚µ', 'ã‚¸', 'ã‚¹', 'ã‚»', 'ã‚¿', 'ãƒ', 'ãƒƒ', 'ãƒ„', 'ãƒ†', 'ãƒˆ', 'ãƒ‰', 'ãƒŠ', 'ãƒ‹', 'ãƒ', 'ãƒŽ', 'ãƒ‘', 'ãƒ’', 'ãƒ“', 'ãƒ”', 'ãƒ•', 'ãƒ–', 'ãƒ—', 'ãƒ', 'ãƒ ', 'ãƒ¡', 'ãƒ¥', 'ãƒ¦', 'ãƒ§', 'ãƒ©', 'ãƒ¬', 'ãƒ­', 'ãƒ®', 'ãƒ¯', 'ãƒ³', 'ãƒ»', 'ãƒ¼', 'ãƒ½', 'ã……', 'ã…ˆ', 'ã…‹', 'ã…', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£', 'ã…¤', '\\u31ef', 'ä¸–', 'ä¸­', 'ä¸»', 'äº’', 'äºº', 'ä»˜', 'ä¼š', 'åƒ', 'å„¿', 'å…', 'å…ˆ', 'å…¥', 'å†™', 'åˆ†', 'åˆ©', 'åˆ¶', 'åˆ¹', 'åŠ›', 'åŠª', 'å‹•', 'åˆ', 'å’', 'å—', 'åˆ', 'å‘Ÿ', 'å˜‰', 'å¢—', 'å¥½', 'å§¿', 'å«Œ', 'å­¦', 'å°”', 'å¸Œ', 'å½¡', 'å½±', 'å½¼', 'å¾Œ', 'æ‚ª', 'æ‰‹', 'æŠ•', 'æ‹¶', 'æŒ¨', 'æ’ƒ', 'æ’®', 'æ–‡', 'æ˜ ', 'æ™‚', 'æ›²', 'æœˆ', 'æœ‰', 'æœ', 'æœ¬', 'æž—', 'æŸ±', 'æ¥­', 'æ©Ÿ', 'æ­Œ', 'æ­³', 'æ¯…', 'æ°—', 'æ´²', 'æ´¸', 'çŽ‹', 'ç”Ÿ', 'ç”¨', 'ç”»', 'ç•Œ', 'ç›¸', 'çœŸ', 'çž¬', 'çŸ¥', 'ç¤¾', 'ç¨¿', 'ç©º', 'ç³Ÿ', 'çµ‚', 'çµ', 'ç¹‹', 'è€…', 'èŠ±', 'èœ', 'è¡Œ', 'è®¸', 'èµ«', 'è¸Š', 'è¾¼', 'é€š', 'é‚£', 'é–“', 'é™', 'é¢¨', 'é­', 'ê Ž', 'ê°€', 'ê°„', 'ê°“', 'ê°•', 'ê±¸', 'ê²€', 'ê²Œ', 'ê²©', 'ê²°', 'ê²½', 'ê³ ', 'ê³¡', 'ê³¼', 'êµ¬', 'êµ­', 'ê·œ', 'ê·¸', 'ê·¼', 'ê¸ˆ', 'ê¸°', 'ê¹€', 'êº½', 'ê¼¼', 'ë‚˜', 'ë‚ ', 'ë‚¨', 'ë‚´', 'ë„ˆ', 'ë„', 'ë„¤', 'ë„·', 'ë…€', 'ë…„', 'ë…¸', 'ë…¼', 'ëˆ„', 'ëŠ”', 'ëŠ˜', 'ë‹ˆ', 'ë‹¤', 'ë‹¨', 'ë‹¹', 'ë‹¿', 'ëŒ€', 'ë”', 'ë„', 'ë™', 'ë‘', 'ë‘‘', 'ë“€', 'ë“œ', 'ë“±', 'ë””', 'ë¼', 'ë½', 'ëž‘', 'ëž™', 'ëžœ', 'ëž¨', 'ëŸ¬', 'ëŸ°', 'ë ˆ', 'ë ›', 'ë¡œ', 'ë¡±', 'ë£Œ', 'ë£¨', 'ë£°', 'ë£¸', 'ë¥¼', 'ë¦„', 'ë¦‰', 'ë¦¬', 'ë¦¼', 'ë§', 'ë§ˆ', 'ë§ž', 'ë§¤', 'ë§¨', 'ëª¬', 'ë¬´', 'ë¯¸', 'ë¯¼', 'ë°€', 'ë°”', 'ë°•', 'ë°©', 'ë°°', 'ë°±', 'ë±€', 'ë²„', 'ë²…', 'ë²•', 'ë² ', 'ë²¨', 'ë²³', 'ë³´', 'ë³µ', 'ë³¸', 'ë´„', 'ë´‰', 'ë·”', 'ë¸Œ', 'ë¸', 'ë¸”', 'ë¹„', 'ë¹…', 'ë¹¼', 'ì‚¬', 'ì‚´', 'ì‚¼', 'ìƒ', 'ìƒ', 'ìƒ¤', 'ìƒµ', 'ì„œ', 'ì„', 'ì„ ', 'ì„±', 'ì„¸', 'ì„¹', 'ì…”', 'ì…˜', 'ì…©', 'ì†Œ', 'ì†¡', 'ìˆ˜', 'ìŠˆ', 'ìŠ¤', 'ìŠ¨', 'ìŠ¬', 'ìŠ¹', 'ì‹œ', 'ì‹', 'ì‹ ', 'ì‹¤', 'ì‹¸', 'ì•„', 'ì•ˆ', 'ì••', 'ì• ', 'ì•¼', 'ì–‘', 'ì–´', 'ì—', 'ì—‘', 'ì—˜', 'ì— ', 'ì—£', 'ì—¬', 'ì—­', 'ì—°', 'ì˜', 'ì˜ˆ', 'ì˜¤', 'ì˜¨', 'ì™€', 'ì™•', 'ì™¸', 'ìš”', 'ìš©', 'ìš°', 'ìš¸', 'ì›Œ', 'ì›', 'ìœ„', 'ìœ ', 'ìœ¤', 'ì˜', 'ì´', 'ì¸', 'ì¼', 'ìž„', 'ìž˜', 'ìž¥', 'ìž¬', 'ìž­', 'ì „', 'ì •', 'ì œ', 'ì ¤', 'ì¢…', 'ì£¼', 'ì¥”', 'ì¦ˆ', 'ì§€', 'ì§', 'ì§„', 'ì§‘', 'ì©œ', 'ì°Œ', 'ì°°', 'ì±„', 'ì²œ', 'ì² ', 'ì´ˆ', 'ìµœ', 'ì¶”', 'ì¶œ', 'ì¸ ', 'ì¹˜', 'ì¹´', 'ì»¤', 'ì½”', 'ì½˜', 'ì½¤', 'ì¿±', 'í¬', 'í‚¤', 'í‚¹', 'íƒ€', 'íƒ„', 'íƒ‘', 'íƒœ', 'í„°', 'í…', 'í† ', 'í†¡', 'íŠ¸', 'í‹°', 'í‹´', 'íŒ', 'íŒŒ', 'íŒ¨', 'íŽ€', 'í¬', 'í’€', 'í”„', 'í”Œ', 'í”¼', 'í•‘', 'í•˜', 'í•œ', 'í•´', 'í•¸', 'í—Œ', 'í—¤', 'í—¨', 'í˜', 'í˜„', 'í˜•', 'í˜¸', 'í™”', 'í™˜', 'í›ˆ', 'íž', 'ï·»', 'ï¸Ž', 'ï¸', 'ï¸µ', 'ï¹', 'ï¹ª', 'ï¼', 'ï¼‚', 'ï¼ƒ', 'ï¼ˆ', 'ï¼‰', 'ï¼Š', 'ï¼Ž', 'ï¼“', 'ï¼–', 'ï¼—', 'ï¼˜', 'ï¼Ÿ', 'ï¼ ', 'ï¼¡', 'ï¼¢', 'ï¼£', 'ï¼¤', 'ï¼¥', 'ï¼¦', 'ï¼§', 'ï¼¨', 'ï¼©', 'ï¼«', 'ï¼¬', 'ï¼­', 'ï¼®', 'ï¼¯', 'ï¼°', 'ï¼±', 'ï¼²', 'ï¼³', 'ï¼´', 'ï¼µ', 'ï¼¶', 'ï¼·', 'ï¼¹', 'ï¼»', 'ï¼½', 'ï¼¿', 'ï½€', 'ï½', 'ï½‚', 'ï½ƒ', 'ï½„', 'ï½…', 'ï½‡', 'ï½‰', 'ï½Œ', 'ï½', 'ï½Ž', 'ï½', 'ï½', 'ï½‘', 'ï½’', 'ï½“', 'ï½”', 'ï½•', 'ï½–', 'ï½—', 'ï½˜', 'ï½™', 'ï½š', 'ï½œ', 'ï½¡', 'ï½¥', 'ï¾‰', 'ï¾’', 'ï¿£', 'ï¿¼', 'ï¿½', 'ðŸƒ', 'ðŸ…°', 'ðŸ…±', 'ðŸ…¾', 'ðŸ…¿', 'ðŸ†‘', 'ðŸ†’', 'ðŸ†“', 'ðŸ†”', 'ðŸ†•', 'ðŸ†–', 'ðŸ†—', 'ðŸ†˜', 'ðŸ†™', 'ðŸ†š', 'ðŸ‡¦', 'ðŸ‡§', 'ðŸ‡¨', 'ðŸ‡©', 'ðŸ‡ª', 'ðŸ‡«', 'ðŸ‡¬', 'ðŸ‡­', 'ðŸ‡®', 'ðŸ‡¯', 'ðŸ‡°', 'ðŸ‡±', 'ðŸ‡²', 'ðŸ‡³', 'ðŸ‡´', 'ðŸ‡µ', 'ðŸ‡·', 'ðŸ‡¸', 'ðŸ‡¹', 'ðŸ‡º', 'ðŸ‡»', 'ðŸ‡¼', 'ðŸ‡½', 'ðŸ‡¾', 'ðŸ‡¿', 'ðŸˆ´', 'ðŸˆµ', 'ðŸˆ¶', 'ðŸˆ·', 'ðŸŒ€', 'ðŸŒƒ', 'ðŸŒ„', 'ðŸŒ…', 'ðŸŒ†', 'ðŸŒ‡', 'ðŸŒˆ', 'ðŸŒŠ', 'ðŸŒ‹', 'ðŸŒŒ', 'ðŸŒ', 'ðŸŒŽ', 'ðŸŒ', 'ðŸŒ', 'ðŸŒ’', 'ðŸŒ“', 'ðŸŒ—', 'ðŸŒ™', 'ðŸŒš', 'ðŸŒ›', 'ðŸŒœ', 'ðŸŒ', 'ðŸŒž', 'ðŸŒŸ', 'ðŸŒ ', 'ðŸŒ¤', 'ðŸŒ¥', 'ðŸŒ§', 'ðŸŒ¨', 'ðŸŒª', 'ðŸŒ«', 'ðŸŒ¬', 'ðŸŒ­', 'ðŸŒ®', 'ðŸŒ¯', 'ðŸŒ°', 'ðŸŒ±', 'ðŸŒ²', 'ðŸŒ³', 'ðŸŒ´', 'ðŸŒµ', 'ðŸŒ¶', 'ðŸŒ·', 'ðŸŒ¸', 'ðŸŒ¹', 'ðŸŒº', 'ðŸŒ»', 'ðŸŒ¼', 'ðŸŒ½', 'ðŸŒ¾', 'ðŸŒ¿', 'ðŸ€', 'ðŸ', 'ðŸ‚', 'ðŸƒ', 'ðŸ…', 'ðŸ†', 'ðŸ‡', 'ðŸ‰', 'ðŸŠ', 'ðŸ‹', 'ðŸŒ', 'ðŸ', 'ðŸŽ', 'ðŸ', 'ðŸ‘', 'ðŸ’', 'ðŸ“', 'ðŸ”', 'ðŸ•', 'ðŸ–', 'ðŸ—', 'ðŸš', 'ðŸ›', 'ðŸœ', 'ðŸ', 'ðŸž', 'ðŸŸ', 'ðŸ£', 'ðŸ¤', 'ðŸ¥', 'ðŸ¦', 'ðŸ¨', 'ðŸ©', 'ðŸª', 'ðŸ«', 'ðŸ¬', 'ðŸ­', 'ðŸ¯', 'ðŸ°', 'ðŸ±', 'ðŸ³', 'ðŸ´', 'ðŸµ', 'ðŸ¶', 'ðŸ·', 'ðŸ¸', 'ðŸ¹', 'ðŸº', 'ðŸ»', 'ðŸ¼', 'ðŸ½', 'ðŸ¾', 'ðŸ¿', 'ðŸŽ€', 'ðŸŽ', 'ðŸŽ‚', 'ðŸŽ…', 'ðŸŽ†', 'ðŸŽ‡', 'ðŸŽˆ', 'ðŸŽ‰', 'ðŸŽŠ', 'ðŸŽ‹', 'ðŸŽ', 'ðŸŽ’', 'ðŸŽ“', 'ðŸŽ—', 'ðŸŽ™', 'ðŸŽž', 'ðŸŽŸ', 'ðŸŽ ', 'ðŸŽ¡', 'ðŸŽ¢', 'ðŸŽ¤', 'ðŸŽ¥', 'ðŸŽ¦', 'ðŸŽ§', 'ðŸŽ¨', 'ðŸŽ©', 'ðŸŽª', 'ðŸŽ«', 'ðŸŽ¬', 'ðŸŽ­', 'ðŸŽ®', 'ðŸŽ¯', 'ðŸŽ°', 'ðŸŽ±', 'ðŸŽ²', 'ðŸŽµ', 'ðŸŽ¶', 'ðŸŽ·', 'ðŸŽ¸', 'ðŸŽ¹', 'ðŸŽº', 'ðŸŽ»', 'ðŸŽ¼', 'ðŸŽ¾', 'ðŸŽ¿', 'ðŸ€', 'ðŸ', 'ðŸƒ', 'ðŸ„', 'ðŸ…', 'ðŸ†', 'ðŸ‡', 'ðŸˆ', 'ðŸ‰', 'ðŸŠ', 'ðŸ‹', 'ðŸŒ', 'ðŸ’', 'ðŸ“', 'ðŸ”', 'ðŸ–', 'ðŸ˜', 'ðŸ™', 'ðŸš', 'ðŸŸ', 'ðŸ ', 'ðŸ¡', 'ðŸ¢', 'ðŸ©', 'ðŸ«', 'ðŸ°', 'ðŸ³', 'ðŸ´', 'ðŸ¹', 'ðŸ»', 'ðŸ¼', 'ðŸ½', 'ðŸ¾', 'ðŸ¿', 'ðŸ€', 'ðŸ', 'ðŸ‚', 'ðŸ„', 'ðŸ†', 'ðŸ‡', 'ðŸˆ', 'ðŸ‰', 'ðŸŠ', 'ðŸ', 'ðŸŽ', 'ðŸ', 'ðŸ‘', 'ðŸ’', 'ðŸ“', 'ðŸ”', 'ðŸ•', 'ðŸ–', 'ðŸ˜', 'ðŸ™', 'ðŸš', 'ðŸœ', 'ðŸ', 'ðŸž', 'ðŸŸ', 'ðŸ ', 'ðŸ¡', 'ðŸ¢', 'ðŸ£', 'ðŸ¥', 'ðŸ¦', 'ðŸ§', 'ðŸ¨', 'ðŸ©', 'ðŸ«', 'ðŸ¬', 'ðŸ­', 'ðŸ®', 'ðŸ¯', 'ðŸ°', 'ðŸ±', 'ðŸ²', 'ðŸ³', 'ðŸ´', 'ðŸ¶', 'ðŸ·', 'ðŸ¸', 'ðŸ¹', 'ðŸº', 'ðŸ»', 'ðŸ¼', 'ðŸ½', 'ðŸ¾', 'ðŸ¿', 'ðŸ‘€', 'ðŸ‘', 'ðŸ‘‚', 'ðŸ‘„', 'ðŸ‘…', 'ðŸ‘†', 'ðŸ‘‡', 'ðŸ‘ˆ', 'ðŸ‘‰', 'ðŸ‘Š', 'ðŸ‘‹', 'ðŸ‘Œ', 'ðŸ‘', 'ðŸ‘Ž', 'ðŸ‘', 'ðŸ‘', 'ðŸ‘‘', 'ðŸ‘“', 'ðŸ‘•', 'ðŸ‘–', 'ðŸ‘—', 'ðŸ‘™', 'ðŸ‘ž', 'ðŸ‘Ÿ', 'ðŸ‘ ', 'ðŸ‘¡', 'ðŸ‘£', 'ðŸ‘¤', 'ðŸ‘¥', 'ðŸ‘¦', 'ðŸ‘§', 'ðŸ‘¨', 'ðŸ‘©', 'ðŸ‘ª', 'ðŸ‘«', 'ðŸ‘­', 'ðŸ‘®', 'ðŸ‘¯', 'ðŸ‘°', 'ðŸ‘±', 'ðŸ‘²', 'ðŸ‘³', 'ðŸ‘µ', 'ðŸ‘¶', 'ðŸ‘·', 'ðŸ‘¸', 'ðŸ‘¹', 'ðŸ‘º', 'ðŸ‘»', 'ðŸ‘¼', 'ðŸ‘½', 'ðŸ‘¿', 'ðŸ’€', 'ðŸ’', 'ðŸ’‚', 'ðŸ’ƒ', 'ðŸ’„', 'ðŸ’…', 'ðŸ’†', 'ðŸ’ˆ', 'ðŸ’‰', 'ðŸ’Š', 'ðŸ’‹', 'ðŸ’Œ', 'ðŸ’', 'ðŸ’Ž', 'ðŸ’', 'ðŸ’', 'ðŸ’‘', 'ðŸ’’', 'ðŸ’“', 'ðŸ’”', 'ðŸ’•', 'ðŸ’–', 'ðŸ’—', 'ðŸ’˜', 'ðŸ’™', 'ðŸ’š', 'ðŸ’›', 'ðŸ’œ', 'ðŸ’', 'ðŸ’ž', 'ðŸ’Ÿ', 'ðŸ’¡', 'ðŸ’¢', 'ðŸ’£', 'ðŸ’¤', 'ðŸ’¥', 'ðŸ’¦', 'ðŸ’§', 'ðŸ’¨', 'ðŸ’©', 'ðŸ’ª', 'ðŸ’«', 'ðŸ’¬', 'ðŸ’­', 'ðŸ’®', 'ðŸ’¯', 'ðŸ’°', 'ðŸ’²', 'ðŸ’³', 'ðŸ’µ', 'ðŸ’¶', 'ðŸ’¸', 'ðŸ’»', 'ðŸ’¼', 'ðŸ’½', 'ðŸ’¿', 'ðŸ“€', 'ðŸ“‚', 'ðŸ“…', 'ðŸ“†', 'ðŸ“ˆ', 'ðŸ“Š', 'ðŸ“‹', 'ðŸ“Œ', 'ðŸ“', 'ðŸ“', 'ðŸ““', 'ðŸ“–', 'ðŸ“š', 'ðŸ“›', 'ðŸ“', 'ðŸ“ž', 'ðŸ“¡', 'ðŸ“¢', 'ðŸ“£', 'ðŸ“¦', 'ðŸ“§', 'ðŸ“©', 'ðŸ“¬', 'ðŸ“¯', 'ðŸ“°', 'ðŸ“±', 'ðŸ“²', 'ðŸ“´', 'ðŸ“·', 'ðŸ“¸', 'ðŸ“¹', 'ðŸ“º', 'ðŸ“»', 'ðŸ“¼', 'ðŸ“½', 'ðŸ“¿', 'ðŸ”', 'ðŸ”‚', 'ðŸ”ƒ', 'ðŸ”„', 'ðŸ”…', 'ðŸ”‰', 'ðŸ”Š', 'ðŸ”‹', 'ðŸ”Œ', 'ðŸ”', 'ðŸ”‘', 'ðŸ”’', 'ðŸ”“', 'ðŸ””', 'ðŸ”˜', 'ðŸ”™', 'ðŸ”›', 'ðŸ”œ', 'ðŸ”', 'ðŸ”ž', 'ðŸ”¥', 'ðŸ”¨', 'ðŸ”©', 'ðŸ”ª', 'ðŸ”«', 'ðŸ”®', 'ðŸ”°', 'ðŸ”±', 'ðŸ”²', 'ðŸ”´', 'ðŸ”µ', 'ðŸ”¶', 'ðŸ”¸', 'ðŸ”¹', 'ðŸ”º', 'ðŸ”»', 'ðŸ”¼', 'ðŸ”½', 'ðŸ•Š', 'ðŸ•‹', 'ðŸ•Œ', 'ðŸ•Ž', 'ðŸ•', 'ðŸ•’', 'ðŸ•˜', 'ðŸ•›', 'ðŸ•œ', 'ðŸ•Ÿ', 'ðŸ•¤', 'ðŸ•ª', 'ðŸ•¯', 'ðŸ•µ', 'ðŸ•¶', 'ðŸ•·', 'ðŸ•º', 'ðŸ–', 'ðŸ–’', 'ðŸ–•', 'ðŸ––', 'ðŸ–¤', 'ðŸ–¥', 'ðŸ–²', 'ðŸ–¼', 'ðŸ—‚', 'ðŸ—“', 'ðŸ—', 'ðŸ—ž', 'ðŸ—¡', 'ðŸ—£', 'ðŸ—¨', 'ðŸ—³', 'ðŸ—»', 'ðŸ—¼', 'ðŸ—½', 'ðŸ—¾', 'ðŸ˜€', 'ðŸ˜', 'ðŸ˜‚', 'ðŸ˜ƒ', 'ðŸ˜„', 'ðŸ˜…', 'ðŸ˜†', 'ðŸ˜‡', 'ðŸ˜ˆ', 'ðŸ˜‰', 'ðŸ˜Š', 'ðŸ˜‹', 'ðŸ˜Œ', 'ðŸ˜', 'ðŸ˜Ž', 'ðŸ˜', 'ðŸ˜', 'ðŸ˜‘', 'ðŸ˜’', 'ðŸ˜“', 'ðŸ˜”', 'ðŸ˜•', 'ðŸ˜–', 'ðŸ˜—', 'ðŸ˜˜', 'ðŸ˜™', 'ðŸ˜š', 'ðŸ˜›', 'ðŸ˜œ', 'ðŸ˜', 'ðŸ˜ž', 'ðŸ˜Ÿ', 'ðŸ˜ ', 'ðŸ˜¡', 'ðŸ˜¢', 'ðŸ˜£', 'ðŸ˜¤', 'ðŸ˜¥', 'ðŸ˜¦', 'ðŸ˜§', 'ðŸ˜¨', 'ðŸ˜©', 'ðŸ˜ª', 'ðŸ˜«', 'ðŸ˜¬', 'ðŸ˜­', 'ðŸ˜®', 'ðŸ˜¯', 'ðŸ˜°', 'ðŸ˜±', 'ðŸ˜²', 'ðŸ˜³', 'ðŸ˜´', 'ðŸ˜µ', 'ðŸ˜¶', 'ðŸ˜·', 'ðŸ˜¸', 'ðŸ˜¹', 'ðŸ˜º', 'ðŸ˜»', 'ðŸ˜¼', 'ðŸ˜½', 'ðŸ˜¿', 'ðŸ™€', 'ðŸ™', 'ðŸ™‚', 'ðŸ™ƒ', 'ðŸ™„', 'ðŸ™…', 'ðŸ™†', 'ðŸ™‡', 'ðŸ™ˆ', 'ðŸ™‰', 'ðŸ™Š', 'ðŸ™‹', 'ðŸ™Œ', 'ðŸ™', 'ðŸ™Ž', 'ðŸ™', 'ðŸš€', 'ðŸš', 'ðŸš‡', 'ðŸšˆ', 'ðŸšŒ', 'ðŸš‘', 'ðŸš“', 'ðŸš”', 'ðŸš–', 'ðŸš—', 'ðŸš˜', 'ðŸš™', 'ðŸš¢', 'ðŸš£', 'ðŸš¦', 'ðŸš§', 'ðŸš¨', 'ðŸš©', 'ðŸš«', 'ðŸš¬', 'ðŸš®', 'ðŸš²', 'ðŸš´', 'ðŸšµ', 'ðŸš¶', 'ðŸš»', 'ðŸš¼', 'ðŸš¿', 'ðŸ›€', 'ðŸ›', 'ðŸ›ƒ', 'ðŸ›„', 'ðŸ›', 'ðŸ›©', 'ðŸ›«', 'ðŸ›¬', 'ðŸ›°', 'ðŸ›³', 'ðŸ›´', 'ðŸ¤', 'ðŸ¤‘', 'ðŸ¤’', 'ðŸ¤“', 'ðŸ¤”', 'ðŸ¤•', 'ðŸ¤–', 'ðŸ¤—', 'ðŸ¤˜', 'ðŸ¤™', 'ðŸ¤š', 'ðŸ¤›', 'ðŸ¤œ', 'ðŸ¤', 'ðŸ¤ž', 'ðŸ¤ ', 'ðŸ¤¡', 'ðŸ¤¢', 'ðŸ¤£', 'ðŸ¤¤', 'ðŸ¤¥', 'ðŸ¤¦', 'ðŸ¤§', 'ðŸ¤³', 'ðŸ¤´', 'ðŸ¤·', 'ðŸ¥€', 'ðŸ¥', 'ðŸ¥‚', 'ðŸ¥ƒ', 'ðŸ¥„', 'ðŸ¥…', 'ðŸ¥‡', 'ðŸ¥Š', 'ðŸ¥', 'ðŸ¥’', 'ðŸ¥“', 'ðŸ¥”', 'ðŸ¥˜', 'ðŸ¥™', 'ðŸ¥ž', 'ðŸ¦€', 'ðŸ¦', 'ðŸ¦ƒ', 'ðŸ¦„', 'ðŸ¦…', 'ðŸ¦‡', 'ðŸ¦‰', 'ðŸ¦‹', 'ðŸ¦‘', '\\U000fe4e6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMC_u8eQbVvZ",
        "colab_type": "code",
        "outputId": "92d992ec-d25f-40ac-d84a-8ecd55d09bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import nltk.data\n",
        "import nltk.tokenize \n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "def find_ngrams(text, n):\n",
        "    counter = Counter()\n",
        "    ngram = {}\n",
        "\n",
        "    num_tokens, tokens = split_into_tokens(text.lower())\n",
        "    for token in tokens:\n",
        "        sequence  = {}\n",
        "        for i,_ in enumerate(token):\n",
        "            if(i>=n - 1):\n",
        "                word, char = token[i - (n - 1):i], token[i]\n",
        "                nchar_word = ''.join(word) + char\n",
        "                counter[nchar_word] = counter.get(nchar_word, 0) + 1\n",
        "                sequence[char] = counter[nchar_word]\n",
        "                ngram[word] = sequence\n",
        "                #print({word:ngram[word]})\n",
        "    return ngram\n",
        "def token_to_unigram(token):\n",
        "    token = token.strip().strip(\",.!|&-_()[]<>{}/\\\"'\").strip()\n",
        "\n",
        "    def has_no_chars(token):\n",
        "        for char in token:\n",
        "            if char.isalpha():\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(token) == 1 or token.isdigit() or has_no_chars(token):\n",
        "        return None\n",
        "    return token\n",
        "def split_into_tokens(text):\n",
        "    tokens = []\n",
        "    for token in nltk.tokenize.WhitespaceTokenizer().tokenize(text):\n",
        "    #for token in tokenize(text):\n",
        "        unigram = token_to_unigram(token)\n",
        "        if unigram:\n",
        "            tokens.append(unigram)\n",
        "    return len(tokens), tokens\n",
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  # TODO\n",
        "  voc_size = len(vocabulary)\n",
        "  if add_one:\n",
        "      model = defaultdict(lambda: defaultdict(lambda: 1/voc_size))\n",
        "  else:\n",
        "      model = defaultdict(lambda: defaultdict(lambda: 1e-08))\n",
        "  with open(data_file_path, encoding='utf-8', newline='') as csv_file:\n",
        "    csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "    corpus = []\n",
        "    for tweet_text in csv_reader['tweet_text']:\n",
        "            corpus.append(tweet_text)\n",
        "    data = find_ngrams(\" \".join(corpus),n)\n",
        "\n",
        "    for word,chars in data.items():\n",
        "        temp_dict = {}\n",
        "        words_total = float(sum(chars.values()))\n",
        "        for char, count in chars.items():\n",
        "            if (add_one == True):\n",
        "                p = float(count + 1)/(words_total + len(vocabulary))\n",
        "            else:\n",
        "                p = float(count)/words_total\n",
        "            temp_dict[char] = p\n",
        "        model[word] = temp_dict\n",
        "  return model\n",
        "n =3\n",
        "file_name = 'en.csv'\n",
        "model = lm(n, vocabulary,file_name , False)\n",
        "print({k: v for i, (k, v) in enumerate(model.items()) if i < 50})\n",
        "#print(sum(v for v in model['abc'].values()))\n",
        "print(model['rt'])\n",
        "print({k: v for i, (k, v) in enumerate(model.items()) if i < 50})\n",
        "# for j, key in enumerate(model.keys()):\n",
        "#     if j < 50:\n",
        "#         print(\"ngram:{:s} {:f}\".format(key,sum(model[key].values())))  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'@h': {'i': 0.06206896551724138, 'l': 0.02666666666666667, 'a': 0.09793103448275862, 'r': 0.1103448275862069, 'y': 0.0910344827586207, 'c': 0.027126436781609194, 'n': 0.17471264367816092, 't': 0.2381609195402299, 'o': 0.0993103448275862, ':': 0.07264367816091954}, 'ha': {'u': 0.023801967629324024, 's': 0.18946366232941922, 't': 0.25261821643922566, 'i': 0.18406854966677244, 'n': 0.06569343065693431, '_': 0.00856870834655665, 'h': 0.0050777530942557915, 'a': 0.008251348778165662, 'l': 0.05553792446842272, 'p': 0.013329101872421454, 'e': 0.008886067914947636, 'r': 0.17613456045699777, ':': 0.00856870834655665}, 'an': {'d': 1.0}, 'nn': {'v': 0.025537634408602152, 'e': 0.29973118279569894, 'b': 0.008064516129032258, 'n': 0.14381720430107528, 't': 0.5161290322580645, 'x': 0.004032258064516129, ':': 0.002688172043010753}, 'na': {'n': 0.10649819494584838, 'a': 0.10613718411552346, 'l': 0.5790613718411552, 'y': 0.20830324909747291}, 'ah': {'h': 0.11515151515151516, 'c': 0.19393939393939394, 'a': 0.6909090909090909}, 'hm': {'h': 0.02666666666666667, 'm': 0.03, 'e': 0.08666666666666667, 'd': 0.6633333333333333, 'g': 0.10666666666666667, 'i': 0.02, 'y': 0.016666666666666666, '1': 0.006666666666666667, '8': 0.013333333333333334, '3': 0.006666666666666667, '9': 0.0033333333333333335, ':': 0.02}, 'me': {'m': 0.3762765121759623, 'e': 0.3692065985860173, 's': 0.2545168892380204}, 'em': {'m': 0.43717277486910994, 'o': 0.56282722513089}, 'mo': {'n': 0.21775147928994082, 'e': 0.6550295857988165, 'y': 0.12721893491124261}, 'or': {'o': 0.4260869565217391, 'r': 0.40347826086956523, 'y': 0.17043478260869566}, 'ri': {'r': 0.03614457831325301, 'p': 0.060240963855421686, 'i': 0.3192771084337349, 's': 0.3506024096385542, 'e': 0.23373493975903614}, 'ie': {'l': 0.11983223487118035, 'i': 0.17555422408627921, 'e': 0.6590772917914919, 'v': 0.04553624925104853}, 'es': {'s': 0.6453433678269049, 'a': 0.1317027281279398, 'y': 0.22295390404515522}, 's1': {'a': 0.1023027557568894, 'n': 0.14571536428841073, 'h': 0.02491506228765572, 'm': 0.0604001510003775, 'e': 0.15175537938844846, 'o': 0.08040770101925254, 'r': 0.18610796526991316, 'i': 0.0928652321630804, 's': 0.15024537561343904, '1': 0.0026425066062665155, ':': 0.0026425066062665155}, 'th': {'e': 0.8972718086897945, 'n': 0.10272819131020546}, 'hi': {'i': 0.577304964539007, 't': 0.4226950354609929}, 'wa': {'n': 0.3510747185261003, 't': 0.6489252814738997}, 'no': {'t': 1.0}, 'on': {'o': 0.0069381598793363496, 'n': 0.2015082956259427, 't': 0.11010558069381599, 'c': 0.0030165912518853697, 'a': 0.0036199095022624436, 'r': 0.24615384615384617, 'e': 0.428657616892911}, 'nl': {'l': 0.5450236966824644, 'y': 0.4549763033175355}, 'ca': {'b': 0.020241593209271956, 'a': 0.08684296441397323, 'm': 0.08749591903362716, 'c': 0.07378387202089455, 'r': 0.267058439438459, 'e': 0.4645772118837741}, 'at': {'e': 0.11459129106187929, 'a': 0.01145912910618793, 't': 0.43544690603514136, 'h': 0.19022154316271964, 'p': 0.008403361344537815, 'n': 0.09702062643239114, 'l': 0.053475935828877004, 's': 0.08938120702826585}, 'tc': {'o': 0.0069381598793363496, 'n': 0.2015082956259427, 't': 0.11010558069381599, 'c': 0.0030165912518853697, 'a': 0.0036199095022624436, 'r': 0.24615384615384617, 'e': 0.428657616892911}, 'ch': {'i': 0.168125, 'c': 0.18875, 'h': 0.215625, 'a': 0.004375, 'e': 0.0825, 'l': 0.115, 'k': 0.0125, 'n': 0.168125, 'p': 0.005, ':': 0.04}, 'bu': {'t': 5.679075446517307e-05, 'p': 0.16994633273703041, 's': 0.16864014538433142, ':': 0.16815742397137745, '/': 0.16210920862083653, '.': 0.16423886191328052, 'c': 0.1634437913507681, 'o': 0.00011358150893034614, '9': 0.001959281029048471, 'f': 0.00014197688616293268, '6': 5.679075446517307e-05, '1': 2.8395377232586536e-05, 'b': 2.8395377232586536e-05, 'u': 2.8395377232586536e-05, 'y': 0.0010222335803731152, 'q': 2.8395377232586536e-05}, 'he': {'e': 0.8972718086897945, 'n': 0.10272819131020546}, 'el': {'e': 0.3236514522821577, 'l': 0.5020746887966805, 't': 0.14937759336099585, 'a': 0.024896265560165973}, 'lp': {'u': 0.023801967629324024, 's': 0.18946366232941922, 't': 0.25261821643922566, 'i': 0.18406854966677244, 'n': 0.06569343065693431, '_': 0.00856870834655665, 'h': 0.0050777530942557915, 'a': 0.008251348778165662, 'l': 0.05553792446842272, 'p': 0.013329101872421454, 'e': 0.008886067914947636, 'r': 0.17613456045699777, ':': 0.00856870834655665}, 'pf': {'a': 0.16795366795366795, 'n': 0.16988416988416988, 'p': 0.013513513513513514, 'f': 0.09652509652509653, 'e': 0.16312741312741313, 'i': 0.008687258687258687, 'r': 0.16312741312741313, ':': 0.2171814671814672}, 'fu': {'o': 0.2235494880546075, 'y': 0.10921501706484642, 'f': 0.025597269624573378, 'u': 0.015358361774744027, 'l': 0.33447098976109213, 'c': 0.027303754266211604, 'a': 0.017064846416382253, 't': 0.24744027303754265}, 'sc': {'o': 0.39185750636132316, 'b': 0.09414758269720101, 'y': 0.08142493638676845, 's': 0.02544529262086514, 'c': 0.022900763358778626, 't': 0.3842239185750636}, 'ho': {'w': 1.0}, 'oo': {'o': 0.8347457627118644, 'f': 0.1652542372881356}, 'da': {'a': 0.16795366795366795, 'n': 0.16988416988416988, 'p': 0.013513513513513514, 'f': 0.09652509652509653, 'e': 0.16312741312741313, 'i': 0.008687258687258687, 'r': 0.16312741312741313, ':': 0.2171814671814672}, 'ay': {'o': 0.06235294117647059, 't': 0.7376470588235294, 'q': 0.004705882352941176, 'u': 0.0058823529411764705, 'a': 0.12705882352941175, 'y': 0.0058823529411764705, ':': 0.05647058823529412}, '#1': {'1': 0.014132762312633832, 'y': 0.014132762312633832, 'e': 0.014132762312633832, 'a': 0.11177730192719486, 'r': 0.332762312633833, 's': 0.1297644539614561, 'o': 0.01670235546038544, 'f': 0.03340471092077088, 'h': 0.02740899357601713, 'n': 0.13147751605995717, 'm': 0.021413276231263382, 't': 0.15289079229122055}, '11': {'1': 0.014132762312633832, 'y': 0.014132762312633832, 'e': 0.014132762312633832, 'a': 0.11177730192719486, 'r': 0.332762312633833, 's': 0.1297644539614561, 'o': 0.01670235546038544, 'f': 0.03340471092077088, 'h': 0.02740899357601713, 'n': 0.13147751605995717, 'm': 0.021413276231263382, 't': 0.15289079229122055}, '1y': {'t': 0.14241713112349663, 'p': 0.1461572308594896, 's': 2.4445096313679475e-05, ':': 0.14461718979172777, '/': 0.13943482937322774, '.': 0.14126821159675368, 'c': 0.14058374889997066, 'o': 0.1401681822626381, 'd': 0.004913464359049575, 'j': 0.00012222548156839739, '3': 7.333528894103843e-05, '9': 4.889019262735895e-05, '7': 2.4445096313679475e-05, '1': 4.889019262735895e-05, 'y': 2.4445096313679475e-05, 'v': 4.889019262735895e-05, 'l': 2.4445096313679475e-05}, 'ye': {'a': 0.26958105646630237, 'r': 0.7304189435336976}, 'ea': {'a': 0.45666666666666667, 'l': 0.29033333333333333, 'i': 0.117, 's': 0.071, 'e': 0.065}, 'ar': {'b': 0.020241593209271956, 'a': 0.08684296441397323, 'm': 0.08749591903362716, 'c': 0.07378387202089455, 'r': 0.267058439438459, 'e': 0.4645772118837741}, 'rs': {'r': 0.6571428571428571, 's': 0.15934065934065933, 'e': 0.1835164835164835}, 'so': {'o': 1.0}, 'of': {'f': 1.0}, 'fh': {'t': 0.1414158855451406, 'p': 0.14514060187469166, 's': 0.1440059200789344, ':': 0.1435865811544154, '/': 0.13848051307350764, '.': 0.14028120374938333, 'c': 0.13963986186482485, 'o': 0.0014553527380365072, 'm': 0.004366058214109522, 'x': 0.000123334977799704, 'd': 4.93339911198816e-05, 'n': 2.46669955599408e-05, 'f': 0.0002220029600394672, 'h': 0.0010113468179575728, 'g': 2.46669955599408e-05, '9': 9.86679822397632e-05, 'v': 7.40009866798224e-05}, 'nt': {'a': 0.20859407592824364, 'n': 0.15310805173133082, 't': 0.2649144764288694, 'e': 0.17271589486858574, 'd': 0.20066750104297038}, 'ta': {'k': 0.25396825396825395, 'e': 0.746031746031746}, 'ht': {'t': 0.16686688404553515, 'p': 2.860248269549797e-05, 's': 0.16995595217664894, ':': 0.16946970997082547, '/': 0.1633773811566844, '.': 0.16552256735884674, 'c': 2.860248269549797e-05, 'o': 0.16426405812024483, '5': 2.860248269549797e-05, '1': 2.860248269549797e-05, 'r': 8.580744808649391e-05, 'b': 2.860248269549797e-05, 'h': 5.720496539099594e-05, 'n': 0.00011440993078199188, 'g': 0.00014301241347748986}, 'tt': {'t': 0.16686688404553515, 'p': 2.860248269549797e-05, 's': 0.16995595217664894, ':': 0.16946970997082547, '/': 0.1633773811566844, '.': 0.16552256735884674, 'c': 2.860248269549797e-05, 'o': 0.16426405812024483, '5': 2.860248269549797e-05, '1': 2.860248269549797e-05, 'r': 8.580744808649391e-05, 'b': 2.860248269549797e-05, 'h': 5.720496539099594e-05, 'n': 0.00011440993078199188, 'g': 0.00014301241347748986}}\n",
            "{'r': 0.7505910165484634, 't': 0.1871552403467297, 'h': 0.062253743104806934}\n",
            "{'@h': {'i': 0.06206896551724138, 'l': 0.02666666666666667, 'a': 0.09793103448275862, 'r': 0.1103448275862069, 'y': 0.0910344827586207, 'c': 0.027126436781609194, 'n': 0.17471264367816092, 't': 0.2381609195402299, 'o': 0.0993103448275862, ':': 0.07264367816091954}, 'ha': {'u': 0.023801967629324024, 's': 0.18946366232941922, 't': 0.25261821643922566, 'i': 0.18406854966677244, 'n': 0.06569343065693431, '_': 0.00856870834655665, 'h': 0.0050777530942557915, 'a': 0.008251348778165662, 'l': 0.05553792446842272, 'p': 0.013329101872421454, 'e': 0.008886067914947636, 'r': 0.17613456045699777, ':': 0.00856870834655665}, 'an': {'d': 1.0}, 'nn': {'v': 0.025537634408602152, 'e': 0.29973118279569894, 'b': 0.008064516129032258, 'n': 0.14381720430107528, 't': 0.5161290322580645, 'x': 0.004032258064516129, ':': 0.002688172043010753}, 'na': {'n': 0.10649819494584838, 'a': 0.10613718411552346, 'l': 0.5790613718411552, 'y': 0.20830324909747291}, 'ah': {'h': 0.11515151515151516, 'c': 0.19393939393939394, 'a': 0.6909090909090909}, 'hm': {'h': 0.02666666666666667, 'm': 0.03, 'e': 0.08666666666666667, 'd': 0.6633333333333333, 'g': 0.10666666666666667, 'i': 0.02, 'y': 0.016666666666666666, '1': 0.006666666666666667, '8': 0.013333333333333334, '3': 0.006666666666666667, '9': 0.0033333333333333335, ':': 0.02}, 'me': {'m': 0.3762765121759623, 'e': 0.3692065985860173, 's': 0.2545168892380204}, 'em': {'m': 0.43717277486910994, 'o': 0.56282722513089}, 'mo': {'n': 0.21775147928994082, 'e': 0.6550295857988165, 'y': 0.12721893491124261}, 'or': {'o': 0.4260869565217391, 'r': 0.40347826086956523, 'y': 0.17043478260869566}, 'ri': {'r': 0.03614457831325301, 'p': 0.060240963855421686, 'i': 0.3192771084337349, 's': 0.3506024096385542, 'e': 0.23373493975903614}, 'ie': {'l': 0.11983223487118035, 'i': 0.17555422408627921, 'e': 0.6590772917914919, 'v': 0.04553624925104853}, 'es': {'s': 0.6453433678269049, 'a': 0.1317027281279398, 'y': 0.22295390404515522}, 's1': {'a': 0.1023027557568894, 'n': 0.14571536428841073, 'h': 0.02491506228765572, 'm': 0.0604001510003775, 'e': 0.15175537938844846, 'o': 0.08040770101925254, 'r': 0.18610796526991316, 'i': 0.0928652321630804, 's': 0.15024537561343904, '1': 0.0026425066062665155, ':': 0.0026425066062665155}, 'th': {'e': 0.8972718086897945, 'n': 0.10272819131020546}, 'hi': {'i': 0.577304964539007, 't': 0.4226950354609929}, 'wa': {'n': 0.3510747185261003, 't': 0.6489252814738997}, 'no': {'t': 1.0}, 'on': {'o': 0.0069381598793363496, 'n': 0.2015082956259427, 't': 0.11010558069381599, 'c': 0.0030165912518853697, 'a': 0.0036199095022624436, 'r': 0.24615384615384617, 'e': 0.428657616892911}, 'nl': {'l': 0.5450236966824644, 'y': 0.4549763033175355}, 'ca': {'b': 0.020241593209271956, 'a': 0.08684296441397323, 'm': 0.08749591903362716, 'c': 0.07378387202089455, 'r': 0.267058439438459, 'e': 0.4645772118837741}, 'at': {'e': 0.11459129106187929, 'a': 0.01145912910618793, 't': 0.43544690603514136, 'h': 0.19022154316271964, 'p': 0.008403361344537815, 'n': 0.09702062643239114, 'l': 0.053475935828877004, 's': 0.08938120702826585}, 'tc': {'o': 0.0069381598793363496, 'n': 0.2015082956259427, 't': 0.11010558069381599, 'c': 0.0030165912518853697, 'a': 0.0036199095022624436, 'r': 0.24615384615384617, 'e': 0.428657616892911}, 'ch': {'i': 0.168125, 'c': 0.18875, 'h': 0.215625, 'a': 0.004375, 'e': 0.0825, 'l': 0.115, 'k': 0.0125, 'n': 0.168125, 'p': 0.005, ':': 0.04}, 'bu': {'t': 5.679075446517307e-05, 'p': 0.16994633273703041, 's': 0.16864014538433142, ':': 0.16815742397137745, '/': 0.16210920862083653, '.': 0.16423886191328052, 'c': 0.1634437913507681, 'o': 0.00011358150893034614, '9': 0.001959281029048471, 'f': 0.00014197688616293268, '6': 5.679075446517307e-05, '1': 2.8395377232586536e-05, 'b': 2.8395377232586536e-05, 'u': 2.8395377232586536e-05, 'y': 0.0010222335803731152, 'q': 2.8395377232586536e-05}, 'he': {'e': 0.8972718086897945, 'n': 0.10272819131020546}, 'el': {'e': 0.3236514522821577, 'l': 0.5020746887966805, 't': 0.14937759336099585, 'a': 0.024896265560165973}, 'lp': {'u': 0.023801967629324024, 's': 0.18946366232941922, 't': 0.25261821643922566, 'i': 0.18406854966677244, 'n': 0.06569343065693431, '_': 0.00856870834655665, 'h': 0.0050777530942557915, 'a': 0.008251348778165662, 'l': 0.05553792446842272, 'p': 0.013329101872421454, 'e': 0.008886067914947636, 'r': 0.17613456045699777, ':': 0.00856870834655665}, 'pf': {'a': 0.16795366795366795, 'n': 0.16988416988416988, 'p': 0.013513513513513514, 'f': 0.09652509652509653, 'e': 0.16312741312741313, 'i': 0.008687258687258687, 'r': 0.16312741312741313, ':': 0.2171814671814672}, 'fu': {'o': 0.2235494880546075, 'y': 0.10921501706484642, 'f': 0.025597269624573378, 'u': 0.015358361774744027, 'l': 0.33447098976109213, 'c': 0.027303754266211604, 'a': 0.017064846416382253, 't': 0.24744027303754265}, 'sc': {'o': 0.39185750636132316, 'b': 0.09414758269720101, 'y': 0.08142493638676845, 's': 0.02544529262086514, 'c': 0.022900763358778626, 't': 0.3842239185750636}, 'ho': {'w': 1.0}, 'oo': {'o': 0.8347457627118644, 'f': 0.1652542372881356}, 'da': {'a': 0.16795366795366795, 'n': 0.16988416988416988, 'p': 0.013513513513513514, 'f': 0.09652509652509653, 'e': 0.16312741312741313, 'i': 0.008687258687258687, 'r': 0.16312741312741313, ':': 0.2171814671814672}, 'ay': {'o': 0.06235294117647059, 't': 0.7376470588235294, 'q': 0.004705882352941176, 'u': 0.0058823529411764705, 'a': 0.12705882352941175, 'y': 0.0058823529411764705, ':': 0.05647058823529412}, '#1': {'1': 0.014132762312633832, 'y': 0.014132762312633832, 'e': 0.014132762312633832, 'a': 0.11177730192719486, 'r': 0.332762312633833, 's': 0.1297644539614561, 'o': 0.01670235546038544, 'f': 0.03340471092077088, 'h': 0.02740899357601713, 'n': 0.13147751605995717, 'm': 0.021413276231263382, 't': 0.15289079229122055}, '11': {'1': 0.014132762312633832, 'y': 0.014132762312633832, 'e': 0.014132762312633832, 'a': 0.11177730192719486, 'r': 0.332762312633833, 's': 0.1297644539614561, 'o': 0.01670235546038544, 'f': 0.03340471092077088, 'h': 0.02740899357601713, 'n': 0.13147751605995717, 'm': 0.021413276231263382, 't': 0.15289079229122055}, '1y': {'t': 0.14241713112349663, 'p': 0.1461572308594896, 's': 2.4445096313679475e-05, ':': 0.14461718979172777, '/': 0.13943482937322774, '.': 0.14126821159675368, 'c': 0.14058374889997066, 'o': 0.1401681822626381, 'd': 0.004913464359049575, 'j': 0.00012222548156839739, '3': 7.333528894103843e-05, '9': 4.889019262735895e-05, '7': 2.4445096313679475e-05, '1': 4.889019262735895e-05, 'y': 2.4445096313679475e-05, 'v': 4.889019262735895e-05, 'l': 2.4445096313679475e-05}, 'ye': {'a': 0.26958105646630237, 'r': 0.7304189435336976}, 'ea': {'a': 0.45666666666666667, 'l': 0.29033333333333333, 'i': 0.117, 's': 0.071, 'e': 0.065}, 'ar': {'b': 0.020241593209271956, 'a': 0.08684296441397323, 'm': 0.08749591903362716, 'c': 0.07378387202089455, 'r': 0.267058439438459, 'e': 0.4645772118837741}, 'rs': {'r': 0.6571428571428571, 's': 0.15934065934065933, 'e': 0.1835164835164835}, 'so': {'o': 1.0}, 'of': {'f': 1.0}, 'fh': {'t': 0.1414158855451406, 'p': 0.14514060187469166, 's': 0.1440059200789344, ':': 0.1435865811544154, '/': 0.13848051307350764, '.': 0.14028120374938333, 'c': 0.13963986186482485, 'o': 0.0014553527380365072, 'm': 0.004366058214109522, 'x': 0.000123334977799704, 'd': 4.93339911198816e-05, 'n': 2.46669955599408e-05, 'f': 0.0002220029600394672, 'h': 0.0010113468179575728, 'g': 2.46669955599408e-05, '9': 9.86679822397632e-05, 'v': 7.40009866798224e-05}, 'nt': {'a': 0.20859407592824364, 'n': 0.15310805173133082, 't': 0.2649144764288694, 'e': 0.17271589486858574, 'd': 0.20066750104297038}, 'ta': {'k': 0.25396825396825395, 'e': 0.746031746031746}, 'ht': {'t': 0.16686688404553515, 'p': 2.860248269549797e-05, 's': 0.16995595217664894, ':': 0.16946970997082547, '/': 0.1633773811566844, '.': 0.16552256735884674, 'c': 2.860248269549797e-05, 'o': 0.16426405812024483, '5': 2.860248269549797e-05, '1': 2.860248269549797e-05, 'r': 8.580744808649391e-05, 'b': 2.860248269549797e-05, 'h': 5.720496539099594e-05, 'n': 0.00011440993078199188, 'g': 0.00014301241347748986}, 'tt': {'t': 0.16686688404553515, 'p': 2.860248269549797e-05, 's': 0.16995595217664894, ':': 0.16946970997082547, '/': 0.1633773811566844, '.': 0.16552256735884674, 'c': 2.860248269549797e-05, 'o': 0.16426405812024483, '5': 2.860248269549797e-05, '1': 2.860248269549797e-05, 'r': 8.580744808649391e-05, 'b': 2.860248269549797e-05, 'h': 5.720496539099594e-05, 'n': 0.00011440993078199188, 'g': 0.00014301241347748986}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3",
        "colab_type": "text"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kkMn328-lJ",
        "colab_type": "code",
        "outputId": "4ee5cb89-5a9f-4e73-ff6f-c1ccc698f28d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "def compute_entropy_xi(ngram):\n",
        "    ll = 0\n",
        "    for key in ngram.items():\n",
        "        p = key[1]\n",
        "        ll += p*np.log2(p)\n",
        "    return ll\n",
        "\n",
        "def eval(n, model, data_file):\n",
        "    \"\"\"\n",
        "    Compute the perplexity of ngrams from model\n",
        "    \"\"\"\n",
        "    h_x = 0\n",
        "    N = 0\n",
        "    with open(data_file, encoding='utf-8', newline='') as csv_file:\n",
        "        csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "        corpus = []\n",
        "        for tweet_text in csv_reader['tweet_text']:\n",
        "                corpus.append(tweet_text)\n",
        "        data = find_ngrams(\" \".join(corpus),n)\n",
        "        N = len(model)   # Total number of words\n",
        "        for word,chars in data.items():\n",
        "            if word in model:\n",
        "                ngram = model[word]\n",
        "                #for i, ngram in enumerate(model.items()):\n",
        "                h_x += compute_entropy_xi(ngram)\n",
        "            #N += len(h_X)\n",
        "    return pow(2, (-1.0 / N) * h_x)\n",
        "\n",
        "n =3\n",
        "file_name = 'en.csv'\n",
        "model = lm(n, vocabulary,file_name , False)\n",
        "test_file_name = 'fr.csv'\n",
        "preplexity = eval(n,model,test_file_name)\n",
        "print(\"Preplexity evaluation : {:f}\".format(preplexity))\n",
        "# import math\n",
        "# from collections import defaultdict\n",
        "# def lm_b(n, vocabulary, data_file_path, add_one):\n",
        "#   # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "#   # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "#   # data_file_path - the data_file from which we record probabilities for our model\n",
        "#   # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "#   voc_size = len(vocabulary)\n",
        "#   if add_one:\n",
        "#       model = defaultdict(lambda: defaultdict(lambda: 1/voc_size))\n",
        "#   else:\n",
        "#       model = defaultdict(lambda: defaultdict(lambda: 1e-08))\n",
        "#   data_file = pd.read_csv(data_file_path)\n",
        "#   for data in data_file['tweet_text'].values:\n",
        "#       data = [\"<s>\"] + list(data) + [\"</s>\"]\n",
        "#       for i in range(len(data) - n):\n",
        "#           word, char = ''.join(data[i:i + n]), data[i + n]\n",
        "#           model[''.join(word)][char] = model[word].get(char, 0) + 1\n",
        "\n",
        "#   for word in model:\n",
        "#       if add_one:\n",
        "#           total_count = float(sum(model[word].values())) + voc_size\n",
        "#           for char in model[word]:\n",
        "#               model[word][char] = (model[word][char] + 1) / total_count\n",
        "#       else:\n",
        "#           total_count = float(sum(model[word].values()))\n",
        "#           for char in model[word]:\n",
        "#               model[word][char] /= total_count\n",
        "\n",
        "#   return model\n",
        "\n",
        "# def eval_b(n, model, data_file_path):\n",
        "#   # n - the n-gram that you used to build your model (must be the same number)\n",
        "#   # model - the dictionary (model) to use for calculating perplexity\n",
        "#   # data_file - the tweets file that you wish to claculate a perplexity score for\n",
        "\n",
        "#   data_file = pd.read_csv(data_file_path)\n",
        "#   count = 0\n",
        "#   total = 0.0\n",
        "#   for data in data_file['tweet_text'].values:\n",
        "#       data = ['<s>'] + list(data) + ['</s>']\n",
        "#       for i in range(len(data) - n):\n",
        "#           count += 1\n",
        "#           word, char = ''.join(data[i:i + n]), data[i + n]\n",
        "#           total += math.log2(model[word][char])\n",
        "\n",
        "#   ent = -1 / count * total\n",
        "#   per = 2 ** ent\n",
        "\n",
        "#   return per\n",
        "# n =3\n",
        "# file_name = '/content/drive/My Drive/IDC/NLP/Assignment1/en.csv'\n",
        "# model_b = lm_b(n, vocabulary,file_name , False)\n",
        "# print(eval_b(2, model_b, '/content/drive/My Drive/IDC/NLP/Assignment1/en.csv'))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preplexity evaluation : 2.589641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p",
        "colab_type": "text"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAxLE9s_fvn",
        "colab_type": "code",
        "outputId": "3ee191cd-d1da-40f9-ccd4-a4bed60de13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# def match(n, add_one):\n",
        "#   # n - the n-gram to use for creating n-gram models\n",
        "#   # add_one - use add_one smoothing or not\n",
        "\n",
        "#   #TODO\n",
        "#   file_path = '/content/drive/My Drive/IDC/NLP/Assignment1/{}.csv'\n",
        "#   vocabulary = preprocess()\n",
        "\n",
        "#   lang = ['en' ,'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "#   df = pd.DataFrame(columns=lang, index=lang)\n",
        "#   for l1 in lang:\n",
        "#     l1_model = lm(n, vocabulary, file_path.format(l1), add_one)\n",
        "#     for l2 in lang:\n",
        "#       df.at[l1, l2] = eval(n, l1_model, file_path.format(l2))\n",
        "\n",
        "#   return df\n",
        "def match(n, add_one):\n",
        "#  path_name = ''\n",
        "#  csvglob = path_name + '*.csv'\n",
        " pathlist = glob('*.csv')\n",
        " num = len(pathlist)\n",
        " matrix = np.empty(shape=(num,num),dtype='float')\n",
        " headers_list = []\n",
        " i = 0\n",
        " for model_path in pathlist:\n",
        "    head, tail = os.path.split(model_path)\n",
        "    headers_list.append(tail.rstrip('.csv'))\n",
        "    j = 0\n",
        "    modeli = lm(n, vocabulary,model_path, add_one)\n",
        "    for data_file_path in pathlist:\n",
        "        #if ( i != j ):\n",
        "        pp = eval(n,modeli,data_file_path)\n",
        "        matrix[i][j] = pp\n",
        "        #print (\"Preplexity of model {:d} with data file {:d} is {:f}\".format(i,j,pp))\n",
        "        j = j+1\n",
        "    i = i + 1\n",
        "#  print(matrix)\n",
        " dataframe = pd.DataFrame(matrix,columns=headers_list,index=headers_list)\n",
        " print(\"passed dataframe\")\n",
        " return dataframe\n",
        "\n",
        "print (match(2, False))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passed dataframe\n",
            "          en        tl        pt  ...        in        it        nl\n",
            "en  3.688392  1.817754  1.705147  ...  1.836229  1.728866  1.855658\n",
            "tl  1.756077  3.526284  1.580283  ...  1.851085  1.622713  1.772741\n",
            "pt  2.077481  1.913563  3.775446  ...  1.933758  1.710553  1.882190\n",
            "e   2.051986  1.956943  1.904216  ...  1.896964  1.928765  1.774035\n",
            "fr  1.783147  1.876383  1.593590  ...  1.907068  1.643849  1.756990\n",
            "in  1.776085  1.769760  1.573840  ...  3.460067  1.549607  1.733575\n",
            "it  2.241573  2.214109  1.836097  ...  2.108697  3.714469  2.024682\n",
            "nl  2.145320  2.159224  1.770823  ...  2.064769  1.790204  3.916633\n",
            "\n",
            "[8 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17",
        "colab_type": "text"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk32naXyAMdl",
        "colab_type": "code",
        "outputId": "05addb27-43b6-4cf3-ba51-6472efce839c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_n = 4 \n",
        "for n in range(4): \n",
        "  print(\"n = {:d} add_one:{:b}\".format(n+1,False))\n",
        "  print(match(n+1,False)) \n",
        "  print(\"n = {:d} add_one:{:b}\".format(n+1,True))\n",
        "  print(match(n+1,True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 1 add_one:0\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  11.1978  11.1978  11.1978  11.1978  11.1978  11.1978  11.1978  11.1978\n",
            "es  10.2632  10.2632  10.2632  10.2632  10.2632  10.2632  10.2632  10.2632\n",
            "fr  11.2849  11.2849  11.2849  11.2849  11.2849  11.2849  11.2849  11.2849\n",
            "in  8.04565  8.04565  8.04565  8.04565  8.04565  8.04565  8.04565  8.04565\n",
            "it  10.7234  10.7234  10.7234  10.7234  10.7234  10.7234  10.7234  10.7234\n",
            "nl  2.91596  2.91596  2.91596  2.91596  2.91596  2.91596  2.91596  2.91596\n",
            "pt  3.86417  3.86417  3.86417  3.86417  3.86417  3.86417  3.86417  3.86417\n",
            "tl  4.75737  4.75737  4.75737  4.75737  4.75737  4.75737  4.75737  4.75737\n",
            "n = 1 add_one:1\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  11.1272  11.1272  11.1272  11.1272  11.1272  11.1272  11.1272  11.1272\n",
            "es  10.1974  10.1974  10.1974  10.1974  10.1974  10.1974  10.1974  10.1974\n",
            "fr  11.2112  11.2112  11.2112  11.2112  11.2112  11.2112  11.2112  11.2112\n",
            "in  7.99758  7.99758  7.99758  7.99758  7.99758  7.99758  7.99758  7.99758\n",
            "it  10.6417  10.6417  10.6417  10.6417  10.6417  10.6417  10.6417  10.6417\n",
            "nl  2.91235  2.91235  2.91235  2.91235  2.91235  2.91235  2.91235  2.91235\n",
            "pt  3.84914  3.84914  3.84914  3.84914  3.84914  3.84914  3.84914  3.84914\n",
            "tl  4.72315  4.72315  4.72315  4.72315  4.72315  4.72315  4.72315  4.72315\n",
            "n = 2 add_one:0\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  3.68839  1.81316  1.69499  1.83623  1.72887  1.85566  1.70515  1.81775\n",
            "es  2.05199  3.46853  1.83994  1.89696  1.92877  1.77403  1.90422  1.95694\n",
            "fr  1.78315  1.68288  3.72805  1.90707  1.64385  1.75699  1.59359  1.87638\n",
            "in  1.77608  1.59407  1.68353  3.46007  1.54961  1.73357  1.57384  1.76976\n",
            "it  2.24157  2.23715  2.03807   2.1087  3.71447  2.02468   1.8361  2.21411\n",
            "nl  2.14532  1.71133  1.91533  2.06477   1.7902  3.91663  1.77082  2.15922\n",
            "pt  2.07748  2.02448   1.7561  1.93376  1.71055  1.88219  3.77545  1.91356\n",
            "tl  1.75608  1.69882  1.75978  1.85108  1.62271  1.77274  1.58028  3.52628\n",
            "n = 2 add_one:1\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  2.39748  1.64365  1.55662  1.54321  1.59917   1.5525  1.48226  1.55904\n",
            "es  1.84712  2.61345  1.74082  1.70485  1.82734  1.67095    1.674  1.78267\n",
            "fr  1.61157  1.63459  2.68794   1.6591  1.58331  1.54962  1.49102  1.67831\n",
            "in   1.4463    1.452  1.45708  1.96496  1.42883  1.39701  1.36567  1.48127\n",
            "it  2.00652  2.11392   1.8967  1.88163  2.88582  1.81549  1.76228  1.96809\n",
            "nl  1.66628    1.613  1.62407  1.58021  1.58818  2.01491  1.51012  1.60718\n",
            "pt  1.68187  1.71325  1.58703  1.55239  1.61783  1.60079  2.18101  1.59645\n",
            "tl  1.50292  1.51679  1.49249  1.49315  1.47314  1.44593  1.41857   1.8967\n",
            "n = 3 add_one:0\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  3.85341  2.61167  2.58964   2.6513    2.604  2.70513  2.50292  2.66826\n",
            "es  2.65065  3.87557  2.61612  2.59481  2.65008   2.6414  2.68388  2.63062\n",
            "fr   2.5908  2.58595  3.90553  2.56599   2.6037  2.64234  2.51636  2.62181\n",
            "in  2.62056  2.51696  2.53873  3.89725   2.5302   2.6141  2.45801  2.59139\n",
            "it  2.87057  2.87184  2.88443  2.82722  4.09414  2.85723  2.76234  2.86227\n",
            "nl  2.75582  2.62373  2.68986  2.71002  2.64253  4.45791   2.5691   2.7513\n",
            "pt   2.9045  3.07283  2.93143  2.89011  2.91876   2.9501  4.06063  2.91605\n",
            "tl  2.63104  2.56088  2.57708  2.61905  2.57144  2.65602   2.4769  4.03432\n",
            "n = 3 add_one:1\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  2.53898  2.14447  2.14501  2.15342   2.1434  2.17423  2.08878  2.15966\n",
            "es  2.19094  2.62179  2.18724  2.16523  2.20123  2.19331  2.20822  2.18062\n",
            "fr  2.12846  2.14455  2.61557  2.12464  2.15741  2.15377  2.10781  2.13349\n",
            "in  2.02952  2.00971  2.01558  2.31512  2.01883  2.02937  1.98067  2.03475\n",
            "it  2.36056  2.37494  2.38519  2.34019  2.83584  2.34579  2.31013  2.35643\n",
            "nl  2.21387  2.18053  2.18706  2.20322  2.18566  2.53223  2.13794  2.19982\n",
            "pt  2.27395  2.34708  2.30244  2.27017  2.29331  2.29514  2.58204  2.29122\n",
            "tl  2.05272  2.04519  2.04033  2.05969  2.04382  2.04502  2.00368  2.30277\n",
            "n = 4 add_one:0\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  5.18886  2.61263  2.75824  2.63288  2.57548    3.055  2.29308  2.39487\n",
            "es   2.8711  5.27109  2.79625  2.64097  2.61063  3.05622   2.3741  2.42284\n",
            "fr   2.8137  2.59835  5.14625  2.58808  2.56046  3.00096  2.28993   2.3682\n",
            "in  2.84698    2.592  2.73156  5.13517  2.56299  3.03081  2.29817  2.40391\n",
            "it  2.92894  2.69301   2.8423  2.69078  5.29207  3.10358  2.36924  2.44825\n",
            "nl   2.8652  2.61002  2.76548  2.63456  2.56513  5.45619  2.28999  2.38071\n",
            "pt  2.95509  2.79196  2.88037  2.74023  2.69594  3.14411  5.16661  2.51084\n",
            "tl  2.90205   2.6712  2.79601  2.69279    2.615  3.06012  2.35882  5.05637\n",
            "n = 4 add_one:1\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  3.22832   1.9503  2.02291  1.94509  1.92978   2.2095  1.76264    1.788\n",
            "es   2.0664  3.11341  2.00574  1.92984  1.90783  2.18147  1.76146  1.78262\n",
            "fr  2.05359  1.92521  3.12582  1.91574  1.90753  2.16785  1.75196  1.76853\n",
            "in   1.9053  1.79263  1.85237  2.76366  1.77385  2.00596  1.64496  1.66561\n",
            "it  2.09499  1.95435  2.03545  1.94678  3.16341  2.19895  1.77622  1.79344\n",
            "nl  2.15588  2.01273  2.09206  2.00201  1.97947  3.43314  1.80932  1.83427\n",
            "pt  2.00905  1.89649  1.95885  1.88601  1.86889  2.11483  2.90397  1.75443\n",
            "tl  1.86913  1.76597  1.81936  1.75301  1.73823  1.94895  1.62581  2.60846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW",
        "colab_type": "text"
      },
      "source": [
        "# **Good luck!**\n",
        "max_n = 4\n",
        "for n = 1 in range(max_n):\n",
        "  data = match(n,False)\n",
        "  print(data)\n",
        "  data = match(n,True)\n",
        "  print(data)\n",
        "  "
      ]
    }
  ]
}