{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ronen_Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronenbendavid/IDC_NLP/blob/master/Ronen_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM",
        "colab_type": "text"
      },
      "source": [
        "*As a preparation for this task, place the data files somewhere in your drive so that you can access the files from this notebook. The files are available to download from the Moodle assignment activity*\n",
        "\n",
        "The relevant files are:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGtqPrCPnLUn",
        "colab_type": "code",
        "outputId": "4a302cfb-2392-4607-ecd9-642eea3efeba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQL54PFxXYqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfzsITW8Yaj",
        "colab_type": "code",
        "outputId": "7911ce99-07f9-4735-8cae-72edce492c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "def preprocess():\n",
        "  # TODO\n",
        " vocabulary = set()\n",
        " pathlist = glob('/content/drive/My Drive/IDC/NLP/Assignment1/*.csv')\n",
        " for path in pathlist:\n",
        "    with open(path, encoding='utf-8', newline='') as csv_file:\n",
        "        csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "        for text in csv_reader['tweet_text']:\n",
        "            list_text = list(text)\n",
        "            vocabulary.update(list_text)\n",
        "            line_count += 1\n",
        "        #print(line_count)\n",
        " return sorted(list(vocabulary))\n",
        "\n",
        "vocabulary = preprocess()\n",
        "print(len(vocabulary))\n",
        "print(vocabulary)\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1859\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x97', '\\x9d', '¡', '£', '¤', '¥', '§', '¨', '©', 'ª', '«', '\\xad', '®', '¯', '°', '²', '³', '´', '¶', '·', '¸', 'º', '»', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Å', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ù', 'Ú', 'Ü', 'à', 'á', 'â', 'ã', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ė', 'Ğ', 'ğ', 'İ', 'ı', 'ń', 'ō', 'Œ', 'œ', 'Ş', 'ş', 'Š', 'Ÿ', 'ƒ', 'ʔ', 'ʕ', 'ʖ', 'ʰ', 'ʳ', 'ʷ', 'ʸ', 'ˍ', '˖', '˘', '˚', '˛', 'ˡ', 'ˢ', '̀', '́', '̃', '̈', '̥', '̮', '̯', '͜', '͡', 'Δ', 'Θ', 'Ω', 'υ', 'ω', 'А', 'И', 'М', 'Н', 'О', 'П', 'Р', 'Ф', 'Ц', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'з', 'и', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'х', 'ы', 'э', 'ю', 'я', 'Ғ', 'ү', '،', 'آ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ض', 'ط', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ی', '۶', 'ं', 'क', 'ग', 'प', 'ब', 'र', 'स', 'ा', 'े', '्', 'ೃ', '෴', 'ก', 'ข', 'ง', 'จ', 'ญ', 'ด', 'ต', 'ถ', 'ท', 'น', 'บ', 'ป', 'พ', 'ภ', 'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ส', 'อ', 'ะ', 'ั', 'า', 'ิ', 'ี', 'ุ', 'ู', 'เ', 'แ', '่', '้', '๐', '๑', 'ຶ', '༎', '༺', '༻', '༼', '༽', 'ღ', 'ᙓ', 'ᴗ', 'ᴬ', 'ᴰ', 'ᵃ', 'ᵇ', 'ᵈ', 'ᵉ', 'ᵍ', 'ᵐ', 'ᵒ', 'ᵖ', 'ᵗ', 'ᵘ', 'ᵛ', 'ᶜ', 'ᶠ', 'ᶦ', 'ᶰ', '\\u2009', '\\u200a', '\\u200b', '\\u200d', '–', '—', '―', '‘', '’', '“', '”', '„', '†', '•', '․', '…', '‰', '′', '‹', '›', '※', '‼', '‿', '⁉', '\\u2066', '\\u2067', '\\u2069', 'ⁱ', '⁷', 'ⁿ', '€', '₹', '⃣', '℃', '℅', '™', 'Ⅰ', 'Ⅱ', 'Ⅲ', 'Ⅳ', 'Ⅴ', '←', '↑', '→', '↓', '↔', '↕', '↗', '↘', '↚', '↛', '↩', '↪', '↯', '↺', '⇘', '⇨', '∀', '∆', '∇', '√', '∞', '∴', '∵', '≤', '≥', '≦', '≧', '⊙', '⋅', '⋪', '⋭', '⌚', '⌛', '⌣', '⎋', '⏩', '⏰', '⏱', '⏳', '⏸', '①', '⑥', '⒈', '⒉', '⒊', '⒋', '⒌', '⒍', '⒎', '⒏', '⒐', '⒑', 'Ⓜ', 'ⓘ', 'ⓙ', 'ⓢ', 'ⓦ', '─', '━', '┃', '┄', '┆', '┌', '┏', '┐', '┓', '└', '┗', '┘', '┛', '┳', '┻', '║', '╔', '╗', '╚', '╝', '╦', '╩', '╬', '╭', '╮', '╯', '╰', '╱', '╲', '╴', '█', '▊', '▏', '▒', '▔', '▕', '▙', '▝', '▣', '▦', '▪', '▲', '△', '▶', '▸', '►', '▼', '▽', '▿', '◀', '◁', '◄', '◆', '◇', '◈', '○', '◎', '●', '◑', '◕', '◡', '◻', '◼', '◽', '◾', '☀', '☁', '☃', '☄', '★', '☆', '☉', '☎', '☑', '☓', '☔', '☕', '☘', '☙', '☚', '☛', '☜', '☝', '☞', '☠', '☣', '☪', '☮', '☯', '☰', '☹', '☺', '☼', '☽', '☾', '♀', '♂', '♊', '♋', '♍', '♎', '♏', '♐', '♓', '♛', '♡', '♣', '♤', '♥', '♦', '♩', '♪', '♫', '♬', '♯', '♻', '⚒', '⚓', '⚔', '⚕', '⚖', '⚘', '⚜', '⚝', '⚠', '⚡', '⚪', '⚫', '⚰', '⚽', '⚾', '⛄', '⛅', '⛈', '⛓', '⛔', '⛩', '⛪', '⛳', '⛷', '⛽', '✁', '✂', '✃', '✅', '✈', '✉', '✊', '✋', '✌', '✍', '✏', '✓', '✔', '✖', '✝', '✡', '✧', '✨', '✩', '✪', '✭', '✰', '✳', '✴', '✵', '✶', '✷', '✿', '❀', '❁', '❄', '❅', '❈', '❋', '❌', '❎', '❓', '❔', '❗', '❝', '❞', '❣', '❤', '❥', '➊', '➋', '➌', '➍', '➎', '➏', '➔', '➖', '➗', '➙', '➛', '➜', '➞', '➟', '➠', '➡', '➢', '➤', '➰', '➵', '⠀', '⤵', '⦑', '⦒', '⬅', '⬇', '⭐', '⸄', '⸅', '\\u3000', '、', '。', '〆', '《', '》', '「', '」', '『', '』', '【', '】', '〜', '〡', '〰', 'い', 'う', 'え', 'お', 'か', 'が', 'き', 'く', 'し', 'せ', 'ぜ', 'た', 'っ', 'づ', 'て', 'で', 'と', 'な', 'に', 'ね', 'の', 'は', 'ひ', 'み', 'む', 'ょ', 'ら', 'り', 'る', 'れ', 'わ', 'を', '゜', 'イ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'キ', 'ク', 'グ', 'コ', 'ゴ', 'サ', 'ジ', 'ス', 'セ', 'タ', 'チ', 'ッ', 'ツ', 'テ', 'ト', 'ド', 'ナ', 'ニ', 'ネ', 'ノ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ポ', 'ム', 'メ', 'ュ', 'ユ', 'ョ', 'ラ', 'レ', 'ロ', 'ヮ', 'ワ', 'ン', '・', 'ー', 'ヽ', 'ㅅ', 'ㅈ', 'ㅋ', 'ㅏ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ', 'ㅤ', '\\u31ef', '世', '中', '主', '互', '人', '付', '会', '像', '儿', '允', '先', '入', '写', '分', '利', '制', '刹', '力', '努', '動', '午', '卒', '南', '合', '呟', '嘉', '増', '好', '姿', '嫌', '学', '尔', '希', '彡', '影', '彼', '後', '悪', '手', '投', '拶', '挨', '撃', '撮', '文', '映', '時', '曲', '月', '有', '服', '本', '林', '柱', '業', '機', '歌', '歳', '毅', '気', '洲', '洸', '王', '生', '用', '画', '界', '相', '真', '瞬', '知', '社', '稿', '空', '糟', '終', '結', '繋', '者', '花', '菜', '行', '许', '赫', '踊', '込', '通', '那', '間', '限', '風', '魏', 'ꠎ', '가', '간', '갓', '강', '걸', '검', '게', '격', '결', '경', '고', '곡', '과', '구', '국', '규', '그', '근', '금', '기', '김', '꺽', '꼼', '나', '날', '남', '내', '너', '널', '네', '넷', '녀', '년', '노', '논', '누', '는', '늘', '니', '다', '단', '당', '닿', '대', '더', '도', '동', '두', '둑', '듀', '드', '등', '디', '라', '락', '랑', '랙', '랜', '램', '러', '런', '레', '렛', '로', '롱', '료', '루', '룰', '룸', '를', '름', '릉', '리', '림', '링', '마', '맞', '매', '맨', '몬', '무', '미', '민', '밀', '바', '박', '방', '배', '백', '뱀', '버', '벅', '법', '베', '벨', '벳', '보', '복', '본', '봄', '봉', '뷔', '브', '븐', '블', '비', '빅', '빼', '사', '살', '삼', '상', '생', '샤', '샵', '서', '석', '선', '성', '세', '섹', '셔', '션', '셩', '소', '송', '수', '슈', '스', '슨', '슬', '승', '시', '식', '신', '실', '싸', '아', '안', '압', '애', '야', '양', '어', '에', '엑', '엘', '엠', '엣', '여', '역', '연', '영', '예', '오', '온', '와', '왕', '외', '요', '용', '우', '울', '워', '원', '위', '유', '윤', '의', '이', '인', '일', '임', '잘', '장', '재', '잭', '전', '정', '제', '젤', '종', '주', '쥔', '즈', '지', '직', '진', '집', '쩜', '찌', '찰', '채', '천', '철', '초', '최', '추', '출', '츠', '치', '카', '커', '코', '콘', '콤', '쿱', '크', '키', '킹', '타', '탄', '탑', '태', '터', '텐', '토', '톡', '트', '티', '틴', '팁', '파', '패', '펀', '포', '풀', '프', '플', '피', '핑', '하', '한', '해', '핸', '헌', '헤', '헨', '혁', '현', '형', '호', '화', '환', '훈', '힐', 'ﷻ', '︎', '️', '︵', '﹏', '﹪', '！', '＂', '＃', '（', '）', '＊', '．', '３', '６', '７', '８', '？', '＠', 'Ａ', 'Ｂ', 'Ｃ', 'Ｄ', 'Ｅ', 'Ｆ', 'Ｇ', 'Ｈ', 'Ｉ', 'Ｋ', 'Ｌ', 'Ｍ', 'Ｎ', 'Ｏ', 'Ｐ', 'Ｑ', 'Ｒ', 'Ｓ', 'Ｔ', 'Ｕ', 'Ｖ', 'Ｗ', 'Ｙ', '［', '］', '＿', '｀', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｇ', 'ｉ', 'ｌ', 'ｍ', 'ｎ', 'ｏ', 'ｐ', 'ｑ', 'ｒ', 'ｓ', 'ｔ', 'ｕ', 'ｖ', 'ｗ', 'ｘ', 'ｙ', 'ｚ', '｜', '｡', '･', 'ﾉ', 'ﾒ', '￣', '￼', '�', '🃏', '🅰', '🅱', '🅾', '🅿', '🆑', '🆒', '🆓', '🆔', '🆕', '🆖', '🆗', '🆘', '🆙', '🆚', '🇦', '🇧', '🇨', '🇩', '🇪', '🇫', '🇬', '🇭', '🇮', '🇯', '🇰', '🇱', '🇲', '🇳', '🇴', '🇵', '🇷', '🇸', '🇹', '🇺', '🇻', '🇼', '🇽', '🇾', '🇿', '🈴', '🈵', '🈶', '🈷', '🌀', '🌃', '🌄', '🌅', '🌆', '🌇', '🌈', '🌊', '🌋', '🌌', '🌍', '🌎', '🌏', '🌐', '🌒', '🌓', '🌗', '🌙', '🌚', '🌛', '🌜', '🌝', '🌞', '🌟', '🌠', '🌤', '🌥', '🌧', '🌨', '🌪', '🌫', '🌬', '🌭', '🌮', '🌯', '🌰', '🌱', '🌲', '🌳', '🌴', '🌵', '🌶', '🌷', '🌸', '🌹', '🌺', '🌻', '🌼', '🌽', '🌾', '🌿', '🍀', '🍁', '🍂', '🍃', '🍅', '🍆', '🍇', '🍉', '🍊', '🍋', '🍌', '🍍', '🍎', '🍏', '🍑', '🍒', '🍓', '🍔', '🍕', '🍖', '🍗', '🍚', '🍛', '🍜', '🍝', '🍞', '🍟', '🍣', '🍤', '🍥', '🍦', '🍨', '🍩', '🍪', '🍫', '🍬', '🍭', '🍯', '🍰', '🍱', '🍳', '🍴', '🍵', '🍶', '🍷', '🍸', '🍹', '🍺', '🍻', '🍼', '🍽', '🍾', '🍿', '🎀', '🎁', '🎂', '🎅', '🎆', '🎇', '🎈', '🎉', '🎊', '🎋', '🎍', '🎒', '🎓', '🎗', '🎙', '🎞', '🎟', '🎠', '🎡', '🎢', '🎤', '🎥', '🎦', '🎧', '🎨', '🎩', '🎪', '🎫', '🎬', '🎭', '🎮', '🎯', '🎰', '🎱', '🎲', '🎵', '🎶', '🎷', '🎸', '🎹', '🎺', '🎻', '🎼', '🎾', '🎿', '🏀', '🏁', '🏃', '🏄', '🏅', '🏆', '🏇', '🏈', '🏉', '🏊', '🏋', '🏌', '🏒', '🏓', '🏔', '🏖', '🏘', '🏙', '🏚', '🏟', '🏠', '🏡', '🏢', '🏩', '🏫', '🏰', '🏳', '🏴', '🏹', '🏻', '🏼', '🏽', '🏾', '🏿', '🐀', '🐁', '🐂', '🐄', '🐆', '🐇', '🐈', '🐉', '🐊', '🐍', '🐎', '🐐', '🐑', '🐒', '🐓', '🐔', '🐕', '🐖', '🐘', '🐙', '🐚', '🐜', '🐝', '🐞', '🐟', '🐠', '🐡', '🐢', '🐣', '🐥', '🐦', '🐧', '🐨', '🐩', '🐫', '🐬', '🐭', '🐮', '🐯', '🐰', '🐱', '🐲', '🐳', '🐴', '🐶', '🐷', '🐸', '🐹', '🐺', '🐻', '🐼', '🐽', '🐾', '🐿', '👀', '👁', '👂', '👄', '👅', '👆', '👇', '👈', '👉', '👊', '👋', '👌', '👍', '👎', '👏', '👐', '👑', '👓', '👕', '👖', '👗', '👙', '👞', '👟', '👠', '👡', '👣', '👤', '👥', '👦', '👧', '👨', '👩', '👪', '👫', '👭', '👮', '👯', '👰', '👱', '👲', '👳', '👵', '👶', '👷', '👸', '👹', '👺', '👻', '👼', '👽', '👿', '💀', '💁', '💂', '💃', '💄', '💅', '💆', '💈', '💉', '💊', '💋', '💌', '💍', '💎', '💏', '💐', '💑', '💒', '💓', '💔', '💕', '💖', '💗', '💘', '💙', '💚', '💛', '💜', '💝', '💞', '💟', '💡', '💢', '💣', '💤', '💥', '💦', '💧', '💨', '💩', '💪', '💫', '💬', '💭', '💮', '💯', '💰', '💲', '💳', '💵', '💶', '💸', '💻', '💼', '💽', '💿', '📀', '📂', '📅', '📆', '📈', '📊', '📋', '📌', '📍', '📏', '📓', '📖', '📚', '📛', '📝', '📞', '📡', '📢', '📣', '📦', '📧', '📩', '📬', '📯', '📰', '📱', '📲', '📴', '📷', '📸', '📹', '📺', '📻', '📼', '📽', '📿', '🔁', '🔂', '🔃', '🔄', '🔅', '🔉', '🔊', '🔋', '🔌', '🔐', '🔑', '🔒', '🔓', '🔔', '🔘', '🔙', '🔛', '🔜', '🔝', '🔞', '🔥', '🔨', '🔩', '🔪', '🔫', '🔮', '🔰', '🔱', '🔲', '🔴', '🔵', '🔶', '🔸', '🔹', '🔺', '🔻', '🔼', '🔽', '🕊', '🕋', '🕌', '🕎', '🕐', '🕒', '🕘', '🕛', '🕜', '🕟', '🕤', '🕪', '🕯', '🕵', '🕶', '🕷', '🕺', '🖐', '🖒', '🖕', '🖖', '🖤', '🖥', '🖲', '🖼', '🗂', '🗓', '🗝', '🗞', '🗡', '🗣', '🗨', '🗳', '🗻', '🗼', '🗽', '🗾', '😀', '😁', '😂', '😃', '😄', '😅', '😆', '😇', '😈', '😉', '😊', '😋', '😌', '😍', '😎', '😏', '😐', '😑', '😒', '😓', '😔', '😕', '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😦', '😧', '😨', '😩', '😪', '😫', '😬', '😭', '😮', '😯', '😰', '😱', '😲', '😳', '😴', '😵', '😶', '😷', '😸', '😹', '😺', '😻', '😼', '😽', '😿', '🙀', '🙁', '🙂', '🙃', '🙄', '🙅', '🙆', '🙇', '🙈', '🙉', '🙊', '🙋', '🙌', '🙍', '🙎', '🙏', '🚀', '🚁', '🚇', '🚈', '🚌', '🚑', '🚓', '🚔', '🚖', '🚗', '🚘', '🚙', '🚢', '🚣', '🚦', '🚧', '🚨', '🚩', '🚫', '🚬', '🚮', '🚲', '🚴', '🚵', '🚶', '🚻', '🚼', '🚿', '🛀', '🛁', '🛃', '🛄', '🛐', '🛩', '🛫', '🛬', '🛰', '🛳', '🛴', '🤐', '🤑', '🤒', '🤓', '🤔', '🤕', '🤖', '🤗', '🤘', '🤙', '🤚', '🤛', '🤜', '🤝', '🤞', '🤠', '🤡', '🤢', '🤣', '🤤', '🤥', '🤦', '🤧', '🤳', '🤴', '🤷', '🥀', '🥁', '🥂', '🥃', '🥄', '🥅', '🥇', '🥊', '🥐', '🥒', '🥓', '🥔', '🥘', '🥙', '🥞', '🦀', '🦁', '🦃', '🦄', '🦅', '🦇', '🦉', '🦋', '🦑', '\\U000fe4e6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMC_u8eQbVvZ",
        "colab_type": "code",
        "outputId": "b81fb63e-def7-41a5-f486-492c6a06df98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "def find_ngrams(text, length):\n",
        "    counter = Counter()\n",
        "    ngram = []\n",
        "    num_tokens, tokens = split_into_tokens(text.lower())\n",
        "    for token in tokens:\n",
        "        for i,_ in enumerate(token):\n",
        "           if(i>=length - 1):\n",
        "                post = token[i]\n",
        "                prior = token[i - (length - 1) : i]\n",
        "                ngram = ''.join(prior) + post\n",
        "                counter[ngram] = counter.get(ngram, 0) + 1\n",
        "    return counter\n",
        "def token_to_unigram(token):\n",
        "    token = token.strip().strip(\",.!|&-_()[]<>{}/\\\"'\").strip()\n",
        "\n",
        "    def has_no_chars(token):\n",
        "        for char in token:\n",
        "            if char.isalpha():\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(token) == 1 or token.isdigit() or has_no_chars(token):\n",
        "        return None\n",
        "    return token\n",
        "def split_into_tokens(text):\n",
        "    tokens = []\n",
        "    for token in nltk.tokenize.WhitespaceTokenizer().tokenize(text):\n",
        "        unigram = token_to_unigram(token)\n",
        "        if unigram:\n",
        "            tokens.append(unigram)\n",
        "    return len(tokens), tokens\n",
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  # TODO\n",
        "  model = {}\n",
        "  with open(data_file_path, encoding='utf-8', newline='') as csv_file:\n",
        "    csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "    corpus = []\n",
        "    for tweet_text in csv_reader['tweet_text']:\n",
        "            corpus.append(tweet_text)\n",
        "    data = find_ngrams(\" \".join(corpus),n)\n",
        "    unigram_total = float(sum(data.values()))\n",
        "    temp_dic = {}\n",
        "\n",
        "    for ngram in data.items():\n",
        "        if (add_one == True):\n",
        "            p = float(ngram[1] + 1)/(unigram_total + len(vocabulary))\n",
        "        else:\n",
        "            p = float(ngram[1])/unigram_total\n",
        "        model.update({ngram[0]:p})\n",
        "  return model\n",
        "\n",
        "model = lm(3, vocabulary, '/content/drive/My Drive/IDC/NLP/Assignment1/en.csv.json', True)\n",
        "print({k: v for i, (k, v) in enumerate(model.items()) if i < 50})\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  # TODO\n",
        "  # model = {}\n",
        "\n",
        "  # corpus_words = sorted(list(set([word for words_list in corpus for word in words_list])))\n",
        "  # num_corpus_words = len(corpus_words)\n",
        "  # data = []\n",
        "  # with open(data_file_path) as csv_file:\n",
        "  #     csv_reader = pd.read_csv(csv_file, delimiter=',')\n",
        "  #       for corpus in csv_reader['tweet_text']:\n",
        "  #         data = sorted(list(set([word for words_list in corpus.split(\" \")]\n",
        "  # #       for word in list(data)\n",
        "  # #      data =  [[START_TOKEN] + [w.lower() for w in list(data.words(f))] + [END_TOKEN] for f in files]\n",
        "  # #     data = (\" \" * n) + data\n",
        "  # #     for i in range(len(data)-n):\n",
        "  # #       word, char = data[i:i+n], data[i+n]\n",
        "  # #       if not word in model:\n",
        "  # #         model[word] = {}\n",
        "  # #       model[word][char] = model[word].get(char, 0) + 1\n",
        "  # # word_freq = Counter(chain(*data))\n",
        "  # return model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bc1696842c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_unigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_into_unigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_unigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m###########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m from nltk.chunk.util import (ChunkScore, accuracy, tagstr2tree, conllstr2tree,\n\u001b[1;32m    159\u001b[0m                              \u001b[0mconlltags2tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conllstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/chunk/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m##//////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmalt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaltParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDependencyEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitionparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransitionParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbllip\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBllipParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorenlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoreNLPParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCoreNLPDependencyParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/transitionparser.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_svmlight_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m _DEFAULT_TAGS = {\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .validation import (as_float_array,\n\u001b[1;32m     29\u001b[0m                          \u001b[0massert_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Preserves earlier default choice of pinvh cutoff `cond` value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Can be removed once issue #14055 is fully addressed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scipy_linalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpinvh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpinvh\u001b[0m \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3",
        "colab_type": "text"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kkMn328-lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(n, model, data_file):\n",
        "  # n - the n-gram that you used to build your model (must be the same number)\n",
        "  # model - the dictionary (model) to use for calculating perplexity\n",
        "  # data_file - the tweets file that you wish to claculate a perplexity score for\n",
        "\n",
        "  # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p",
        "colab_type": "text"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAxLE9s_fvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match(n, add_one):\n",
        "  # n - the n-gram to use for creating n-gram models\n",
        "  # add_one - use add_one smoothing or not\n",
        "\n",
        "  #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17",
        "colab_type": "text"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk32naXyAMdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW",
        "colab_type": "text"
      },
      "source": [
        "# **Good luck!**"
      ]
    }
  ]
}